Script started on Sun Feb 20 16:49:03 2022
[1m[7m%[27m[1m[0m                                                 [0m[27m[24m[Jarimuu@ARs-MacBook-Pro GINN % [K[?2004h  cd /Users/arimuu/O [KnneDrive/TMU/Thä¿®å£«è«–æ–‡/ScraperProgram/GINN ; /usr [K//bin/env /opt/homebrew/Caskroom/miniforge/base/en [Kvvs/tensorflow/bin/python /Users/arimuu/.vscode/ex [Kttensions/ms-python.python-2022.0.1814523869/pytho [KnnFiles/lib/python/debugpy/launcher 51714 -- /User [Kss/arimuu/OneDrive/TMU/Thä¿®å£«è«–æ–‡/ScraperProgram/G [KIINN/ginn_model.py [?2004l
Init Plugin
Init Graph Optimizer
Init Kernel
===End Reading Pickles==
Metal device set to: Apple M1

systemMemory: 16.00 GB
maxCacheSize: 5.33 GB

2022-02-20 16:49:11.744609: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2022-02-20 16:49:11.745005: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
self.x is  tf.Tensor(
[[[1. 0. 0. ... 0. 0. 0.]]

 [[0. 0. 0. ... 0. 0. 0.]]

 [[1. 0. 0. ... 0. 0. 0.]]

 ...

 [[1. 0. 0. ... 0. 0. 0.]]

 [[1. 0. 0. ... 0. 0. 0.]]

 [[1. 0. 1. ... 0. 0. 0.]]], shape=(56, 1, 915), dtype=float32)
self.x is  tf.Tensor(
[[[1. 0. 0. ... 0. 0. 0.]]

 [[0. 0. 0. ... 0. 0. 0.]]

 [[1. 0. 0. ... 0. 0. 0.]]

 ...

 [[1. 0. 0. ... 0. 0. 0.]]

 [[1. 0. 0. ... 0. 0. 0.]]

 [[1. 0. 1. ... 0. 0. 0.]]], shape=(56, 1, 915), dtype=float32)
tf.Tensor(
[[[1. 0. 0. ... 0. 0. 0.]]

 [[0. 0. 0. ... 0. 0. 0.]]

 [[1. 0. 0. ... 0. 0. 0.]]

 ...

 [[1. 0. 0. ... 0. 0. 0.]]

 [[1. 0. 0. ... 0. 0. 0.]]

 [[1. 0. 1. ... 0. 0. 0.]]], shape=(56, 1, 915), dtype=float32)
tf.Tensor(
[[[1.]]

 [[0.]]

 [[0.]]

 [[0.]]

 [[0.]]

 [[1.]]

 [[0.]]

 [[0.]]

 [[0.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[0.]]

 [[0.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[0.]]

 [[0.]]

 [[1.]]

 [[1.]]

 [[0.]]

 [[1.]]

 [[1.]]

 [[0.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[0.]]

 [[0.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[0.]]

 [[1.]]], shape=(56, 1, 1), dtype=float32)
top.input= <BatchDataset shapes: ((10, 1, 915), (10, 1, 1)), types: (tf.float32, tf.float32)>
losses in LossesContainerClass <tensorflow.python.keras.losses.BinaryCrossentropy object at 0x15effe520>
True
adapter_cls is [<class 'tensorflow.python.keras.engine.data_adapter.DatasetAdapter'>]
x is <BatchDataset shapes: ((10, 1, 915), (10, 1, 1)), types: (tf.float32, tf.float32)>
y is None
dataset is  <BatchDataset shapes: ((10, 1, 915), (10, 1, 1)), types: (tf.float32, tf.float32)>
came thuru steps_per_execution.numpy().item() is ture
self.train_function is  <function Model.make_train_function.<locals>.train_function at 0x16676b310>
data_iterator is  <tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x1691520a0>
epoch is  0
printing iterator: <tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x1691520a0>
Epoch 1/3
step is 0
next(iter) in training.py is  (<tf.Tensor: shape=(10, 1, 915), dtype=float32, numpy=
array([[[1., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.]],

       [[1., 0., 0., ..., 0., 0., 0.]],

       ...,

       [[3., 0., 0., ..., 0., 0., 0.]],

       [[2., 0., 0., ..., 0., 0., 0.]],

       [[5., 0., 1., ..., 0., 0., 0.]]], dtype=float32)>, <tf.Tensor: shape=(10, 1, 1), dtype=float32, numpy=
array([[[1.]],

       [[0.]],

       [[0.]],

       [[0.]],

       [[0.]],

       [[1.]],

       [[0.]],

       [[0.]],

       [[0.]],

       [[1.]]], dtype=float32)>)
data in run_step in training.py is  (<tf.Tensor: shape=(10, 1, 915), dtype=float32, numpy=
array([[[3., 0., 0., ..., 0., 0., 0.]],

       [[3., 0., 0., ..., 0., 0., 0.]],

       [[5., 0., 0., ..., 0., 0., 0.]],

       ...,

       [[1., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>, <tf.Tensor: shape=(10, 1, 1), dtype=float32, numpy=
array([[[1.]],

       [[1.]],

       [[1.]],

       [[1.]],

       [[1.]],

       [[1.]],

       [[1.]],

       [[1.]],

       [[1.]],

       [[0.]]], dtype=float32)>)
x in train_step tf.Tensor(
[[[3. 0. 0. ... 0. 0. 0.]]

 [[3. 0. 0. ... 0. 0. 0.]]

 [[5. 0. 0. ... 0. 0. 0.]]

 ...

 [[1. 0. 0. ... 0. 0. 0.]]

 [[0. 0. 0. ... 0. 0. 0.]]

 [[0. 0. 0. ... 0. 0. 0.]]], shape=(10, 1, 915), dtype=float32)
y in train_step tf.Tensor(
[[[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[0.]]], shape=(10, 1, 1), dtype=float32)
y.shape (10, 1, 1)
y.get_shape() (10, 1, 1)
is eager in GINN_model build True
is eager in __init__ True
il_batch_input_shape is  (10, 1, 915)
self.batch_size is  10
input_shape is  (10, 1, 915)
inputs Tensor("Placeholder:0", shape=(10, 1, 915), dtype=float32)
is eager in GINN_model call False
is eager in build True
inputs in call is  Tensor("Placeholder:0", shape=(10, 1, 915), dtype=float32)
is eager in call False
self.K is  20
self.n is  [405 140   1   3  13  84   1   2  10   1  16   3  37   6   8   1   3   2
 177   2]
self.flattened_W is  [ 7.93516159e-01 -1.15385696e-01  2.13972121e-01 -3.56907606e-01
  7.01226830e-01 -4.01539952e-01  6.82839215e-01  7.49478638e-01
 -6.74894750e-01  4.01318759e-01  3.12671304e-01 -6.55154586e-01
  6.73290253e-01 -8.49015772e-01  6.00232422e-01  8.14236701e-01
 -5.97876608e-01 -9.71457064e-01 -1.82775170e-01 -3.08911860e-01
 -7.84737587e-01  4.16453540e-01  9.21217918e-01 -1.32101342e-01
  7.13061213e-01 -9.88716006e-01  4.17406201e-01 -7.27670491e-01
  8.10134292e-01  5.45784235e-01 -7.41476834e-01  5.32974005e-01
  7.80502141e-01  7.22185254e-01 -7.42396355e-01 -4.60910112e-01
  5.70265949e-01 -9.58594739e-01 -7.49043941e-01  6.74115941e-02
  7.92037725e-01  3.64689738e-01 -9.34246480e-01  8.75723720e-01
 -9.64111924e-01 -1.33518279e-01 -1.00408934e-01  6.36220574e-01
  3.92315000e-01 -6.05255246e-01 -2.46369153e-01  9.13717449e-01
 -9.23626304e-01 -4.72151935e-02 -5.45031250e-01  3.17253262e-01
  8.93762827e-01 -8.07591915e-01 -7.49511421e-01  4.51531798e-01
 -1.91506639e-01 -4.52097833e-01  3.62567037e-01  1.56589478e-01
  4.14750785e-01  1.84347361e-01  2.15772212e-01 -2.72486985e-01
 -1.19274467e-01  7.74010122e-01 -1.52535558e-01 -9.25496697e-01
  5.89907289e-01  7.55588591e-01 -7.55951166e-01 -1.51026741e-01
  5.01725554e-01  3.40664059e-01 -2.31372818e-01  1.64513066e-01
 -7.78356194e-01  6.33505225e-01  5.83806515e-01  5.04386872e-02
 -5.64036191e-01 -1.95216954e-01  6.80999339e-01  9.35262978e-01
  8.48973274e-01 -3.33662219e-02  3.12933177e-01 -3.02711755e-01
  6.05760515e-01 -5.96679091e-01  2.00930253e-01 -4.32431012e-01
 -5.00997841e-01  3.53070229e-01 -1.59008369e-01  7.02168941e-01
 -3.91317070e-01 -5.66817880e-01 -2.98116524e-02  9.57965612e-01
 -1.61312863e-01  4.57871675e-01  2.59735376e-01  2.89803594e-01
  4.60936427e-01 -9.42323565e-01  2.03163862e-01 -5.37147522e-01
  6.04791045e-01 -7.52415657e-01  4.67093021e-01  1.20635360e-01
 -2.74869084e-01 -2.34385237e-01  4.97040063e-01 -4.28952664e-01
 -4.06379253e-01 -8.14750493e-01 -6.84553504e-01  8.23036551e-01
  2.75692105e-01  7.16948152e-01 -5.38795412e-01 -3.02735895e-01
  4.35444206e-01 -2.90485263e-01 -8.75433207e-01 -8.51987779e-01
  4.00432408e-01  9.18666661e-01  5.31239621e-02 -3.11867654e-01
  4.95588899e-01  2.33586743e-01 -6.76062405e-01  2.28712693e-01
 -7.14569747e-01  4.94450808e-01  5.05838633e-01 -6.48721337e-01
 -4.92254794e-01 -7.00271010e-01 -5.33242166e-01  2.71822691e-01
 -7.64674902e-01  7.57902205e-01  9.34066176e-02 -4.48642612e-01
 -4.15863484e-01  7.65939593e-01 -8.57596576e-01  9.29914415e-01
 -2.87626714e-01 -7.59094715e-01 -8.91866922e-01 -4.70454484e-01
 -1.49182901e-01 -7.34306455e-01 -2.55929798e-01  6.75577998e-01
 -8.89151454e-01  4.78854328e-01  5.53010941e-01  8.15914989e-01
 -2.92665064e-01 -5.97735405e-01 -7.81376958e-01  2.62847245e-01
 -6.15575969e-01 -6.52716637e-01  3.12758803e-01  8.99402022e-01
  6.64472222e-01  8.34712625e-01  9.13999498e-01 -5.79623096e-02
 -9.12859887e-02  7.16535032e-01  3.99974048e-01 -4.46744263e-01
 -6.80343151e-01 -3.70075822e-01 -1.18865266e-01  8.40933263e-01
  9.54293370e-01  3.51915151e-01  2.70979315e-01  6.91612244e-01
  2.57793099e-01  6.30734712e-02  8.60368907e-01  2.40743145e-01
  7.57942975e-01 -7.93695271e-01  6.81751430e-01  2.06740767e-01
  2.65540302e-01  6.45625949e-01 -7.60298669e-02 -2.71219850e-01
 -1.07950859e-01  4.92492802e-02 -6.99860454e-01 -3.73157382e-01
  4.33491707e-01 -1.70329630e-01  7.54544139e-01  6.19512498e-01
 -2.13935357e-02  2.30903625e-01  6.65790260e-01 -9.21040833e-01
  3.02875489e-01 -3.32171798e-01  8.81671786e-01  8.20830047e-01
 -1.32236302e-01  5.09334028e-01 -3.35963786e-01  2.48345047e-01
 -1.67326048e-01 -6.42568290e-01 -1.75206587e-01 -6.96798265e-01
  5.73418140e-01 -3.09532732e-01  8.05500150e-01 -1.79001167e-01
 -7.56307662e-01  9.80614066e-01 -6.45165145e-01 -4.63108197e-02
 -6.70987666e-01 -5.03446341e-01  7.88571656e-01  3.84187192e-01
  3.76560360e-01  6.14994347e-01 -7.77750537e-02 -6.62579775e-01
 -8.12150419e-01  9.85716045e-01 -4.95815188e-01 -4.99244928e-01
 -6.19847417e-01 -5.81861794e-01 -7.47032344e-01  2.23896444e-01
  8.38207066e-01  4.68075037e-01  1.99287161e-01 -4.10758704e-01
  7.22799003e-01  4.24241662e-01 -4.03945714e-01  4.12813395e-01
  3.76868993e-01 -6.60853028e-01  7.29760647e-01  8.26023102e-01
  9.09006536e-01 -4.90947247e-01  5.72275758e-01  2.57622093e-01
  6.27599418e-01 -2.67918110e-01 -1.93039373e-01 -8.47003818e-01
  4.95854497e-01 -7.79381156e-01 -7.63011396e-01 -7.87712812e-01
  7.14009255e-02 -6.13082170e-01  7.12333322e-01 -6.08763814e-01
 -5.58755994e-01 -7.65175998e-01 -8.04499149e-01  2.68162731e-02
  1.07454203e-01  8.39715898e-01  6.84023619e-01 -9.37083781e-01
  9.29887831e-01  2.59375433e-03 -9.30839963e-03  4.68937516e-01
 -4.10054535e-01  6.61686480e-01  4.79839295e-01  8.31101775e-01
  4.31255490e-01 -5.00757933e-01 -5.96308112e-02  3.05695236e-01
 -9.11173701e-01 -9.03489828e-01  2.10280251e-02 -8.07422996e-01
  7.35139668e-01  1.65286154e-01  1.93287343e-01  6.93862200e-01
  4.08962462e-03  4.51857924e-01 -3.53804082e-01  7.92855978e-01
 -9.33615983e-01  2.37632439e-01  7.69394159e-01 -9.28829253e-01
 -8.47534835e-01  4.86700863e-01 -2.15562042e-02 -8.64588022e-01
 -8.52388799e-01  1.59852386e-01 -5.71861207e-01  1.89441353e-01
  9.53917503e-01 -5.69051504e-01 -1.59345970e-01  4.19490397e-01
  6.39653325e-01 -1.58325449e-01 -2.66265482e-01  7.53009558e-01
  8.02227437e-01 -2.72938788e-01  9.45739076e-02  3.55973452e-01
 -7.62331188e-01 -9.42325965e-02  3.20461057e-02  8.44855785e-01
  9.11553919e-01  7.69554496e-01 -9.38220322e-01 -7.78641403e-01
 -9.16897297e-01  7.75148034e-01  9.21225488e-01 -3.46087694e-01
  1.47081584e-01  8.29313338e-01 -9.84962753e-06  2.35204995e-02
  4.72158909e-01 -3.25753726e-02 -3.82105231e-01  1.19027793e-01
 -2.60956734e-01 -7.89207339e-01  8.87176931e-01  4.44869369e-01
 -9.05357778e-01 -4.94934410e-01 -4.32844073e-01 -5.37272394e-01
  6.18817449e-01  3.71095061e-01  2.14727730e-01  5.12148917e-01
 -5.13655901e-01 -5.16705990e-01 -7.82700896e-01 -1.31557688e-01
 -5.77414930e-01 -5.20610750e-01  2.47468978e-01  3.10324818e-01
 -9.36218381e-01 -6.27615809e-01 -6.47190809e-01  6.79873466e-01
  2.59767830e-01 -1.20428160e-01 -8.03644240e-01  7.71399438e-01
 -6.71058297e-01 -2.66981572e-01 -3.46099734e-02 -6.78539276e-01
  5.33905387e-01 -9.84341979e-01 -1.42501026e-01  8.22666049e-01
  5.26133657e-01  3.57674509e-01 -5.54029524e-01  4.60919648e-01
 -6.79934084e-01  4.92051870e-01 -3.99262518e-01  9.77681518e-01
  6.46438658e-01 -3.31808060e-01  4.31028426e-01  6.32493258e-01
  9.61214960e-01  2.28483185e-01  3.67525131e-01 -5.31227708e-01
  5.62833726e-01 -3.42546672e-01  5.76312482e-01 -3.55331391e-01
  5.05981982e-01 -3.07111889e-01  3.10372800e-01 -9.04191494e-01
  1.19186588e-01 -6.50552690e-01 -8.18181515e-01  7.86605239e-01
  6.77419424e-01  8.27478111e-01 -5.34120612e-02 -3.78601164e-01
 -4.88200694e-01 -8.81703675e-01 -9.08705175e-01  8.58496726e-01
  9.27098572e-01 -9.00791883e-02  7.87826300e-01  8.34187493e-02
  4.58274305e-01  5.65948486e-01 -7.58592963e-01 -9.98876244e-02
 -7.03127086e-01 -3.77307624e-01 -2.27397785e-01 -6.17142022e-01
  9.93997693e-01  9.21337903e-01 -2.83166647e-01  6.52795434e-01
  4.92285863e-02  2.06521768e-02  5.43294191e-01 -4.99895886e-02
  3.39711130e-01 -1.80405691e-01 -3.49055320e-01  2.39553273e-01
  6.83580101e-01 -8.92544329e-01 -7.08269119e-01  6.88179672e-01
 -5.22891104e-01  2.77255863e-01 -5.21658957e-02 -5.62973440e-01
  6.84849143e-01 -7.90948093e-01  4.78244334e-01 -7.91660964e-01
 -8.51218700e-01  7.44482696e-01 -8.49836409e-01 -9.84452903e-01
 -5.63973546e-01  8.04286897e-01 -1.29123569e-01  5.18038094e-01
  3.14814538e-01  2.98627734e-01 -7.20108626e-03 -1.50889456e-01
 -8.12483490e-01 -2.10928842e-01 -2.19156116e-01 -6.79104626e-01
  5.54768443e-01  1.79884955e-01  1.53498739e-01  4.28206146e-01
 -4.01899457e-01  3.26350003e-01 -1.18499734e-01 -2.45337084e-01
 -6.55303359e-01  4.81619507e-01  9.24482524e-01 -7.22549498e-01
  3.39168787e-01 -2.23176498e-02  5.78347921e-01  2.54417229e-02
 -8.04846644e-01  3.70094776e-01  6.64084077e-01 -6.99092388e-01
  5.63493222e-02  2.76599854e-01 -5.60552478e-01 -9.31780934e-01
 -8.61873746e-01 -9.53682661e-01 -4.37726557e-01 -3.82820249e-01
  4.81511146e-01 -7.67710388e-01  7.79068112e-01  4.80325252e-01
  5.09483993e-01  8.49776208e-01  5.66444695e-01  2.63929844e-01
  6.31401598e-01  2.62384444e-01 -5.99204063e-01  1.75366446e-01
  5.63576877e-01  4.21183139e-01  6.29834056e-01 -1.34502545e-01
  8.92880619e-01  8.43460441e-01  1.78625137e-01 -8.74525383e-02
  7.15093553e-01 -1.03573829e-01 -1.10415958e-01 -4.49235320e-01
 -4.04078901e-01  3.00892383e-01  3.32444489e-01  2.21581474e-01
  3.12259644e-02  5.80145359e-01 -7.40136027e-01  4.91394311e-01
  2.29297861e-01  8.67696628e-02  2.49223039e-01 -4.63047117e-01
 -7.89206386e-01  9.47640836e-01 -2.92576820e-01 -6.12265766e-01
 -8.27940047e-01  4.12244678e-01  5.19411922e-01 -2.05165580e-01
 -1.12215534e-01 -5.74243784e-01  7.11920381e-01  2.25884140e-01
 -8.41259599e-01 -6.36766076e-01 -9.23310295e-02 -3.10472786e-01
 -8.44645739e-01  9.47636217e-02 -1.75975546e-01  9.51979637e-01
 -4.30163920e-01 -8.93091917e-01 -2.35014930e-01 -9.73374724e-01
  8.40144753e-01  2.92504638e-01  1.11310557e-01 -9.18948591e-01
 -2.50212044e-01  7.19416499e-01  7.19179571e-01 -7.71073759e-01
  9.86190915e-01  2.12537184e-01 -4.17973325e-02 -8.47433209e-01
 -1.24700047e-01 -4.76794094e-01 -2.99198478e-01 -5.50191058e-03
  8.09562266e-01  7.55835831e-01 -9.40107554e-03 -8.16706836e-01
 -7.15602338e-01 -4.01378125e-01 -3.03894520e-01  2.47593746e-02
 -9.18365240e-01  1.41392658e-02 -8.46523762e-01 -3.61272320e-02
  7.67259359e-01 -8.79878581e-01  3.54704648e-01  8.09604943e-01
 -5.91717005e-01 -8.16509843e-01 -2.30212286e-01  3.53081912e-01
  4.11080271e-01 -7.05946445e-01  7.72230774e-02 -6.03936911e-01
  5.66292167e-01  5.93103588e-01  6.96171820e-01 -1.34702995e-01
 -4.06626582e-01 -9.95727539e-01 -2.43696317e-01  3.32226485e-01
  2.70544440e-01  4.79659081e-01 -2.12323681e-01 -9.67576683e-01
  6.16217852e-02 -7.24134624e-01  2.14993194e-01  9.47745442e-01
 -3.23512316e-01 -1.19472377e-01 -2.99522728e-01  3.88082601e-02
  9.63428542e-02 -6.40858114e-01 -5.96045554e-01 -8.95039737e-02
  8.76676023e-01 -6.92015231e-01 -7.04747915e-01  1.25715109e-02
 -5.30444443e-01  2.75263935e-01  3.99501845e-02 -8.08264256e-01
 -4.07682449e-01  1.55385628e-01  2.97612697e-01 -2.45129615e-01
  7.42328286e-01 -4.77650762e-01  6.92618787e-01  6.91681445e-01
 -1.24203349e-02  8.96921575e-01  8.41405571e-01 -7.88622856e-01
 -6.05597079e-01 -4.20300126e-01 -1.16413616e-01 -3.24636102e-01
 -1.74107701e-02 -6.34151638e-01  8.02538276e-01 -3.72257292e-01
 -4.35904175e-01 -5.87755144e-01 -9.55397427e-01 -2.73766518e-01
 -2.23747715e-01  3.79369229e-01  3.22493166e-01  9.66449916e-01
  9.82773066e-01  6.47633493e-01  6.53691649e-01  7.23368451e-02
 -5.74297547e-01 -4.29185033e-02  7.30818450e-01  1.04165263e-01
  7.71589339e-01 -1.50467068e-01  7.28371978e-01 -4.62551415e-01
  9.31184739e-02 -6.38178170e-01 -1.62047014e-01  7.04480827e-01
 -9.53352034e-01 -7.35463381e-01 -7.63665020e-01  9.53523278e-01
  9.82595742e-01  1.51749790e-01 -1.96608186e-01 -2.67549634e-01
 -5.10963857e-01  8.28903615e-01  4.42490578e-01 -8.40775788e-01
 -8.44293296e-01 -8.03983152e-01 -1.33394981e-02  6.90434575e-01
 -1.73007786e-01  7.79271126e-01 -9.18660998e-01  5.55827133e-02
 -9.84062180e-02  8.56834650e-01  6.29358709e-01  4.73670065e-02
 -8.05251837e-01  7.27132678e-01 -6.92767322e-01  6.06335402e-01
  4.98829901e-01  4.75333244e-01  5.95172882e-01  2.81566590e-01
  8.10948968e-01 -6.86232626e-01  7.67588913e-01 -9.60782111e-01
 -2.29166791e-01 -8.14122319e-01 -7.53189862e-01  8.73016059e-01
 -8.13780963e-01  7.01651454e-01 -1.71414346e-01  2.00813130e-01
 -2.71361619e-01  1.42430425e-01  5.83109399e-03  9.36371505e-01
  8.71648312e-01 -7.45763257e-02  4.63911325e-01  3.35245579e-01
 -9.74325240e-01 -9.05486166e-01  6.23792768e-01  5.84032357e-01
  9.99527454e-01  1.40901580e-01 -9.21331406e-01 -1.96454853e-01
 -4.67264593e-01  3.23344111e-01  6.85002029e-01  4.41983163e-01
  8.39803100e-01 -8.38038549e-02 -2.10069403e-01 -8.16436827e-01
  2.37198070e-01 -2.49484032e-01  5.52244484e-01 -8.91737580e-01
 -8.65937352e-01 -7.17030764e-01 -2.90644504e-02 -7.68740177e-01
 -1.24625161e-01 -9.82306421e-01 -3.96685898e-01  1.00334741e-01
  9.72205475e-02 -2.94238508e-01  4.44057763e-01 -5.01254141e-01
  6.02735102e-01 -7.86399007e-01 -6.97803855e-01 -5.07117033e-01
  6.17018938e-01  2.99585313e-01 -5.99193394e-01 -4.68407601e-01
  1.18298821e-01  5.34882426e-01 -8.73750389e-01 -8.68183821e-02
  5.35658561e-02 -5.87285757e-01 -5.13376474e-01 -2.66507193e-02
  2.42357299e-01 -9.82281148e-01  3.24351996e-01  2.28503332e-01
 -1.39907598e-01 -4.61589009e-01  9.96358171e-02 -1.98858172e-01
  5.28355986e-02  3.55033964e-01  4.13769126e-01 -4.49858606e-01
  4.63773459e-02  1.05900131e-01  2.88647473e-01 -6.84072077e-01
 -8.90591741e-01  3.05030078e-01 -9.31919873e-01 -8.79714847e-01
 -3.68878275e-01  1.86875582e-01  1.99888758e-02 -8.48924100e-01
 -5.87907374e-01 -7.87871420e-01  4.09344316e-01 -2.76625156e-01
  6.66155219e-01 -3.44162658e-02 -1.14109349e-02  5.58116317e-01
 -3.29952925e-01 -4.84653592e-01  8.84273708e-01 -5.64809561e-01
  8.21206391e-01  3.20832044e-01  1.22462921e-01 -2.10871503e-01
 -1.89640686e-01  5.14908016e-01  6.82054520e-01  2.00348005e-01
 -7.34734237e-01  8.06159317e-01  3.35550278e-01  7.54157722e-01
 -9.21684265e-01 -7.14775980e-01  9.87451613e-01 -3.86934951e-02
  9.29499090e-01  9.67469454e-01  5.96027911e-01  3.63380075e-01
  9.99995708e-01 -9.71786857e-01  3.45398277e-01 -5.27924061e-01
  2.21933737e-01  1.64580271e-01 -2.16902327e-02 -3.15749079e-01
 -4.89310950e-01  8.16900134e-02  2.75878385e-02  9.97382700e-01
 -2.51740187e-01 -9.96001959e-01 -8.90745759e-01  3.59113246e-01
 -6.13297999e-01 -5.46035051e-01 -6.41274631e-01 -7.90135145e-01
  3.03470820e-01  4.26194482e-02  8.65728974e-01 -9.78875935e-01
 -7.91749597e-01 -9.52945650e-01 -8.39593887e-01  2.84619778e-01
  9.77977633e-01 -8.75860095e-01  7.82754064e-01 -7.06715405e-01
 -4.06687081e-01  5.18890023e-01  4.67880443e-02  3.53223354e-01
 -4.37214226e-02 -2.08883196e-01  9.29436564e-01 -1.84513062e-01
 -6.65034115e-01  8.53922844e-01 -1.78869963e-01  8.19963038e-01
  5.84212601e-01 -2.28866175e-01  2.31312960e-01  7.84833491e-01
  1.08052492e-01  3.98010254e-01 -6.41333520e-01 -6.47005081e-01
  9.14078891e-01 -5.11660520e-03  5.84843159e-01  1.18692480e-01
 -5.97188771e-01 -7.17348754e-01  9.39676464e-01 -7.90006697e-01
  8.36795866e-01  9.47739959e-01 -5.33035398e-01  5.60236394e-01
 -1.69225544e-01  4.41332728e-01 -7.40779519e-01 -4.17580515e-01
 -5.95755041e-01  8.88479710e-01 -9.75655079e-01 -4.71207231e-01
 -9.72735047e-01  4.66476142e-01 -5.75098693e-01]
xs is  Tensor("ginn_input_layer/strided_slice_2:0", shape=(915,), dtype=float32)
is eager in custom_gradient False
u2_j = Tensor("ginn_input_layer/concat:0", shape=(20,), dtype=float32)
vCS_j is Tensor("ginn_input_layer/Tanh:0", shape=(20,), dtype=float32)
self.vCS is  ListWrapper([<tf.Tensor 'ginn_input_layer/Tanh:0' shape=(20,) dtype=float32>])
xs is  Tensor("ginn_input_layer/strided_slice_45:0", shape=(915,), dtype=float32)
is eager in custom_gradient False
u2_j = Tensor("ginn_input_layer/concat_1:0", shape=(20,), dtype=float32)
vCS_j is Tensor("ginn_input_layer/Tanh_1:0", shape=(20,), dtype=float32)
self.vCS is  ListWrapper([<tf.Tensor 'ginn_input_layer/Tanh:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_1:0' shape=(20,) dtype=float32>])
xs is  Tensor("ginn_input_layer/strided_slice_88:0", shape=(915,), dtype=float32)
is eager in custom_gradient False
u2_j = Tensor("ginn_input_layer/concat_2:0", shape=(20,), dtype=float32)
vCS_j is Tensor("ginn_input_layer/Tanh_2:0", shape=(20,), dtype=float32)
self.vCS is  ListWrapper([<tf.Tensor 'ginn_input_layer/Tanh:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_1:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_2:0' shape=(20,) dtype=float32>])
xs is  Tensor("ginn_input_layer/strided_slice_131:0", shape=(915,), dtype=float32)
is eager in custom_gradient False
u2_j = Tensor("ginn_input_layer/concat_3:0", shape=(20,), dtype=float32)
vCS_j is Tensor("ginn_input_layer/Tanh_3:0", shape=(20,), dtype=float32)
self.vCS is  ListWrapper([<tf.Tensor 'ginn_input_layer/Tanh:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_1:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_2:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_3:0' shape=(20,) dtype=float32>])
xs is  Tensor("ginn_input_layer/strided_slice_174:0", shape=(915,), dtype=float32)
is eager in custom_gradient False
u2_j = Tensor("ginn_input_layer/concat_4:0", shape=(20,), dtype=float32)
vCS_j is Tensor("ginn_input_layer/Tanh_4:0", shape=(20,), dtype=float32)
self.vCS is  ListWrapper([<tf.Tensor 'ginn_input_layer/Tanh:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_1:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_2:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_3:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_4:0' shape=(20,) dtype=float32>])
xs is  Tensor("ginn_input_layer/strided_slice_217:0", shape=(915,), dtype=float32)
is eager in custom_gradient False
u2_j = Tensor("ginn_input_layer/concat_5:0", shape=(20,), dtype=float32)
vCS_j is Tensor("ginn_input_layer/Tanh_5:0", shape=(20,), dtype=float32)
self.vCS is  ListWrapper([<tf.Tensor 'ginn_input_layer/Tanh:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_1:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_2:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_3:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_4:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_5:0' shape=(20,) dtype=float32>])
xs is  Tensor("ginn_input_layer/strided_slice_260:0", shape=(915,), dtype=float32)
is eager in custom_gradient False
u2_j = Tensor("ginn_input_layer/concat_6:0", shape=(20,), dtype=float32)
vCS_j is Tensor("ginn_input_layer/Tanh_6:0", shape=(20,), dtype=float32)
self.vCS is  ListWrapper([<tf.Tensor 'ginn_input_layer/Tanh:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_1:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_2:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_3:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_4:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_5:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_6:0' shape=(20,) dtype=float32>])
xs is  Tensor("ginn_input_layer/strided_slice_303:0", shape=(915,), dtype=float32)
is eager in custom_gradient False
u2_j = Tensor("ginn_input_layer/concat_7:0", shape=(20,), dtype=float32)
vCS_j is Tensor("ginn_input_layer/Tanh_7:0", shape=(20,), dtype=float32)
self.vCS is  ListWrapper([<tf.Tensor 'ginn_input_layer/Tanh:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_1:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_2:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_3:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_4:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_5:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_6:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_7:0' shape=(20,) dtype=float32>])
xs is  Tensor("ginn_input_layer/strided_slice_346:0", shape=(915,), dtype=float32)
is eager in custom_gradient False
u2_j = Tensor("ginn_input_layer/concat_8:0", shape=(20,), dtype=float32)
vCS_j is Tensor("ginn_input_layer/Tanh_8:0", shape=(20,), dtype=float32)
self.vCS is  ListWrapper([<tf.Tensor 'ginn_input_layer/Tanh:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_1:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_2:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_3:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_4:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_5:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_6:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_7:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_8:0' shape=(20,) dtype=float32>])
xs is  Tensor("ginn_input_layer/strided_slice_389:0", shape=(915,), dtype=float32)
is eager in custom_gradient False
u2_j = Tensor("ginn_input_layer/concat_9:0", shape=(20,), dtype=float32)
vCS_j is Tensor("ginn_input_layer/Tanh_9:0", shape=(20,), dtype=float32)
self.vCS is  ListWrapper([<tf.Tensor 'ginn_input_layer/Tanh:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_1:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_2:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_3:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_4:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_5:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_6:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_7:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_8:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/Tanh_9:0' shape=(20,) dtype=float32>])
pre-concatenation vCS is  ListWrapper([<tf.Tensor 'ginn_input_layer/IdentityN:0' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/IdentityN:1' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/IdentityN:2' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/IdentityN:3' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/IdentityN:4' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/IdentityN:5' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/IdentityN:6' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/IdentityN:7' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/IdentityN:8' shape=(20,) dtype=float32>, <tf.Tensor 'ginn_input_layer/IdentityN:9' shape=(20,) dtype=float32>])
post-expand_dims vCS is  Tensor("ExpandDims:0", shape=(10, 1, 20), dtype=float32)
V3 is  Tensor("dense/Tanh:0", shape=(10, 1, 40), dtype=float32)
y is  Tensor("dense_1/Softmax:0", shape=(10, 1, 2), dtype=float32)
self.y in GINN_model call() is Tensor("dense_1/Softmax:0", shape=(10, 1, 2), dtype=float32)
self.y.shape in GINN_model call() is (10, 1, 2)
y_prob_pred is  Tensor("Max:0", shape=(10, 1, 1), dtype=float32)
inputs tf.Tensor(
[[[3. 0. 0. ... 0. 0. 0.]]

 [[3. 0. 0. ... 0. 0. 0.]]

 [[5. 0. 0. ... 0. 0. 0.]]

 ...

 [[1. 0. 0. ... 0. 0. 0.]]

 [[0. 0. 0. ... 0. 0. 0.]]

 [[0. 0. 0. ... 0. 0. 0.]]], shape=(10, 1, 915), dtype=float32)
is eager in GINN_model call True
inputs in call is  tf.Tensor(
[[[3. 0. 0. ... 0. 0. 0.]]

 [[3. 0. 0. ... 0. 0. 0.]]

 [[5. 0. 0. ... 0. 0. 0.]]

 ...

 [[1. 0. 0. ... 0. 0. 0.]]

 [[0. 0. 0. ... 0. 0. 0.]]

 [[0. 0. 0. ... 0. 0. 0.]]], shape=(10, 1, 915), dtype=float32)
is eager in call True
self.K is  20
self.n is  [405 140   1   3  13  84   1   2  10   1  16   3  37   6   8   1   3   2
 177   2]
self.flattened_W is  [ 7.93516159e-01 -1.15385696e-01  2.13972121e-01 -3.56907606e-01
  7.01226830e-01 -4.01539952e-01  6.82839215e-01  7.49478638e-01
 -6.74894750e-01  4.01318759e-01  3.12671304e-01 -6.55154586e-01
  6.73290253e-01 -8.49015772e-01  6.00232422e-01  8.14236701e-01
 -5.97876608e-01 -9.71457064e-01 -1.82775170e-01 -3.08911860e-01
 -7.84737587e-01  4.16453540e-01  9.21217918e-01 -1.32101342e-01
  7.13061213e-01 -9.88716006e-01  4.17406201e-01 -7.27670491e-01
  8.10134292e-01  5.45784235e-01 -7.41476834e-01  5.32974005e-01
  7.80502141e-01  7.22185254e-01 -7.42396355e-01 -4.60910112e-01
  5.70265949e-01 -9.58594739e-01 -7.49043941e-01  6.74115941e-02
  7.92037725e-01  3.64689738e-01 -9.34246480e-01  8.75723720e-01
 -9.64111924e-01 -1.33518279e-01 -1.00408934e-01  6.36220574e-01
  3.92315000e-01 -6.05255246e-01 -2.46369153e-01  9.13717449e-01
 -9.23626304e-01 -4.72151935e-02 -5.45031250e-01  3.17253262e-01
  8.93762827e-01 -8.07591915e-01 -7.49511421e-01  4.51531798e-01
 -1.91506639e-01 -4.52097833e-01  3.62567037e-01  1.56589478e-01
  4.14750785e-01  1.84347361e-01  2.15772212e-01 -2.72486985e-01
 -1.19274467e-01  7.74010122e-01 -1.52535558e-01 -9.25496697e-01
  5.89907289e-01  7.55588591e-01 -7.55951166e-01 -1.51026741e-01
  5.01725554e-01  3.40664059e-01 -2.31372818e-01  1.64513066e-01
 -7.78356194e-01  6.33505225e-01  5.83806515e-01  5.04386872e-02
 -5.64036191e-01 -1.95216954e-01  6.80999339e-01  9.35262978e-01
  8.48973274e-01 -3.33662219e-02  3.12933177e-01 -3.02711755e-01
  6.05760515e-01 -5.96679091e-01  2.00930253e-01 -4.32431012e-01
 -5.00997841e-01  3.53070229e-01 -1.59008369e-01  7.02168941e-01
 -3.91317070e-01 -5.66817880e-01 -2.98116524e-02  9.57965612e-01
 -1.61312863e-01  4.57871675e-01  2.59735376e-01  2.89803594e-01
  4.60936427e-01 -9.42323565e-01  2.03163862e-01 -5.37147522e-01
  6.04791045e-01 -7.52415657e-01  4.67093021e-01  1.20635360e-01
 -2.74869084e-01 -2.34385237e-01  4.97040063e-01 -4.28952664e-01
 -4.06379253e-01 -8.14750493e-01 -6.84553504e-01  8.23036551e-01
  2.75692105e-01  7.16948152e-01 -5.38795412e-01 -3.02735895e-01
  4.35444206e-01 -2.90485263e-01 -8.75433207e-01 -8.51987779e-01
  4.00432408e-01  9.18666661e-01  5.31239621e-02 -3.11867654e-01
  4.95588899e-01  2.33586743e-01 -6.76062405e-01  2.28712693e-01
 -7.14569747e-01  4.94450808e-01  5.05838633e-01 -6.48721337e-01
 -4.92254794e-01 -7.00271010e-01 -5.33242166e-01  2.71822691e-01
 -7.64674902e-01  7.57902205e-01  9.34066176e-02 -4.48642612e-01
 -4.15863484e-01  7.65939593e-01 -8.57596576e-01  9.29914415e-01
 -2.87626714e-01 -7.59094715e-01 -8.91866922e-01 -4.70454484e-01
 -1.49182901e-01 -7.34306455e-01 -2.55929798e-01  6.75577998e-01
 -8.89151454e-01  4.78854328e-01  5.53010941e-01  8.15914989e-01
 -2.92665064e-01 -5.97735405e-01 -7.81376958e-01  2.62847245e-01
 -6.15575969e-01 -6.52716637e-01  3.12758803e-01  8.99402022e-01
  6.64472222e-01  8.34712625e-01  9.13999498e-01 -5.79623096e-02
 -9.12859887e-02  7.16535032e-01  3.99974048e-01 -4.46744263e-01
 -6.80343151e-01 -3.70075822e-01 -1.18865266e-01  8.40933263e-01
  9.54293370e-01  3.51915151e-01  2.70979315e-01  6.91612244e-01
  2.57793099e-01  6.30734712e-02  8.60368907e-01  2.40743145e-01
  7.57942975e-01 -7.93695271e-01  6.81751430e-01  2.06740767e-01
  2.65540302e-01  6.45625949e-01 -7.60298669e-02 -2.71219850e-01
 -1.07950859e-01  4.92492802e-02 -6.99860454e-01 -3.73157382e-01
  4.33491707e-01 -1.70329630e-01  7.54544139e-01  6.19512498e-01
 -2.13935357e-02  2.30903625e-01  6.65790260e-01 -9.21040833e-01
  3.02875489e-01 -3.32171798e-01  8.81671786e-01  8.20830047e-01
 -1.32236302e-01  5.09334028e-01 -3.35963786e-01  2.48345047e-01
 -1.67326048e-01 -6.42568290e-01 -1.75206587e-01 -6.96798265e-01
  5.73418140e-01 -3.09532732e-01  8.05500150e-01 -1.79001167e-01
 -7.56307662e-01  9.80614066e-01 -6.45165145e-01 -4.63108197e-02
 -6.70987666e-01 -5.03446341e-01  7.88571656e-01  3.84187192e-01
  3.76560360e-01  6.14994347e-01 -7.77750537e-02 -6.62579775e-01
 -8.12150419e-01  9.85716045e-01 -4.95815188e-01 -4.99244928e-01
 -6.19847417e-01 -5.81861794e-01 -7.47032344e-01  2.23896444e-01
  8.38207066e-01  4.68075037e-01  1.99287161e-01 -4.10758704e-01
  7.22799003e-01  4.24241662e-01 -4.03945714e-01  4.12813395e-01
  3.76868993e-01 -6.60853028e-01  7.29760647e-01  8.26023102e-01
  9.09006536e-01 -4.90947247e-01  5.72275758e-01  2.57622093e-01
  6.27599418e-01 -2.67918110e-01 -1.93039373e-01 -8.47003818e-01
  4.95854497e-01 -7.79381156e-01 -7.63011396e-01 -7.87712812e-01
  7.14009255e-02 -6.13082170e-01  7.12333322e-01 -6.08763814e-01
 -5.58755994e-01 -7.65175998e-01 -8.04499149e-01  2.68162731e-02
  1.07454203e-01  8.39715898e-01  6.84023619e-01 -9.37083781e-01
  9.29887831e-01  2.59375433e-03 -9.30839963e-03  4.68937516e-01
 -4.10054535e-01  6.61686480e-01  4.79839295e-01  8.31101775e-01
  4.31255490e-01 -5.00757933e-01 -5.96308112e-02  3.05695236e-01
 -9.11173701e-01 -9.03489828e-01  2.10280251e-02 -8.07422996e-01
  7.35139668e-01  1.65286154e-01  1.93287343e-01  6.93862200e-01
  4.08962462e-03  4.51857924e-01 -3.53804082e-01  7.92855978e-01
 -9.33615983e-01  2.37632439e-01  7.69394159e-01 -9.28829253e-01
 -8.47534835e-01  4.86700863e-01 -2.15562042e-02 -8.64588022e-01
 -8.52388799e-01  1.59852386e-01 -5.71861207e-01  1.89441353e-01
  9.53917503e-01 -5.69051504e-01 -1.59345970e-01  4.19490397e-01
  6.39653325e-01 -1.58325449e-01 -2.66265482e-01  7.53009558e-01
  8.02227437e-01 -2.72938788e-01  9.45739076e-02  3.55973452e-01
 -7.62331188e-01 -9.42325965e-02  3.20461057e-02  8.44855785e-01
  9.11553919e-01  7.69554496e-01 -9.38220322e-01 -7.78641403e-01
 -9.16897297e-01  7.75148034e-01  9.21225488e-01 -3.46087694e-01
  1.47081584e-01  8.29313338e-01 -9.84962753e-06  2.35204995e-02
  4.72158909e-01 -3.25753726e-02 -3.82105231e-01  1.19027793e-01
 -2.60956734e-01 -7.89207339e-01  8.87176931e-01  4.44869369e-01
 -9.05357778e-01 -4.94934410e-01 -4.32844073e-01 -5.37272394e-01
  6.18817449e-01  3.71095061e-01  2.14727730e-01  5.12148917e-01
 -5.13655901e-01 -5.16705990e-01 -7.82700896e-01 -1.31557688e-01
 -5.77414930e-01 -5.20610750e-01  2.47468978e-01  3.10324818e-01
 -9.36218381e-01 -6.27615809e-01 -6.47190809e-01  6.79873466e-01
  2.59767830e-01 -1.20428160e-01 -8.03644240e-01  7.71399438e-01
 -6.71058297e-01 -2.66981572e-01 -3.46099734e-02 -6.78539276e-01
  5.33905387e-01 -9.84341979e-01 -1.42501026e-01  8.22666049e-01
  5.26133657e-01  3.57674509e-01 -5.54029524e-01  4.60919648e-01
 -6.79934084e-01  4.92051870e-01 -3.99262518e-01  9.77681518e-01
  6.46438658e-01 -3.31808060e-01  4.31028426e-01  6.32493258e-01
  9.61214960e-01  2.28483185e-01  3.67525131e-01 -5.31227708e-01
  5.62833726e-01 -3.42546672e-01  5.76312482e-01 -3.55331391e-01
  5.05981982e-01 -3.07111889e-01  3.10372800e-01 -9.04191494e-01
  1.19186588e-01 -6.50552690e-01 -8.18181515e-01  7.86605239e-01
  6.77419424e-01  8.27478111e-01 -5.34120612e-02 -3.78601164e-01
 -4.88200694e-01 -8.81703675e-01 -9.08705175e-01  8.58496726e-01
  9.27098572e-01 -9.00791883e-02  7.87826300e-01  8.34187493e-02
  4.58274305e-01  5.65948486e-01 -7.58592963e-01 -9.98876244e-02
 -7.03127086e-01 -3.77307624e-01 -2.27397785e-01 -6.17142022e-01
  9.93997693e-01  9.21337903e-01 -2.83166647e-01  6.52795434e-01
  4.92285863e-02  2.06521768e-02  5.43294191e-01 -4.99895886e-02
  3.39711130e-01 -1.80405691e-01 -3.49055320e-01  2.39553273e-01
  6.83580101e-01 -8.92544329e-01 -7.08269119e-01  6.88179672e-01
 -5.22891104e-01  2.77255863e-01 -5.21658957e-02 -5.62973440e-01
  6.84849143e-01 -7.90948093e-01  4.78244334e-01 -7.91660964e-01
 -8.51218700e-01  7.44482696e-01 -8.49836409e-01 -9.84452903e-01
 -5.63973546e-01  8.04286897e-01 -1.29123569e-01  5.18038094e-01
  3.14814538e-01  2.98627734e-01 -7.20108626e-03 -1.50889456e-01
 -8.12483490e-01 -2.10928842e-01 -2.19156116e-01 -6.79104626e-01
  5.54768443e-01  1.79884955e-01  1.53498739e-01  4.28206146e-01
 -4.01899457e-01  3.26350003e-01 -1.18499734e-01 -2.45337084e-01
 -6.55303359e-01  4.81619507e-01  9.24482524e-01 -7.22549498e-01
  3.39168787e-01 -2.23176498e-02  5.78347921e-01  2.54417229e-02
 -8.04846644e-01  3.70094776e-01  6.64084077e-01 -6.99092388e-01
  5.63493222e-02  2.76599854e-01 -5.60552478e-01 -9.31780934e-01
 -8.61873746e-01 -9.53682661e-01 -4.37726557e-01 -3.82820249e-01
  4.81511146e-01 -7.67710388e-01  7.79068112e-01  4.80325252e-01
  5.09483993e-01  8.49776208e-01  5.66444695e-01  2.63929844e-01
  6.31401598e-01  2.62384444e-01 -5.99204063e-01  1.75366446e-01
  5.63576877e-01  4.21183139e-01  6.29834056e-01 -1.34502545e-01
  8.92880619e-01  8.43460441e-01  1.78625137e-01 -8.74525383e-02
  7.15093553e-01 -1.03573829e-01 -1.10415958e-01 -4.49235320e-01
 -4.04078901e-01  3.00892383e-01  3.32444489e-01  2.21581474e-01
  3.12259644e-02  5.80145359e-01 -7.40136027e-01  4.91394311e-01
  2.29297861e-01  8.67696628e-02  2.49223039e-01 -4.63047117e-01
 -7.89206386e-01  9.47640836e-01 -2.92576820e-01 -6.12265766e-01
 -8.27940047e-01  4.12244678e-01  5.19411922e-01 -2.05165580e-01
 -1.12215534e-01 -5.74243784e-01  7.11920381e-01  2.25884140e-01
 -8.41259599e-01 -6.36766076e-01 -9.23310295e-02 -3.10472786e-01
 -8.44645739e-01  9.47636217e-02 -1.75975546e-01  9.51979637e-01
 -4.30163920e-01 -8.93091917e-01 -2.35014930e-01 -9.73374724e-01
  8.40144753e-01  2.92504638e-01  1.11310557e-01 -9.18948591e-01
 -2.50212044e-01  7.19416499e-01  7.19179571e-01 -7.71073759e-01
  9.86190915e-01  2.12537184e-01 -4.17973325e-02 -8.47433209e-01
 -1.24700047e-01 -4.76794094e-01 -2.99198478e-01 -5.50191058e-03
  8.09562266e-01  7.55835831e-01 -9.40107554e-03 -8.16706836e-01
 -7.15602338e-01 -4.01378125e-01 -3.03894520e-01  2.47593746e-02
 -9.18365240e-01  1.41392658e-02 -8.46523762e-01 -3.61272320e-02
  7.67259359e-01 -8.79878581e-01  3.54704648e-01  8.09604943e-01
 -5.91717005e-01 -8.16509843e-01 -2.30212286e-01  3.53081912e-01
  4.11080271e-01 -7.05946445e-01  7.72230774e-02 -6.03936911e-01
  5.66292167e-01  5.93103588e-01  6.96171820e-01 -1.34702995e-01
 -4.06626582e-01 -9.95727539e-01 -2.43696317e-01  3.32226485e-01
  2.70544440e-01  4.79659081e-01 -2.12323681e-01 -9.67576683e-01
  6.16217852e-02 -7.24134624e-01  2.14993194e-01  9.47745442e-01
 -3.23512316e-01 -1.19472377e-01 -2.99522728e-01  3.88082601e-02
  9.63428542e-02 -6.40858114e-01 -5.96045554e-01 -8.95039737e-02
  8.76676023e-01 -6.92015231e-01 -7.04747915e-01  1.25715109e-02
 -5.30444443e-01  2.75263935e-01  3.99501845e-02 -8.08264256e-01
 -4.07682449e-01  1.55385628e-01  2.97612697e-01 -2.45129615e-01
  7.42328286e-01 -4.77650762e-01  6.92618787e-01  6.91681445e-01
 -1.24203349e-02  8.96921575e-01  8.41405571e-01 -7.88622856e-01
 -6.05597079e-01 -4.20300126e-01 -1.16413616e-01 -3.24636102e-01
 -1.74107701e-02 -6.34151638e-01  8.02538276e-01 -3.72257292e-01
 -4.35904175e-01 -5.87755144e-01 -9.55397427e-01 -2.73766518e-01
 -2.23747715e-01  3.79369229e-01  3.22493166e-01  9.66449916e-01
  9.82773066e-01  6.47633493e-01  6.53691649e-01  7.23368451e-02
 -5.74297547e-01 -4.29185033e-02  7.30818450e-01  1.04165263e-01
  7.71589339e-01 -1.50467068e-01  7.28371978e-01 -4.62551415e-01
  9.31184739e-02 -6.38178170e-01 -1.62047014e-01  7.04480827e-01
 -9.53352034e-01 -7.35463381e-01 -7.63665020e-01  9.53523278e-01
  9.82595742e-01  1.51749790e-01 -1.96608186e-01 -2.67549634e-01
 -5.10963857e-01  8.28903615e-01  4.42490578e-01 -8.40775788e-01
 -8.44293296e-01 -8.03983152e-01 -1.33394981e-02  6.90434575e-01
 -1.73007786e-01  7.79271126e-01 -9.18660998e-01  5.55827133e-02
 -9.84062180e-02  8.56834650e-01  6.29358709e-01  4.73670065e-02
 -8.05251837e-01  7.27132678e-01 -6.92767322e-01  6.06335402e-01
  4.98829901e-01  4.75333244e-01  5.95172882e-01  2.81566590e-01
  8.10948968e-01 -6.86232626e-01  7.67588913e-01 -9.60782111e-01
 -2.29166791e-01 -8.14122319e-01 -7.53189862e-01  8.73016059e-01
 -8.13780963e-01  7.01651454e-01 -1.71414346e-01  2.00813130e-01
 -2.71361619e-01  1.42430425e-01  5.83109399e-03  9.36371505e-01
  8.71648312e-01 -7.45763257e-02  4.63911325e-01  3.35245579e-01
 -9.74325240e-01 -9.05486166e-01  6.23792768e-01  5.84032357e-01
  9.99527454e-01  1.40901580e-01 -9.21331406e-01 -1.96454853e-01
 -4.67264593e-01  3.23344111e-01  6.85002029e-01  4.41983163e-01
  8.39803100e-01 -8.38038549e-02 -2.10069403e-01 -8.16436827e-01
  2.37198070e-01 -2.49484032e-01  5.52244484e-01 -8.91737580e-01
 -8.65937352e-01 -7.17030764e-01 -2.90644504e-02 -7.68740177e-01
 -1.24625161e-01 -9.82306421e-01 -3.96685898e-01  1.00334741e-01
  9.72205475e-02 -2.94238508e-01  4.44057763e-01 -5.01254141e-01
  6.02735102e-01 -7.86399007e-01 -6.97803855e-01 -5.07117033e-01
  6.17018938e-01  2.99585313e-01 -5.99193394e-01 -4.68407601e-01
  1.18298821e-01  5.34882426e-01 -8.73750389e-01 -8.68183821e-02
  5.35658561e-02 -5.87285757e-01 -5.13376474e-01 -2.66507193e-02
  2.42357299e-01 -9.82281148e-01  3.24351996e-01  2.28503332e-01
 -1.39907598e-01 -4.61589009e-01  9.96358171e-02 -1.98858172e-01
  5.28355986e-02  3.55033964e-01  4.13769126e-01 -4.49858606e-01
  4.63773459e-02  1.05900131e-01  2.88647473e-01 -6.84072077e-01
 -8.90591741e-01  3.05030078e-01 -9.31919873e-01 -8.79714847e-01
 -3.68878275e-01  1.86875582e-01  1.99888758e-02 -8.48924100e-01
 -5.87907374e-01 -7.87871420e-01  4.09344316e-01 -2.76625156e-01
  6.66155219e-01 -3.44162658e-02 -1.14109349e-02  5.58116317e-01
 -3.29952925e-01 -4.84653592e-01  8.84273708e-01 -5.64809561e-01
  8.21206391e-01  3.20832044e-01  1.22462921e-01 -2.10871503e-01
 -1.89640686e-01  5.14908016e-01  6.82054520e-01  2.00348005e-01
 -7.34734237e-01  8.06159317e-01  3.35550278e-01  7.54157722e-01
 -9.21684265e-01 -7.14775980e-01  9.87451613e-01 -3.86934951e-02
  9.29499090e-01  9.67469454e-01  5.96027911e-01  3.63380075e-01
  9.99995708e-01 -9.71786857e-01  3.45398277e-01 -5.27924061e-01
  2.21933737e-01  1.64580271e-01 -2.16902327e-02 -3.15749079e-01
 -4.89310950e-01  8.16900134e-02  2.75878385e-02  9.97382700e-01
 -2.51740187e-01 -9.96001959e-01 -8.90745759e-01  3.59113246e-01
 -6.13297999e-01 -5.46035051e-01 -6.41274631e-01 -7.90135145e-01
  3.03470820e-01  4.26194482e-02  8.65728974e-01 -9.78875935e-01
 -7.91749597e-01 -9.52945650e-01 -8.39593887e-01  2.84619778e-01
  9.77977633e-01 -8.75860095e-01  7.82754064e-01 -7.06715405e-01
 -4.06687081e-01  5.18890023e-01  4.67880443e-02  3.53223354e-01
 -4.37214226e-02 -2.08883196e-01  9.29436564e-01 -1.84513062e-01
 -6.65034115e-01  8.53922844e-01 -1.78869963e-01  8.19963038e-01
  5.84212601e-01 -2.28866175e-01  2.31312960e-01  7.84833491e-01
  1.08052492e-01  3.98010254e-01 -6.41333520e-01 -6.47005081e-01
  9.14078891e-01 -5.11660520e-03  5.84843159e-01  1.18692480e-01
 -5.97188771e-01 -7.17348754e-01  9.39676464e-01 -7.90006697e-01
  8.36795866e-01  9.47739959e-01 -5.33035398e-01  5.60236394e-01
 -1.69225544e-01  4.41332728e-01 -7.40779519e-01 -4.17580515e-01
 -5.95755041e-01  8.88479710e-01 -9.75655079e-01 -4.71207231e-01
 -9.72735047e-01  4.66476142e-01 -5.75098693e-01]
xs is  tf.Tensor(
[3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 6. 0. 0.
 0. 0. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 2. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0.], shape=(915,), dtype=float32)
is eager in custom_gradient True
u2_j = tf.Tensor(
[ 1.2607465   0.04325497  0.          0.          0.5194119   0.16981728
  0.          0.          0.          0.          0.93985456  0.
 -0.998152    0.          0.          0.          0.8716483   0.
 -1.0597756   0.        ], shape=(20,), dtype=float32)
vCS_j is tf.Tensor(
[ 0.8512698   0.04322795  0.          0.          0.477246    0.16820349
  0.          0.          0.          0.          0.7351554   0.
 -0.760817    0.          0.          0.          0.7022106   0.
 -0.78557795  0.        ], shape=(20,), dtype=float32)
self.vCS is  ListWrapper([<tf.Tensor: shape=(20,), dtype=float32, numpy=
array([ 0.8512698 ,  0.04322795,  0.        ,  0.        ,  0.477246  ,
        0.16820349,  0.        ,  0.        ,  0.        ,  0.        ,
        0.7351554 ,  0.        , -0.760817  ,  0.        ,  0.        ,
        0.        ,  0.7022106 ,  0.        , -0.78557795,  0.        ],
      dtype=float32)>])
xs is  tf.Tensor(
[3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 2. 0.
 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 2. 0. 0.
 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 3. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 2.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0.], shape=(915,), dtype=float32)
is eager in custom_gradient True
u2_j = tf.Tensor(
[-5.297363   -0.3989312   0.          0.          0.         -0.0176751
  0.          0.         -0.6055971   0.          0.          0.62112224
 -0.918661    0.          0.          0.          0.          0.
 -0.42986974  0.        ], shape=(20,), dtype=float32)
vCS_j is tf.Tensor(
[-0.99994993 -0.37903413  0.          0.          0.         -0.0176733
  0.          0.         -0.54102045  0.          0.          0.55190897
 -0.72526336  0.          0.          0.          0.          0.
 -0.40521252  0.        ], shape=(20,), dtype=float32)
self.vCS is  ListWrapper([<tf.Tensor: shape=(20,), dtype=float32, numpy=
array([ 0.8512698 ,  0.04322795,  0.        ,  0.        ,  0.477246  ,
        0.16820349,  0.        ,  0.        ,  0.        ,  0.        ,
        0.7351554 ,  0.        , -0.760817  ,  0.        ,  0.        ,
        0.        ,  0.7022106 ,  0.        , -0.78557795,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-0.99994993, -0.37903413,  0.        ,  0.        ,  0.        ,
       -0.0176733 ,  0.        ,  0.        , -0.54102045,  0.        ,
        0.        ,  0.55190897, -0.72526336,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.40521252,  0.        ],
      dtype=float32)>])
xs is  tf.Tensor(
[5. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0.
 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.
 0. 0. 0. 2. 2. 5. 1. 0. 3. 1. 0. 3. 3. 4. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 0. 0. 0. 2. 0. 0. 1. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 1. 2.
 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0.], shape=(915,), dtype=float32)
is eager in custom_gradient True
u2_j = tf.Tensor(
[ 5.0449758  -1.1401381   0.          0.          0.09476362  0.26976204
  0.          0.69168144  0.          0.          0.          0.
  0.6293587   0.          0.87301606  0.          0.          0.
 -2.1456423   0.        ], shape=(20,), dtype=float32)
vCS_j is tf.Tensor(
[ 0.999917   -0.81446064  0.          0.          0.09448095  0.26340336
  0.          0.59906113  0.          0.          0.          0.
  0.5576105   0.          0.7029033   0.          0.          0.
 -0.972995    0.        ], shape=(20,), dtype=float32)
self.vCS is  ListWrapper([<tf.Tensor: shape=(20,), dtype=float32, numpy=
array([ 0.8512698 ,  0.04322795,  0.        ,  0.        ,  0.477246  ,
        0.16820349,  0.        ,  0.        ,  0.        ,  0.        ,
        0.7351554 ,  0.        , -0.760817  ,  0.        ,  0.        ,
        0.        ,  0.7022106 ,  0.        , -0.78557795,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-0.99994993, -0.37903413,  0.        ,  0.        ,  0.        ,
       -0.0176733 ,  0.        ,  0.        , -0.54102045,  0.        ,
        0.        ,  0.55190897, -0.72526336,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.40521252,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([ 0.999917  , -0.81446064,  0.        ,  0.        ,  0.09448095,
        0.26340336,  0.        ,  0.59906113,  0.        ,  0.        ,
        0.        ,  0.        ,  0.5576105 ,  0.        ,  0.7029033 ,
        0.        ,  0.        ,  0.        , -0.972995  ,  0.        ],
      dtype=float32)>])
xs is  tf.Tensor(
[30.  0.  0.  2.  0.  0.  1.  1.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  1.  2.
  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  1.  1.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  2.  0.  1.  0.  0.
  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.
  0.  0.  1.  2.  0.  1.  1.  0.  5.  0.  0.  3.  0.  3.  1.  1.  4.  3.
  0.  0.  0.  4.  1.  2.  3.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.
  5.  2.  0.  4.  2.  0.  3.  0.  3.  5.  0.  1.  3.  0.  2.  0.  1.  5.
  1.  3.  1.  3.  0.  1.  2.  0.  4.  0.  3.  5.  3.  3.  0.  6. 11.  8.
 10.  3.  6.  4.  8. 13. 26.  5.  0.  4.  0.  2.  2.  2.  0.  0.  0.  4.
  0.  0.  9.  0.  4.  0.  2.  0.  0.  1.  3.  0.  0.  0.  7.  3.  0.  0.
  0.  0.  0.  1.  0.  0.  0.  0.  0.  2.  0.  0.  0.  0.  2.  0.  2.  0.
  0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  4.  2.  0.  2.  0.  1.  1.
  0.  0.  1.  0.  1.  0.  1.  0.  0.  0.  0.  2.  0.  1.  0.  3.  0.  0.
  0.  0.  2.  0.  0.  0.  0.  0.  0.  3.  5.  3.  0.  1.  0.  0.  0.  0.
  1.  3.  0.  0.  1.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  1.  0.
  0.  0.  0.  1.  0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0.
  0.  0.  0.  1.  1.  1.  1.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.
  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.
  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.
  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  1.
  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  1.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.
  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  1.  1.  1.  0.  0.
  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.
  1.  1.  0.  1.  1.  1.  0.  0.  0.  1.  0.  0.  0.  0.  1.], shape=(915,), dtype=float32)
is eager in custom_gradient True
u2_j = tf.Tensor(
[-10.797419    -1.8438272    0.           0.           0.41224468
  -1.7109946    0.           0.           0.           0.
  -0.21388817   0.           1.0524383    0.           0.08868362
   0.           0.           0.33524558   2.4290395   -0.5750987 ], shape=(20,), dtype=float32)
vCS_j is tf.Tensor(
[-1.         -0.95116127  0.          0.          0.39037699 -0.93676955
  0.          0.          0.          0.         -0.21068515  0.
  0.78275245  0.          0.08845185  0.          0.          0.32322627
  0.9845889  -0.519094  ], shape=(20,), dtype=float32)
self.vCS is  ListWrapper([<tf.Tensor: shape=(20,), dtype=float32, numpy=
array([ 0.8512698 ,  0.04322795,  0.        ,  0.        ,  0.477246  ,
        0.16820349,  0.        ,  0.        ,  0.        ,  0.        ,
        0.7351554 ,  0.        , -0.760817  ,  0.        ,  0.        ,
        0.        ,  0.7022106 ,  0.        , -0.78557795,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-0.99994993, -0.37903413,  0.        ,  0.        ,  0.        ,
       -0.0176733 ,  0.        ,  0.        , -0.54102045,  0.        ,
        0.        ,  0.55190897, -0.72526336,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.40521252,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([ 0.999917  , -0.81446064,  0.        ,  0.        ,  0.09448095,
        0.26340336,  0.        ,  0.59906113,  0.        ,  0.        ,
        0.        ,  0.        ,  0.5576105 ,  0.        ,  0.7029033 ,
        0.        ,  0.        ,  0.        , -0.972995  ,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-1.        , -0.95116127,  0.        ,  0.        ,  0.39037699,
       -0.93676955,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.21068515,  0.        ,  0.78275245,  0.        ,  0.08845185,
        0.        ,  0.        ,  0.32322627,  0.9845889 , -0.519094  ],
      dtype=float32)>])
xs is  tf.Tensor(
[47.  0.  0.  0.  0.  2.  1.  0.  2.  1.  1.  1.  2.  2.  0.  2.  1.  0.
  2.  0.  2.  0.  0.  2.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  2.  2.  1.  0.
  1.  2.  2.  2.  2.  2.  2.  1.  0.  0.  2.  1.  1.  2.  1.  1.  1.  1.
  1.  2.  2.  2.  2.  2.  2.  2.  2.  0.  1.  2.  2.  0.  0.  1.  1.  2.
  2.  2.  0.  0.  2.  2.  2.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  1.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  1.  1.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.
  1.  0.  0.  0.  1.  2.  5.  0.  0.  3.  6.  0.  0.  3.  2.  0.  0.  3.
  5.  0.  0.  2.  1.  3.  5.  6.  1.  2.  4.  0.  4.  0.  0.  5.  0.  0.
  0.  2.  5.  0.  2.  4.  0.  2.  1.  0.  2.  5.  2.  5.  4.  0.  5.  0.
  0.  2.  2. 10. 11.  7. 12.  0.  0.  0.  8.  9.  5.  8.  0. 14.  9. 16.
 13.  9. 19. 25. 21. 20. 39.  6.  0.  8.  9.  5.  5.  4.  8.  8.  8.  3.
  8.  0.  1.  0.  5.  0.  5. 11.  0.  2.  6.  2.  0. 12.  5.  2.  0.  0.
  3.  0.  0.  0.  0.  0.  4.  0.  4.  1.  0.  2.  3.  4.  1.  3.  0.  0.
  0.  1.  0.  0.  0.  0.  3.  0.  0.  4.  0.  0.  0.  0.  2.  1.  2.  0.
  0.  2.  0.  1.  1.  0.  1.  4.  0.  4.  0.  1.  0.  0.  0.  0.  0.  0.
  0.  0.  2.  4.  0.  0.  1.  0.  2.  2.  0.  1.  0.  0.  2.  0.  0.  0.
  3.  0.  0.  0.  1.  1.  1.  0.  3.  2.  1.  0.  2.  4.  0.  3.  1.  3.
  2.  2.  0.  2.  0.  1.  3.  0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.
  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  1.  1.
  1.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  1.  1.  1.  1.  0.  1.  1.  0.  1.  0.  1.  0.  1.  1.  1.
  0.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.
  0.  0.  0.  0.  0.  1.  1.  1.  0.  0.  0.  1.  1.  1.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  1.
  0.  1.  0.  1.  0.  1.  0.  0.  0.  2.  2.  1.  0.  0.  0.  0.  0.  0.
  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.
  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.
  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.
  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  1.  1.  0.  1.  0.  0.
  1.  1.  0.  0.  0.  0.  1.  1.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0.
  0.  1.  0.  0.  0.  0.  1.  1.  1.  0.  0.  1.  0.  0.  0.  0.  1.  0.
  0.  1.  1.  1.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.
  0.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  1.  1.  0.  1.  0.
  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  0.
  1.  0.  0.  1.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.], shape=(915,), dtype=float32)
is eager in custom_gradient True
u2_j = tf.Tensor(
[-3.3917189  -3.4054146   0.          0.         -1.4155034  -3.0246034
  0.6926188   0.         -0.42030013  0.          0.45170608  0.
  0.26011807 -0.6862326  -0.27136162  0.          0.          0.
  0.1687991   0.        ], shape=(20,), dtype=float32)
vCS_j is tf.Tensor(
[-0.9977378  -0.9977989   0.          0.         -0.88865715 -0.9952916
  0.59966165  0.         -0.39718327  0.          0.4233004   0.
  0.25440595 -0.5955564  -0.2648914   0.          0.          0.
  0.16721396  0.        ], shape=(20,), dtype=float32)
self.vCS is  ListWrapper([<tf.Tensor: shape=(20,), dtype=float32, numpy=
array([ 0.8512698 ,  0.04322795,  0.        ,  0.        ,  0.477246  ,
        0.16820349,  0.        ,  0.        ,  0.        ,  0.        ,
        0.7351554 ,  0.        , -0.760817  ,  0.        ,  0.        ,
        0.        ,  0.7022106 ,  0.        , -0.78557795,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-0.99994993, -0.37903413,  0.        ,  0.        ,  0.        ,
       -0.0176733 ,  0.        ,  0.        , -0.54102045,  0.        ,
        0.        ,  0.55190897, -0.72526336,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.40521252,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([ 0.999917  , -0.81446064,  0.        ,  0.        ,  0.09448095,
        0.26340336,  0.        ,  0.59906113,  0.        ,  0.        ,
        0.        ,  0.        ,  0.5576105 ,  0.        ,  0.7029033 ,
        0.        ,  0.        ,  0.        , -0.972995  ,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-1.        , -0.95116127,  0.        ,  0.        ,  0.39037699,
       -0.93676955,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.21068515,  0.        ,  0.78275245,  0.        ,  0.08845185,
        0.        ,  0.        ,  0.32322627,  0.9845889 , -0.519094  ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-0.9977378 , -0.9977989 ,  0.        ,  0.        , -0.88865715,
       -0.9952916 ,  0.59966165,  0.        , -0.39718327,  0.        ,
        0.4233004 ,  0.        ,  0.25440595, -0.5955564 , -0.2648914 ,
        0.        ,  0.        ,  0.        ,  0.16721396,  0.        ],
      dtype=float32)>])
xs is  tf.Tensor(
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 6. 0. 0.
 0. 0. 4. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0.], shape=(915,), dtype=float32)
is eager in custom_gradient True
u2_j = tf.Tensor(
[-2.0873263   0.          0.          0.          0.         -0.9675767
  0.          0.          0.          0.         -0.43590418  0.
  0.          0.          0.          0.          0.          0.
  0.94582325  0.        ], shape=(20,), dtype=float32)
vCS_j is tf.Tensor(
[-0.9697049   0.          0.          0.          0.         -0.7476374
  0.          0.          0.          0.         -0.41024375  0.
  0.          0.          0.          0.          0.          0.
  0.7378863   0.        ], shape=(20,), dtype=float32)
self.vCS is  ListWrapper([<tf.Tensor: shape=(20,), dtype=float32, numpy=
array([ 0.8512698 ,  0.04322795,  0.        ,  0.        ,  0.477246  ,
        0.16820349,  0.        ,  0.        ,  0.        ,  0.        ,
        0.7351554 ,  0.        , -0.760817  ,  0.        ,  0.        ,
        0.        ,  0.7022106 ,  0.        , -0.78557795,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-0.99994993, -0.37903413,  0.        ,  0.        ,  0.        ,
       -0.0176733 ,  0.        ,  0.        , -0.54102045,  0.        ,
        0.        ,  0.55190897, -0.72526336,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.40521252,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([ 0.999917  , -0.81446064,  0.        ,  0.        ,  0.09448095,
        0.26340336,  0.        ,  0.59906113,  0.        ,  0.        ,
        0.        ,  0.        ,  0.5576105 ,  0.        ,  0.7029033 ,
        0.        ,  0.        ,  0.        , -0.972995  ,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-1.        , -0.95116127,  0.        ,  0.        ,  0.39037699,
       -0.93676955,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.21068515,  0.        ,  0.78275245,  0.        ,  0.08845185,
        0.        ,  0.        ,  0.32322627,  0.9845889 , -0.519094  ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-0.9977378 , -0.9977989 ,  0.        ,  0.        , -0.88865715,
       -0.9952916 ,  0.59966165,  0.        , -0.39718327,  0.        ,
        0.4233004 ,  0.        ,  0.25440595, -0.5955564 , -0.2648914 ,
        0.        ,  0.        ,  0.        ,  0.16721396,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-0.9697049 ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.7476374 ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.41024375,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.7378863 ,  0.        ],
      dtype=float32)>])
xs is  tf.Tensor(
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0.], shape=(915,), dtype=float32)
is eager in custom_gradient True
u2_j = tf.Tensor(
[0.23475096 0.         0.         0.         0.         0.15614676
 0.         0.         0.8414056  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ], shape=(20,), dtype=float32)
vCS_j is tf.Tensor(
[0.23053169 0.         0.         0.         0.         0.15488996
 0.         0.         0.6865528  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ], shape=(20,), dtype=float32)
self.vCS is  ListWrapper([<tf.Tensor: shape=(20,), dtype=float32, numpy=
array([ 0.8512698 ,  0.04322795,  0.        ,  0.        ,  0.477246  ,
        0.16820349,  0.        ,  0.        ,  0.        ,  0.        ,
        0.7351554 ,  0.        , -0.760817  ,  0.        ,  0.        ,
        0.        ,  0.7022106 ,  0.        , -0.78557795,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-0.99994993, -0.37903413,  0.        ,  0.        ,  0.        ,
       -0.0176733 ,  0.        ,  0.        , -0.54102045,  0.        ,
        0.        ,  0.55190897, -0.72526336,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.40521252,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([ 0.999917  , -0.81446064,  0.        ,  0.        ,  0.09448095,
        0.26340336,  0.        ,  0.59906113,  0.        ,  0.        ,
        0.        ,  0.        ,  0.5576105 ,  0.        ,  0.7029033 ,
        0.        ,  0.        ,  0.        , -0.972995  ,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-1.        , -0.95116127,  0.        ,  0.        ,  0.39037699,
       -0.93676955,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.21068515,  0.        ,  0.78275245,  0.        ,  0.08845185,
        0.        ,  0.        ,  0.32322627,  0.9845889 , -0.519094  ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-0.9977378 , -0.9977989 ,  0.        ,  0.        , -0.88865715,
       -0.9952916 ,  0.59966165,  0.        , -0.39718327,  0.        ,
        0.4233004 ,  0.        ,  0.25440595, -0.5955564 , -0.2648914 ,
        0.        ,  0.        ,  0.        ,  0.16721396,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-0.9697049 ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.7476374 ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.41024375,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.7378863 ,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([0.23053169, 0.        , 0.        , 0.        , 0.        ,
       0.15488996, 0.        , 0.        , 0.6865528 , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ],
      dtype=float32)>])
xs is  tf.Tensor(
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 3. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0.], shape=(915,), dtype=float32)
is eager in custom_gradient True
u2_j = tf.Tensor(
[ 3.9919424  -2.1613743   0.          0.          0.          0.35470465
  0.          0.          0.          0.          0.          0.
  0.05558271  0.          0.          0.          0.          0.
 -0.513422    0.        ], shape=(20,), dtype=float32)
vCS_j is tf.Tensor(
[ 0.9993185  -0.9738205   0.          0.          0.          0.3405412
  0.          0.          0.          0.          0.          0.
  0.05552552  0.          0.          0.          0.          0.
 -0.4726072   0.        ], shape=(20,), dtype=float32)
self.vCS is  ListWrapper([<tf.Tensor: shape=(20,), dtype=float32, numpy=
array([ 0.8512698 ,  0.04322795,  0.        ,  0.        ,  0.477246  ,
        0.16820349,  0.        ,  0.        ,  0.        ,  0.        ,
        0.7351554 ,  0.        , -0.760817  ,  0.        ,  0.        ,
        0.        ,  0.7022106 ,  0.        , -0.78557795,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-0.99994993, -0.37903413,  0.        ,  0.        ,  0.        ,
       -0.0176733 ,  0.        ,  0.        , -0.54102045,  0.        ,
        0.        ,  0.55190897, -0.72526336,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.40521252,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([ 0.999917  , -0.81446064,  0.        ,  0.        ,  0.09448095,
        0.26340336,  0.        ,  0.59906113,  0.        ,  0.        ,
        0.        ,  0.        ,  0.5576105 ,  0.        ,  0.7029033 ,
        0.        ,  0.        ,  0.        , -0.972995  ,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-1.        , -0.95116127,  0.        ,  0.        ,  0.39037699,
       -0.93676955,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.21068515,  0.        ,  0.78275245,  0.        ,  0.08845185,
        0.        ,  0.        ,  0.32322627,  0.9845889 , -0.519094  ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-0.9977378 , -0.9977989 ,  0.        ,  0.        , -0.88865715,
       -0.9952916 ,  0.59966165,  0.        , -0.39718327,  0.        ,
        0.4233004 ,  0.        ,  0.25440595, -0.5955564 , -0.2648914 ,
        0.        ,  0.        ,  0.        ,  0.16721396,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-0.9697049 ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.7476374 ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.41024375,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.7378863 ,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([0.23053169, 0.        , 0.        , 0.        , 0.        ,
       0.15488996, 0.        , 0.        , 0.6865528 , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([ 0.9993185 , -0.9738205 ,  0.        ,  0.        ,  0.        ,
        0.3405412 ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.05552552,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.4726072 ,  0.        ],
      dtype=float32)>])
xs is  tf.Tensor(
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0.], shape=(915,), dtype=float32)
is eager in custom_gradient True
u2_j = tf.Tensor(
[1.4625696  0.6037896  0.         0.         0.         0.56629217
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.        ], shape=(20,), dtype=float32)
vCS_j is tf.Tensor(
[0.8981505 0.5397407 0.        0.        0.        0.512631  0.
 0.        0.        0.        0.        0.        0.        0.
 0.        0.        0.        0.        0.        0.       ], shape=(20,), dtype=float32)
self.vCS is  ListWrapper([<tf.Tensor: shape=(20,), dtype=float32, numpy=
array([ 0.8512698 ,  0.04322795,  0.        ,  0.        ,  0.477246  ,
        0.16820349,  0.        ,  0.        ,  0.        ,  0.        ,
        0.7351554 ,  0.        , -0.760817  ,  0.        ,  0.        ,
        0.        ,  0.7022106 ,  0.        , -0.78557795,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-0.99994993, -0.37903413,  0.        ,  0.        ,  0.        ,
       -0.0176733 ,  0.        ,  0.        , -0.54102045,  0.        ,
        0.        ,  0.55190897, -0.72526336,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.40521252,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([ 0.999917  , -0.81446064,  0.        ,  0.        ,  0.09448095,
        0.26340336,  0.        ,  0.59906113,  0.        ,  0.        ,
        0.        ,  0.        ,  0.5576105 ,  0.        ,  0.7029033 ,
        0.        ,  0.        ,  0.        , -0.972995  ,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-1.        , -0.95116127,  0.        ,  0.        ,  0.39037699,
       -0.93676955,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.21068515,  0.        ,  0.78275245,  0.        ,  0.08845185,
        0.        ,  0.        ,  0.32322627,  0.9845889 , -0.519094  ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-0.9977378 , -0.9977989 ,  0.        ,  0.        , -0.88865715,
       -0.9952916 ,  0.59966165,  0.        , -0.39718327,  0.        ,
        0.4233004 ,  0.        ,  0.25440595, -0.5955564 , -0.2648914 ,
        0.        ,  0.        ,  0.        ,  0.16721396,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-0.9697049 ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.7476374 ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.41024375,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.7378863 ,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([0.23053169, 0.        , 0.        , 0.        , 0.        ,
       0.15488996, 0.        , 0.        , 0.6865528 , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([ 0.9993185 , -0.9738205 ,  0.        ,  0.        ,  0.        ,
        0.3405412 ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.05552552,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.4726072 ,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([0.8981505, 0.5397407, 0.       , 0.       , 0.       , 0.512631 ,
       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,
       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,
       0.       , 0.       ], dtype=float32)>])
xs is  tf.Tensor(
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0.], shape=(915,), dtype=float32)
is eager in custom_gradient True
u2_j = tf.Tensor(
[-1.6940076   0.          0.          0.          0.          0.
  0.          0.         -0.11641362  0.          0.          0.
 -1.6079663   0.          0.          0.          0.          0.
  0.          0.        ], shape=(20,), dtype=float32)
vCS_j is tf.Tensor(
[-0.9346557  0.         0.         0.         0.         0.
  0.         0.        -0.1158906  0.         0.         0.
 -0.922859   0.         0.         0.         0.         0.
  0.         0.       ], shape=(20,), dtype=float32)
self.vCS is  ListWrapper([<tf.Tensor: shape=(20,), dtype=float32, numpy=
array([ 0.8512698 ,  0.04322795,  0.        ,  0.        ,  0.477246  ,
        0.16820349,  0.        ,  0.        ,  0.        ,  0.        ,
        0.7351554 ,  0.        , -0.760817  ,  0.        ,  0.        ,
        0.        ,  0.7022106 ,  0.        , -0.78557795,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-0.99994993, -0.37903413,  0.        ,  0.        ,  0.        ,
       -0.0176733 ,  0.        ,  0.        , -0.54102045,  0.        ,
        0.        ,  0.55190897, -0.72526336,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.40521252,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([ 0.999917  , -0.81446064,  0.        ,  0.        ,  0.09448095,
        0.26340336,  0.        ,  0.59906113,  0.        ,  0.        ,
        0.        ,  0.        ,  0.5576105 ,  0.        ,  0.7029033 ,
        0.        ,  0.        ,  0.        , -0.972995  ,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-1.        , -0.95116127,  0.        ,  0.        ,  0.39037699,
       -0.93676955,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.21068515,  0.        ,  0.78275245,  0.        ,  0.08845185,
        0.        ,  0.        ,  0.32322627,  0.9845889 , -0.519094  ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-0.9977378 , -0.9977989 ,  0.        ,  0.        , -0.88865715,
       -0.9952916 ,  0.59966165,  0.        , -0.39718327,  0.        ,
        0.4233004 ,  0.        ,  0.25440595, -0.5955564 , -0.2648914 ,
        0.        ,  0.        ,  0.        ,  0.16721396,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-0.9697049 ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.7476374 ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.41024375,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.7378863 ,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([0.23053169, 0.        , 0.        , 0.        , 0.        ,
       0.15488996, 0.        , 0.        , 0.6865528 , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([ 0.9993185 , -0.9738205 ,  0.        ,  0.        ,  0.        ,
        0.3405412 ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.05552552,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.4726072 ,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([0.8981505, 0.5397407, 0.       , 0.       , 0.       , 0.512631 ,
       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,
       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,
       0.       , 0.       ], dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-0.9346557,  0.       ,  0.       ,  0.       ,  0.       ,
        0.       ,  0.       ,  0.       , -0.1158906,  0.       ,
        0.       ,  0.       , -0.922859 ,  0.       ,  0.       ,
        0.       ,  0.       ,  0.       ,  0.       ,  0.       ],
      dtype=float32)>])
all_inputs is  [<tf.Tensor: shape=(10, 1, 915), dtype=float32, numpy=
array([[[3., 0., 0., ..., 0., 0., 0.]],

       [[3., 0., 0., ..., 0., 0., 0.]],

       [[5., 0., 0., ..., 0., 0., 0.]],

       ...,

       [[1., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>]
pre-concatenation vCS is  ListWrapper([<tf.Tensor: shape=(20,), dtype=float32, numpy=
array([ 0.8512698 ,  0.04322795,  0.        ,  0.        ,  0.477246  ,
        0.16820349,  0.        ,  0.        ,  0.        ,  0.        ,
        0.7351554 ,  0.        , -0.760817  ,  0.        ,  0.        ,
        0.        ,  0.7022106 ,  0.        , -0.78557795,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-0.99994993, -0.37903413,  0.        ,  0.        ,  0.        ,
       -0.0176733 ,  0.        ,  0.        , -0.54102045,  0.        ,
        0.        ,  0.55190897, -0.72526336,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.40521252,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([ 0.999917  , -0.81446064,  0.        ,  0.        ,  0.09448095,
        0.26340336,  0.        ,  0.59906113,  0.        ,  0.        ,
        0.        ,  0.        ,  0.5576105 ,  0.        ,  0.7029033 ,
        0.        ,  0.        ,  0.        , -0.972995  ,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-1.        , -0.95116127,  0.        ,  0.        ,  0.39037699,
       -0.93676955,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.21068515,  0.        ,  0.78275245,  0.        ,  0.08845185,
        0.        ,  0.        ,  0.32322627,  0.9845889 , -0.519094  ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-0.9977378 , -0.9977989 ,  0.        ,  0.        , -0.88865715,
       -0.9952916 ,  0.59966165,  0.        , -0.39718327,  0.        ,
        0.4233004 ,  0.        ,  0.25440595, -0.5955564 , -0.2648914 ,
        0.        ,  0.        ,  0.        ,  0.16721396,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-0.9697049 ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.7476374 ,  0.        ,  0.        ,  0.        ,  0.        ,
       -0.41024375,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.7378863 ,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([0.23053169, 0.        , 0.        , 0.        , 0.        ,
       0.15488996, 0.        , 0.        , 0.6865528 , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([ 0.9993185 , -0.9738205 ,  0.        ,  0.        ,  0.        ,
        0.3405412 ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.05552552,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        , -0.4726072 ,  0.        ],
      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([0.8981505, 0.5397407, 0.       , 0.       , 0.       , 0.512631 ,
       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,
       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,
       0.       , 0.       ], dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=
array([-0.9346557,  0.       ,  0.       ,  0.       ,  0.       ,
        0.       ,  0.       ,  0.       , -0.1158906,  0.       ,
        0.       ,  0.       , -0.922859 ,  0.       ,  0.       ,
        0.       ,  0.       ,  0.       ,  0.       ,  0.       ],
      dtype=float32)>])
post-expand_dims vCS is  tf.Tensor(
[[[ 0.8512698   0.04322795  0.          0.          0.477246
    0.16820349  0.          0.          0.          0.
    0.7351554   0.         -0.760817    0.          0.
    0.          0.7022106   0.         -0.78557795  0.        ]]

 [[-0.99994993 -0.37903413  0.          0.          0.
   -0.0176733   0.          0.         -0.54102045  0.
    0.          0.55190897 -0.72526336  0.          0.
    0.          0.          0.         -0.40521252  0.        ]]

 [[ 0.999917   -0.81446064  0.          0.          0.09448095
    0.26340336  0.          0.59906113  0.          0.
    0.          0.          0.5576105   0.          0.7029033
    0.          0.          0.         -0.972995    0.        ]]

 [[-1.         -0.95116127  0.          0.          0.39037699
   -0.93676955  0.          0.          0.          0.
   -0.21068515  0.          0.78275245  0.          0.08845185
    0.          0.          0.32322627  0.9845889  -0.519094  ]]

 [[-0.9977378  -0.9977989   0.          0.         -0.88865715
   -0.9952916   0.59966165  0.         -0.39718327  0.
    0.4233004   0.          0.25440595 -0.5955564  -0.2648914
    0.          0.          0.          0.16721396  0.        ]]

 [[-0.9697049   0.          0.          0.          0.
   -0.7476374   0.          0.          0.          0.
   -0.41024375  0.          0.          0.          0.
    0.          0.          0.          0.7378863   0.        ]]

 [[ 0.23053169  0.          0.          0.          0.
    0.15488996  0.          0.          0.6865528   0.
    0.          0.          0.          0.          0.
    0.          0.          0.          0.          0.        ]]

 [[ 0.9993185  -0.9738205   0.          0.          0.
    0.3405412   0.          0.          0.          0.
    0.          0.          0.05552552  0.          0.
    0.          0.          0.         -0.4726072   0.        ]]

 [[ 0.8981505   0.5397407   0.          0.          0.
    0.512631    0.          0.          0.          0.
    0.          0.          0.          0.          0.
    0.          0.          0.          0.          0.        ]]

 [[-0.9346557   0.          0.          0.          0.
    0.          0.          0.         -0.1158906   0.
    0.          0.         -0.922859    0.          0.
    0.          0.          0.          0.          0.        ]]], shape=(10, 1, 20), dtype=float32)
V3 is  tf.Tensor(
[[[-1.17460713e-02 -6.61793202e-02 -4.83387858e-02  2.76923110e-03
   -2.55997442e-02 -5.23298904e-02 -1.84428468e-01 -3.94352414e-02
    5.95881194e-02 -8.61344561e-02 -9.31251943e-02 -5.19499332e-02
    1.59560233e-01 -5.27494252e-02  1.09845608e-01 -8.51696208e-02
    8.63527954e-02  7.63682276e-02 -9.80373695e-02  1.04629889e-01
   -9.55683887e-02 -1.86414085e-02 -3.30321305e-02  1.09533556e-01
   -1.50224105e-01  5.54566495e-02 -1.84145719e-02 -5.69205880e-02
   -5.41189872e-02 -1.35012880e-01 -1.81394830e-01 -1.53701585e-02
    8.56024027e-03 -4.71307372e-04  1.43869027e-01  3.02920258e-03
   -7.98596442e-02 -2.06049681e-01 -3.82146202e-02  1.32734384e-02]]

 [[ 6.67023584e-02 -4.12558094e-02 -7.26742810e-03 -1.02629840e-01
   -3.21156606e-02  4.52373698e-02  1.62423775e-02  2.61741430e-02
   -4.38162647e-02 -6.09892830e-02  1.40505016e-01 -5.63446917e-02
   -2.98581347e-02  4.34691682e-02 -1.28807396e-01 -5.82364760e-02
   -4.67638345e-03 -1.61684509e-02 -5.91059625e-02 -5.83152771e-02
    6.20512404e-02  6.25770018e-02 -3.34972776e-02 -4.12993357e-02
    8.65841575e-04  5.72503060e-02  6.07314287e-03  7.59419724e-02
    9.67039317e-02  8.22476431e-05 -2.30134819e-02  2.80829170e-03
    4.86835418e-03 -4.71022585e-03 -3.83707359e-02 -7.46233687e-02
   -6.28151968e-02 -7.04457760e-02  4.87332381e-02 -3.51093896e-02]]

 [[-7.28851333e-02 -9.17629749e-02 -4.92795743e-02  4.29768413e-02
   -4.65163551e-02 -6.92767790e-03 -5.14321849e-02  1.17760867e-01
    1.53722644e-01 -7.16154650e-02 -2.87468940e-01 -2.50794172e-01
   -7.68493488e-02 -1.32551612e-02  4.93378602e-02  3.66004859e-03
    1.90644898e-03  7.46844262e-02 -1.23041250e-01  6.99523017e-02
   -1.40996456e-01  5.75609058e-02 -5.33435643e-02  6.89789131e-02
    4.26208554e-03 -4.52088639e-02  7.08428323e-02 -1.00055113e-01
   -9.23087969e-02 -1.43436000e-01 -1.46818057e-01 -6.38104677e-02
   -4.75824364e-02  9.22784433e-02 -8.28523748e-03  5.03957868e-02
   -2.30991691e-02 -1.19295409e-02  2.47499440e-02  1.52440043e-02]]

 [[-2.95720305e-02  1.77543104e-01  3.68619338e-02 -2.70909257e-02
   -3.88551056e-02  1.32204056e-01  4.99175563e-02  1.46651044e-01
   -1.43856928e-01  1.04696974e-01  1.95742529e-02 -4.46101390e-02
   -1.80143937e-01  7.77077153e-02 -3.85176986e-02  2.10794285e-01
   -1.41105309e-01 -1.70252714e-02  1.10571451e-01  1.29776150e-02
    1.87457219e-01 -4.05690186e-02  4.24512476e-03 -5.58830798e-02
   -3.07301320e-02 -5.75521365e-02  1.92436635e-01  3.43692079e-02
    8.14199969e-02  1.76004782e-01 -9.05679539e-03 -1.29313888e-02
   -6.24062419e-02 -1.22562714e-01 -6.43016919e-02 -7.44794495e-03
    6.45196065e-02  2.54711956e-01  4.35312837e-02 -7.59135932e-02]]

 [[-1.82560571e-02  5.68818264e-02  1.58922955e-01 -1.43584073e-01
   -1.08581610e-01  1.57522231e-01  1.63721383e-01  1.24546833e-01
   -6.08207211e-02 -7.33149722e-02  2.65705045e-02 -2.00866178e-01
   -1.18805192e-01  1.57544613e-01 -9.76181254e-02  8.84934887e-03
   -4.96244133e-02  7.15213791e-02  1.19650714e-01 -1.08951740e-01
   -2.47531161e-02  1.35109097e-01 -7.79104233e-02 -6.85940757e-02
    7.24000763e-03  6.42238632e-02 -9.52620246e-03 -6.80867434e-02
    1.14160053e-01 -1.75244920e-02  4.01064642e-02  8.80991742e-02
   -1.23574786e-01 -3.11384499e-02 -5.37969731e-02  1.01914108e-02
   -2.32247431e-02  1.66919343e-02  5.30688874e-02 -1.53557286e-01]]

 [[-4.47201915e-03  7.29181916e-02  8.42548907e-02 -1.07860574e-02
   -8.92135501e-03  7.68996999e-02  1.01027228e-01  5.28448410e-02
   -1.29505008e-01  1.25533462e-01  1.21322587e-01  8.95622224e-02
   -4.51708995e-02  1.60300527e-02 -8.62327069e-02  1.13038249e-01
   -6.82347268e-02 -4.71822694e-02  1.02012277e-01 -4.19063978e-02
    1.41200453e-01 -1.76520981e-02  8.85859504e-02 -7.43057579e-02
    3.16892751e-02  3.71167585e-02  1.51596069e-02  9.05686989e-02
    5.39947413e-02  8.10183361e-02  6.58714920e-02  1.29002621e-02
    1.64789967e-02 -6.33300096e-02 -3.98378521e-02 -8.59359279e-02
   -5.21335751e-03  1.14085875e-01 -2.11320873e-02 -6.99229762e-02]]

 [[ 3.04110777e-02 -2.23640958e-03 -1.13139171e-02  4.97360006e-02
    5.26426211e-02 -4.72240672e-02  4.20937762e-02 -5.24776019e-02
    3.92235331e-02  3.33787985e-02 -6.66585043e-02  6.98019227e-04
    2.23777462e-02 -5.48492931e-02  5.02274744e-02  3.29045691e-02
   -1.04353195e-02 -3.29335779e-02  4.70837504e-02  8.04477558e-02
   -4.37212326e-02  4.49328125e-03  2.13953909e-02  5.29482961e-02
   -5.08387163e-02 -6.93514105e-03  9.83937993e-04 -2.73544528e-02
   -2.51987129e-02 -4.10952419e-02 -9.18708183e-03 -6.19898643e-03
    6.24157814e-03  1.64929498e-02 -1.23762405e-02  3.33634838e-02
    8.32067281e-02  2.61422619e-02 -2.45463215e-02  2.16436330e-02]]

 [[ 8.02304409e-03 -6.38375431e-02  7.16140028e-03  5.52886389e-02
   -3.66966240e-02 -4.69504148e-02 -1.82403605e-02  6.47256449e-02
    1.71900570e-01 -5.49311675e-02 -1.92216858e-01 -1.83949038e-01
    5.26879318e-02 -8.80493689e-03  2.59279143e-02 -6.07255213e-02
    4.69844192e-02  4.82823290e-02 -1.17483605e-02  7.10759237e-02
   -3.75757515e-02  2.23877728e-02 -8.18715468e-02  5.40814660e-02
   -7.38027096e-02 -2.88998671e-02  6.56047091e-02 -7.09445849e-02
   -2.00936142e-02 -1.18342876e-01 -1.57117546e-01 -1.00056589e-01
   -1.01487063e-01  8.82128254e-02  2.84568942e-03  7.80671537e-02
   -2.78181843e-02 -5.09631634e-02  2.68532149e-02  4.63219956e-02]]

 [[-2.28229184e-02 -1.74886696e-02 -4.14432548e-02  4.29131314e-02
    8.85508582e-03 -4.62620966e-02 -3.37723158e-02 -5.87965995e-02
    7.76138157e-02  3.43627995e-03 -2.85267681e-02  5.30909598e-02
    6.25433698e-02 -4.30321842e-02  6.13946915e-02 -4.08942029e-02
    5.49105294e-02  1.15006650e-02 -6.10852391e-02  2.06112657e-02
   -2.82252692e-02 -2.96620410e-02 -4.75105131e-03  1.88103858e-02
   -2.48751370e-03 -2.39459090e-02 -7.24641308e-02 -4.12878357e-02
   -8.16821307e-02 -4.51385118e-02 -4.53454256e-02 -1.89444888e-02
    3.39058042e-02  1.88717265e-02  3.06045823e-02  4.41607200e-02
   -1.62344966e-02 -4.97661121e-02  6.10711705e-03  5.68049438e-02]]

 [[ 4.96556349e-02 -3.92083153e-02  4.56581600e-02 -9.35745388e-02
    4.29715998e-02  4.86304099e-03  3.03594116e-02 -2.98146121e-02
   -3.48876715e-02 -1.99361611e-02  1.61830232e-01  8.27303976e-02
    5.54493181e-02 -2.76539940e-02 -8.95071849e-02 -4.25396971e-02
    1.90371126e-02 -5.46189956e-02  6.61933422e-03 -2.52319500e-02
    5.71246371e-02  4.16730084e-02  3.64717506e-02 -2.82466318e-02
    1.91471111e-02  7.00276196e-02  9.04716653e-05  8.33336040e-02
    7.59424865e-02  6.57074805e-03  6.48965314e-02  2.48716436e-02
    1.32848127e-02  3.21027152e-02 -1.92846265e-02 -9.03862938e-02
   -4.04604413e-02 -6.87410757e-02  1.21686505e-02 -6.03357218e-02]]], shape=(10, 1, 40), dtype=float32)
y is  tf.Tensor(
[[[0.55674344 0.44325653]]

 [[0.48688495 0.5131151 ]]

 [[0.6102812  0.38971886]]

 [[0.45396575 0.5460342 ]]

 [[0.54179746 0.45820254]]

 [[0.44038284 0.5596171 ]]

 [[0.50789934 0.49210063]]

 [[0.5696154  0.43038452]]

 [[0.5041972  0.4958028 ]]

 [[0.461884   0.53811604]]], shape=(10, 1, 2), dtype=float32)
self.y in GINN_model call() is tf.Tensor(
[[[0.55674344 0.44325653]]

 [[0.48688495 0.5131151 ]]

 [[0.6102812  0.38971886]]

 [[0.45396575 0.5460342 ]]

 [[0.54179746 0.45820254]]

 [[0.44038284 0.5596171 ]]

 [[0.50789934 0.49210063]]

 [[0.5696154  0.43038452]]

 [[0.5041972  0.4958028 ]]

 [[0.461884   0.53811604]]], shape=(10, 1, 2), dtype=float32)
self.y.shape in GINN_model call() is (10, 1, 2)
y_prob_pred is  tf.Tensor(
[[[0.55674344]]

 [[0.5131151 ]]

 [[0.6102812 ]]

 [[0.5460342 ]]

 [[0.54179746]]

 [[0.5596171 ]]

 [[0.50789934]]

 [[0.5696154 ]]

 [[0.5041972 ]]

 [[0.53811604]]], shape=(10, 1, 1), dtype=float32)
y_pred in backprop.GradientTape() in Model.py train_step tf.Tensor(
[[[0.55674344]]

 [[0.5131151 ]]

 [[0.6102812 ]]

 [[0.5460342 ]]

 [[0.54179746]]

 [[0.5596171 ]]

 [[0.50789934]]

 [[0.5696154 ]]

 [[0.5041972 ]]

 [[0.53811604]]], shape=(10, 1, 1), dtype=float32)
nesd.flattend y_pred [<tf.Tensor: shape=(10, 1, 1), dtype=float32, numpy=
array([[[0.55674344]],

       [[0.5131151 ]],

       [[0.6102812 ]],

       [[0.5460342 ]],

       [[0.54179746]],

       [[0.5596171 ]],

       [[0.50789934]],

       [[0.5696154 ]],

       [[0.5041972 ]],

       [[0.53811604]]], dtype=float32)>]
nesd.flattend y_true [<tf.Tensor: shape=(10, 1, 1), dtype=float32, numpy=
array([[[1.]],

       [[1.]],

       [[1.]],

       [[1.]],

       [[1.]],

       [[1.]],

       [[1.]],

       [[1.]],

       [[1.]],

       [[0.]]], dtype=float32)>]
y_true in LossesContainer call() [<tf.Tensor: shape=(10, 1, 1), dtype=float32, numpy=
array([[[1.]],

       [[1.]],

       [[1.]],

       [[1.]],

       [[1.]],

       [[1.]],

       [[1.]],

       [[1.]],

       [[1.]],

       [[0.]]], dtype=float32)>]
y_pred in LossesContainer call() [<tf.Tensor: shape=(10, 1, 1), dtype=float32, numpy=
array([[[0.55674344]],

       [[0.5131151 ]],

       [[0.6102812 ]],

       [[0.5460342 ]],

       [[0.54179746]],

       [[0.5596171 ]],

       [[0.50789934]],

       [[0.5696154 ]],

       [[0.5041972 ]],

       [[0.53811604]]], dtype=float32)>]
y_pred in LossFunctionWrapper call() in losses.py is  tf.Tensor(
[[[0.55674344]]

 [[0.5131151 ]]

 [[0.6102812 ]]

 [[0.5460342 ]]

 [[0.54179746]]

 [[0.5596171 ]]

 [[0.50789934]]

 [[0.5696154 ]]

 [[0.5041972 ]]

 [[0.53811604]]], shape=(10, 1, 1), dtype=float32)
y_true in LossFunctionWrapper call() in losses.py is  tf.Tensor(
[[[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[0.]]], shape=(10, 1, 1), dtype=float32)
from_logits is  False
y_true is  tf.Tensor(
[[[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[1.]]

 [[0.]]], shape=(10, 1, 1), dtype=float32)
y_pred is  tf.Tensor(
[[[0.55674344]]

 [[0.5131151 ]]

 [[0.6102812 ]]

 [[0.5460342 ]]

 [[0.54179746]]

 [[0.5596171 ]]

 [[0.50789934]]

 [[0.5696154 ]]

 [[0.5041972 ]]

 [[0.53811604]]], shape=(10, 1, 1), dtype=float32)
x is 1e-07
dtype is  <dtype: 'float32'>
loss in optimizer_v2.minimize is  tf.Tensor(0.62426734, shape=(), dtype=float32)
flat_targets pre-call imperative_grad is  [<tf.Tensor: shape=(), dtype=float32, numpy=0.62426734>]
flat_sources pre-call imperative_grad is  [<tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>, <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>, <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>, <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>]
variables in custom_gradiet.py [<tf.Variable 'ginn_input_layer/Variable:0' shape=(915,) dtype=float32, numpy=
array([ 7.93516159e-01, -1.15385696e-01,  2.13972121e-01, -3.56907606e-01,
        7.01226830e-01, -4.01539952e-01,  6.82839215e-01,  7.49478638e-01,
       -6.74894750e-01,  4.01318759e-01,  3.12671304e-01, -6.55154586e-01,
        6.73290253e-01, -8.49015772e-01,  6.00232422e-01,  8.14236701e-01,
       -5.97876608e-01, -9.71457064e-01, -1.82775170e-01, -3.08911860e-01,
       -7.84737587e-01,  4.16453540e-01,  9.21217918e-01, -1.32101342e-01,
        7.13061213e-01, -9.88716006e-01,  4.17406201e-01, -7.27670491e-01,
        8.10134292e-01,  5.45784235e-01, -7.41476834e-01,  5.32974005e-01,
        7.80502141e-01,  7.22185254e-01, -7.42396355e-01, -4.60910112e-01,
        5.70265949e-01, -9.58594739e-01, -7.49043941e-01,  6.74115941e-02,
        7.92037725e-01,  3.64689738e-01, -9.34246480e-01,  8.75723720e-01,
       -9.64111924e-01, -1.33518279e-01, -1.00408934e-01,  6.36220574e-01,
        3.92315000e-01, -6.05255246e-01, -2.46369153e-01,  9.13717449e-01,
       -9.23626304e-01, -4.72151935e-02, -5.45031250e-01,  3.17253262e-01,
        8.93762827e-01, -8.07591915e-01, -7.49511421e-01,  4.51531798e-01,
       -1.91506639e-01, -4.52097833e-01,  3.62567037e-01,  1.56589478e-01,
        4.14750785e-01,  1.84347361e-01,  2.15772212e-01, -2.72486985e-01,
       -1.19274467e-01,  7.74010122e-01, -1.52535558e-01, -9.25496697e-01,
        5.89907289e-01,  7.55588591e-01, -7.55951166e-01, -1.51026741e-01,
        5.01725554e-01,  3.40664059e-01, -2.31372818e-01,  1.64513066e-01,
       -7.78356194e-01,  6.33505225e-01,  5.83806515e-01,  5.04386872e-02,
       -5.64036191e-01, -1.95216954e-01,  6.80999339e-01,  9.35262978e-01,
        8.48973274e-01, -3.33662219e-02,  3.12933177e-01, -3.02711755e-01,
        6.05760515e-01, -5.96679091e-01,  2.00930253e-01, -4.32431012e-01,
       -5.00997841e-01,  3.53070229e-01, -1.59008369e-01,  7.02168941e-01,
       -3.91317070e-01, -5.66817880e-01, -2.98116524e-02,  9.57965612e-01,
       -1.61312863e-01,  4.57871675e-01,  2.59735376e-01,  2.89803594e-01,
        4.60936427e-01, -9.42323565e-01,  2.03163862e-01, -5.37147522e-01,
        6.04791045e-01, -7.52415657e-01,  4.67093021e-01,  1.20635360e-01,
       -2.74869084e-01, -2.34385237e-01,  4.97040063e-01, -4.28952664e-01,
       -4.06379253e-01, -8.14750493e-01, -6.84553504e-01,  8.23036551e-01,
        2.75692105e-01,  7.16948152e-01, -5.38795412e-01, -3.02735895e-01,
        4.35444206e-01, -2.90485263e-01, -8.75433207e-01, -8.51987779e-01,
        4.00432408e-01,  9.18666661e-01,  5.31239621e-02, -3.11867654e-01,
        4.95588899e-01,  2.33586743e-01, -6.76062405e-01,  2.28712693e-01,
       -7.14569747e-01,  4.94450808e-01,  5.05838633e-01, -6.48721337e-01,
       -4.92254794e-01, -7.00271010e-01, -5.33242166e-01,  2.71822691e-01,
       -7.64674902e-01,  7.57902205e-01,  9.34066176e-02, -4.48642612e-01,
       -4.15863484e-01,  7.65939593e-01, -8.57596576e-01,  9.29914415e-01,
       -2.87626714e-01, -7.59094715e-01, -8.91866922e-01, -4.70454484e-01,
       -1.49182901e-01, -7.34306455e-01, -2.55929798e-01,  6.75577998e-01,
       -8.89151454e-01,  4.78854328e-01,  5.53010941e-01,  8.15914989e-01,
       -2.92665064e-01, -5.97735405e-01, -7.81376958e-01,  2.62847245e-01,
       -6.15575969e-01, -6.52716637e-01,  3.12758803e-01,  8.99402022e-01,
        6.64472222e-01,  8.34712625e-01,  9.13999498e-01, -5.79623096e-02,
       -9.12859887e-02,  7.16535032e-01,  3.99974048e-01, -4.46744263e-01,
       -6.80343151e-01, -3.70075822e-01, -1.18865266e-01,  8.40933263e-01,
        9.54293370e-01,  3.51915151e-01,  2.70979315e-01,  6.91612244e-01,
        2.57793099e-01,  6.30734712e-02,  8.60368907e-01,  2.40743145e-01,
        7.57942975e-01, -7.93695271e-01,  6.81751430e-01,  2.06740767e-01,
        2.65540302e-01,  6.45625949e-01, -7.60298669e-02, -2.71219850e-01,
       -1.07950859e-01,  4.92492802e-02, -6.99860454e-01, -3.73157382e-01,
        4.33491707e-01, -1.70329630e-01,  7.54544139e-01,  6.19512498e-01,
       -2.13935357e-02,  2.30903625e-01,  6.65790260e-01, -9.21040833e-01,
        3.02875489e-01, -3.32171798e-01,  8.81671786e-01,  8.20830047e-01,
       -1.32236302e-01,  5.09334028e-01, -3.35963786e-01,  2.48345047e-01,
       -1.67326048e-01, -6.42568290e-01, -1.75206587e-01, -6.96798265e-01,
        5.73418140e-01, -3.09532732e-01,  8.05500150e-01, -1.79001167e-01,
       -7.56307662e-01,  9.80614066e-01, -6.45165145e-01, -4.63108197e-02,
       -6.70987666e-01, -5.03446341e-01,  7.88571656e-01,  3.84187192e-01,
        3.76560360e-01,  6.14994347e-01, -7.77750537e-02, -6.62579775e-01,
       -8.12150419e-01,  9.85716045e-01, -4.95815188e-01, -4.99244928e-01,
       -6.19847417e-01, -5.81861794e-01, -7.47032344e-01,  2.23896444e-01,
        8.38207066e-01,  4.68075037e-01,  1.99287161e-01, -4.10758704e-01,
        7.22799003e-01,  4.24241662e-01, -4.03945714e-01,  4.12813395e-01,
        3.76868993e-01, -6.60853028e-01,  7.29760647e-01,  8.26023102e-01,
        9.09006536e-01, -4.90947247e-01,  5.72275758e-01,  2.57622093e-01,
        6.27599418e-01, -2.67918110e-01, -1.93039373e-01, -8.47003818e-01,
        4.95854497e-01, -7.79381156e-01, -7.63011396e-01, -7.87712812e-01,
        7.14009255e-02, -6.13082170e-01,  7.12333322e-01, -6.08763814e-01,
       -5.58755994e-01, -7.65175998e-01, -8.04499149e-01,  2.68162731e-02,
        1.07454203e-01,  8.39715898e-01,  6.84023619e-01, -9.37083781e-01,
        9.29887831e-01,  2.59375433e-03, -9.30839963e-03,  4.68937516e-01,
       -4.10054535e-01,  6.61686480e-01,  4.79839295e-01,  8.31101775e-01,
        4.31255490e-01, -5.00757933e-01, -5.96308112e-02,  3.05695236e-01,
       -9.11173701e-01, -9.03489828e-01,  2.10280251e-02, -8.07422996e-01,
        7.35139668e-01,  1.65286154e-01,  1.93287343e-01,  6.93862200e-01,
        4.08962462e-03,  4.51857924e-01, -3.53804082e-01,  7.92855978e-01,
       -9.33615983e-01,  2.37632439e-01,  7.69394159e-01, -9.28829253e-01,
       -8.47534835e-01,  4.86700863e-01, -2.15562042e-02, -8.64588022e-01,
       -8.52388799e-01,  1.59852386e-01, -5.71861207e-01,  1.89441353e-01,
        9.53917503e-01, -5.69051504e-01, -1.59345970e-01,  4.19490397e-01,
        6.39653325e-01, -1.58325449e-01, -2.66265482e-01,  7.53009558e-01,
        8.02227437e-01, -2.72938788e-01,  9.45739076e-02,  3.55973452e-01,
       -7.62331188e-01, -9.42325965e-02,  3.20461057e-02,  8.44855785e-01,
        9.11553919e-01,  7.69554496e-01, -9.38220322e-01, -7.78641403e-01,
       -9.16897297e-01,  7.75148034e-01,  9.21225488e-01, -3.46087694e-01,
        1.47081584e-01,  8.29313338e-01, -9.84962753e-06,  2.35204995e-02,
        4.72158909e-01, -3.25753726e-02, -3.82105231e-01,  1.19027793e-01,
       -2.60956734e-01, -7.89207339e-01,  8.87176931e-01,  4.44869369e-01,
       -9.05357778e-01, -4.94934410e-01, -4.32844073e-01, -5.37272394e-01,
        6.18817449e-01,  3.71095061e-01,  2.14727730e-01,  5.12148917e-01,
       -5.13655901e-01, -5.16705990e-01, -7.82700896e-01, -1.31557688e-01,
       -5.77414930e-01, -5.20610750e-01,  2.47468978e-01,  3.10324818e-01,
       -9.36218381e-01, -6.27615809e-01, -6.47190809e-01,  6.79873466e-01,
        2.59767830e-01, -1.20428160e-01, -8.03644240e-01,  7.71399438e-01,
       -6.71058297e-01, -2.66981572e-01, -3.46099734e-02, -6.78539276e-01,
        5.33905387e-01, -9.84341979e-01, -1.42501026e-01,  8.22666049e-01,
        5.26133657e-01,  3.57674509e-01, -5.54029524e-01,  4.60919648e-01,
       -6.79934084e-01,  4.92051870e-01, -3.99262518e-01,  9.77681518e-01,
        6.46438658e-01, -3.31808060e-01,  4.31028426e-01,  6.32493258e-01,
        9.61214960e-01,  2.28483185e-01,  3.67525131e-01, -5.31227708e-01,
        5.62833726e-01, -3.42546672e-01,  5.76312482e-01, -3.55331391e-01,
        5.05981982e-01, -3.07111889e-01,  3.10372800e-01, -9.04191494e-01,
        1.19186588e-01, -6.50552690e-01, -8.18181515e-01,  7.86605239e-01,
        6.77419424e-01,  8.27478111e-01, -5.34120612e-02, -3.78601164e-01,
       -4.88200694e-01, -8.81703675e-01, -9.08705175e-01,  8.58496726e-01,
        9.27098572e-01, -9.00791883e-02,  7.87826300e-01,  8.34187493e-02,
        4.58274305e-01,  5.65948486e-01, -7.58592963e-01, -9.98876244e-02,
       -7.03127086e-01, -3.77307624e-01, -2.27397785e-01, -6.17142022e-01,
        9.93997693e-01,  9.21337903e-01, -2.83166647e-01,  6.52795434e-01,
        4.92285863e-02,  2.06521768e-02,  5.43294191e-01, -4.99895886e-02,
        3.39711130e-01, -1.80405691e-01, -3.49055320e-01,  2.39553273e-01,
        6.83580101e-01, -8.92544329e-01, -7.08269119e-01,  6.88179672e-01,
       -5.22891104e-01,  2.77255863e-01, -5.21658957e-02, -5.62973440e-01,
        6.84849143e-01, -7.90948093e-01,  4.78244334e-01, -7.91660964e-01,
       -8.51218700e-01,  7.44482696e-01, -8.49836409e-01, -9.84452903e-01,
       -5.63973546e-01,  8.04286897e-01, -1.29123569e-01,  5.18038094e-01,
        3.14814538e-01,  2.98627734e-01, -7.20108626e-03, -1.50889456e-01,
       -8.12483490e-01, -2.10928842e-01, -2.19156116e-01, -6.79104626e-01,
        5.54768443e-01,  1.79884955e-01,  1.53498739e-01,  4.28206146e-01,
       -4.01899457e-01,  3.26350003e-01, -1.18499734e-01, -2.45337084e-01,
       -6.55303359e-01,  4.81619507e-01,  9.24482524e-01, -7.22549498e-01,
        3.39168787e-01, -2.23176498e-02,  5.78347921e-01,  2.54417229e-02,
       -8.04846644e-01,  3.70094776e-01,  6.64084077e-01, -6.99092388e-01,
        5.63493222e-02,  2.76599854e-01, -5.60552478e-01, -9.31780934e-01,
       -8.61873746e-01, -9.53682661e-01, -4.37726557e-01, -3.82820249e-01,
        4.81511146e-01, -7.67710388e-01,  7.79068112e-01,  4.80325252e-01,
        5.09483993e-01,  8.49776208e-01,  5.66444695e-01,  2.63929844e-01,
        6.31401598e-01,  2.62384444e-01, -5.99204063e-01,  1.75366446e-01,
        5.63576877e-01,  4.21183139e-01,  6.29834056e-01, -1.34502545e-01,
        8.92880619e-01,  8.43460441e-01,  1.78625137e-01, -8.74525383e-02,
        7.15093553e-01, -1.03573829e-01, -1.10415958e-01, -4.49235320e-01,
       -4.04078901e-01,  3.00892383e-01,  3.32444489e-01,  2.21581474e-01,
        3.12259644e-02,  5.80145359e-01, -7.40136027e-01,  4.91394311e-01,
        2.29297861e-01,  8.67696628e-02,  2.49223039e-01, -4.63047117e-01,
       -7.89206386e-01,  9.47640836e-01, -2.92576820e-01, -6.12265766e-01,
       -8.27940047e-01,  4.12244678e-01,  5.19411922e-01, -2.05165580e-01,
       -1.12215534e-01, -5.74243784e-01,  7.11920381e-01,  2.25884140e-01,
       -8.41259599e-01, -6.36766076e-01, -9.23310295e-02, -3.10472786e-01,
       -8.44645739e-01,  9.47636217e-02, -1.75975546e-01,  9.51979637e-01,
       -4.30163920e-01, -8.93091917e-01, -2.35014930e-01, -9.73374724e-01,
        8.40144753e-01,  2.92504638e-01,  1.11310557e-01, -9.18948591e-01,
       -2.50212044e-01,  7.19416499e-01,  7.19179571e-01, -7.71073759e-01,
        9.86190915e-01,  2.12537184e-01, -4.17973325e-02, -8.47433209e-01,
       -1.24700047e-01, -4.76794094e-01, -2.99198478e-01, -5.50191058e-03,
        8.09562266e-01,  7.55835831e-01, -9.40107554e-03, -8.16706836e-01,
       -7.15602338e-01, -4.01378125e-01, -3.03894520e-01,  2.47593746e-02,
       -9.18365240e-01,  1.41392658e-02, -8.46523762e-01, -3.61272320e-02,
        7.67259359e-01, -8.79878581e-01,  3.54704648e-01,  8.09604943e-01,
       -5.91717005e-01, -8.16509843e-01, -2.30212286e-01,  3.53081912e-01,
        4.11080271e-01, -7.05946445e-01,  7.72230774e-02, -6.03936911e-01,
        5.66292167e-01,  5.93103588e-01,  6.96171820e-01, -1.34702995e-01,
       -4.06626582e-01, -9.95727539e-01, -2.43696317e-01,  3.32226485e-01,
        2.70544440e-01,  4.79659081e-01, -2.12323681e-01, -9.67576683e-01,
        6.16217852e-02, -7.24134624e-01,  2.14993194e-01,  9.47745442e-01,
       -3.23512316e-01, -1.19472377e-01, -2.99522728e-01,  3.88082601e-02,
        9.63428542e-02, -6.40858114e-01, -5.96045554e-01, -8.95039737e-02,
        8.76676023e-01, -6.92015231e-01, -7.04747915e-01,  1.25715109e-02,
       -5.30444443e-01,  2.75263935e-01,  3.99501845e-02, -8.08264256e-01,
       -4.07682449e-01,  1.55385628e-01,  2.97612697e-01, -2.45129615e-01,
        7.42328286e-01, -4.77650762e-01,  6.92618787e-01,  6.91681445e-01,
       -1.24203349e-02,  8.96921575e-01,  8.41405571e-01, -7.88622856e-01,
       -6.05597079e-01, -4.20300126e-01, -1.16413616e-01, -3.24636102e-01,
       -1.74107701e-02, -6.34151638e-01,  8.02538276e-01, -3.72257292e-01,
       -4.35904175e-01, -5.87755144e-01, -9.55397427e-01, -2.73766518e-01,
       -2.23747715e-01,  3.79369229e-01,  3.22493166e-01,  9.66449916e-01,
        9.82773066e-01,  6.47633493e-01,  6.53691649e-01,  7.23368451e-02,
       -5.74297547e-01, -4.29185033e-02,  7.30818450e-01,  1.04165263e-01,
        7.71589339e-01, -1.50467068e-01,  7.28371978e-01, -4.62551415e-01,
        9.31184739e-02, -6.38178170e-01, -1.62047014e-01,  7.04480827e-01,
       -9.53352034e-01, -7.35463381e-01, -7.63665020e-01,  9.53523278e-01,
        9.82595742e-01,  1.51749790e-01, -1.96608186e-01, -2.67549634e-01,
       -5.10963857e-01,  8.28903615e-01,  4.42490578e-01, -8.40775788e-01,
       -8.44293296e-01, -8.03983152e-01, -1.33394981e-02,  6.90434575e-01,
       -1.73007786e-01,  7.79271126e-01, -9.18660998e-01,  5.55827133e-02,
       -9.84062180e-02,  8.56834650e-01,  6.29358709e-01,  4.73670065e-02,
       -8.05251837e-01,  7.27132678e-01, -6.92767322e-01,  6.06335402e-01,
        4.98829901e-01,  4.75333244e-01,  5.95172882e-01,  2.81566590e-01,
        8.10948968e-01, -6.86232626e-01,  7.67588913e-01, -9.60782111e-01,
       -2.29166791e-01, -8.14122319e-01, -7.53189862e-01,  8.73016059e-01,
       -8.13780963e-01,  7.01651454e-01, -1.71414346e-01,  2.00813130e-01,
       -2.71361619e-01,  1.42430425e-01,  5.83109399e-03,  9.36371505e-01,
        8.71648312e-01, -7.45763257e-02,  4.63911325e-01,  3.35245579e-01,
       -9.74325240e-01, -9.05486166e-01,  6.23792768e-01,  5.84032357e-01,
        9.99527454e-01,  1.40901580e-01, -9.21331406e-01, -1.96454853e-01,
       -4.67264593e-01,  3.23344111e-01,  6.85002029e-01,  4.41983163e-01,
        8.39803100e-01, -8.38038549e-02, -2.10069403e-01, -8.16436827e-01,
        2.37198070e-01, -2.49484032e-01,  5.52244484e-01, -8.91737580e-01,
       -8.65937352e-01, -7.17030764e-01, -2.90644504e-02, -7.68740177e-01,
       -1.24625161e-01, -9.82306421e-01, -3.96685898e-01,  1.00334741e-01,
        9.72205475e-02, -2.94238508e-01,  4.44057763e-01, -5.01254141e-01,
        6.02735102e-01, -7.86399007e-01, -6.97803855e-01, -5.07117033e-01,
        6.17018938e-01,  2.99585313e-01, -5.99193394e-01, -4.68407601e-01,
        1.18298821e-01,  5.34882426e-01, -8.73750389e-01, -8.68183821e-02,
        5.35658561e-02, -5.87285757e-01, -5.13376474e-01, -2.66507193e-02,
        2.42357299e-01, -9.82281148e-01,  3.24351996e-01,  2.28503332e-01,
       -1.39907598e-01, -4.61589009e-01,  9.96358171e-02, -1.98858172e-01,
        5.28355986e-02,  3.55033964e-01,  4.13769126e-01, -4.49858606e-01,
        4.63773459e-02,  1.05900131e-01,  2.88647473e-01, -6.84072077e-01,
       -8.90591741e-01,  3.05030078e-01, -9.31919873e-01, -8.79714847e-01,
       -3.68878275e-01,  1.86875582e-01,  1.99888758e-02, -8.48924100e-01,
       -5.87907374e-01, -7.87871420e-01,  4.09344316e-01, -2.76625156e-01,
        6.66155219e-01, -3.44162658e-02, -1.14109349e-02,  5.58116317e-01,
       -3.29952925e-01, -4.84653592e-01,  8.84273708e-01, -5.64809561e-01,
        8.21206391e-01,  3.20832044e-01,  1.22462921e-01, -2.10871503e-01,
       -1.89640686e-01,  5.14908016e-01,  6.82054520e-01,  2.00348005e-01,
       -7.34734237e-01,  8.06159317e-01,  3.35550278e-01,  7.54157722e-01,
       -9.21684265e-01, -7.14775980e-01,  9.87451613e-01, -3.86934951e-02,
        9.29499090e-01,  9.67469454e-01,  5.96027911e-01,  3.63380075e-01,
        9.99995708e-01, -9.71786857e-01,  3.45398277e-01, -5.27924061e-01,
        2.21933737e-01,  1.64580271e-01, -2.16902327e-02, -3.15749079e-01,
       -4.89310950e-01,  8.16900134e-02,  2.75878385e-02,  9.97382700e-01,
       -2.51740187e-01, -9.96001959e-01, -8.90745759e-01,  3.59113246e-01,
       -6.13297999e-01, -5.46035051e-01, -6.41274631e-01, -7.90135145e-01,
        3.03470820e-01,  4.26194482e-02,  8.65728974e-01, -9.78875935e-01,
       -7.91749597e-01, -9.52945650e-01, -8.39593887e-01,  2.84619778e-01,
        9.77977633e-01, -8.75860095e-01,  7.82754064e-01, -7.06715405e-01,
       -4.06687081e-01,  5.18890023e-01,  4.67880443e-02,  3.53223354e-01,
       -4.37214226e-02, -2.08883196e-01,  9.29436564e-01, -1.84513062e-01,
       -6.65034115e-01,  8.53922844e-01, -1.78869963e-01,  8.19963038e-01,
        5.84212601e-01, -2.28866175e-01,  2.31312960e-01,  7.84833491e-01,
        1.08052492e-01,  3.98010254e-01, -6.41333520e-01, -6.47005081e-01,
        9.14078891e-01, -5.11660520e-03,  5.84843159e-01,  1.18692480e-01,
       -5.97188771e-01, -7.17348754e-01,  9.39676464e-01, -7.90006697e-01,
        8.36795866e-01,  9.47739959e-01, -5.33035398e-01,  5.60236394e-01,
       -1.69225544e-01,  4.41332728e-01, -7.40779519e-01, -4.17580515e-01,
       -5.95755041e-01,  8.88479710e-01, -9.75655079e-01, -4.71207231e-01,
       -9.72735047e-01,  4.66476142e-01, -5.75098693e-01], dtype=float32)>]
len(variables) in custom_gradiet.py is  1
*upstream right before asseritons are tf.Tensor(
[-5.5718329e-03  4.1388371e-03 -6.2714500e-04  4.0502273e-03
  1.0474693e-03  4.0274900e-03  7.4933661e-04 -8.2278981e-05
 -1.0554376e-03 -1.9467189e-03 -4.2492566e-03  3.6226172e-04
 -1.5876447e-03  8.7062353e-03 -4.0293001e-03 -4.0567485e-03
  2.9936789e-03  2.9543947e-04  8.6850561e-03 -3.5907452e-03], shape=(20,), dtype=float32) tf.Tensor(
[ 6.1180568e-03 -4.5216805e-03  7.2098261e-04 -4.5033647e-03
 -1.2093008e-03 -4.3923124e-03 -8.0203946e-04  6.3920947e-05
  1.1492672e-03  2.1760326e-03  4.6752021e-03 -4.4747442e-04
  1.6762521e-03 -9.5691467e-03  4.4834451e-03  4.4820821e-03
 -3.2909517e-03 -3.3736936e-04 -9.5916064e-03  3.9687585e-03], shape=(20,), dtype=float32) tf.Tensor(
[-4.7872704e-03  3.4707282e-03 -6.4134761e-04  3.6101516e-03
  9.2457794e-04  3.3822884e-03  6.7164405e-04 -8.2821338e-05
 -8.7093969e-04 -1.7302440e-03 -3.6502311e-03  4.3508314e-04
 -1.2544764e-03  7.5453268e-03 -3.4950387e-03 -3.5394873e-03
  2.5397269e-03  3.1968835e-04  7.4207913e-03 -3.1388022e-03], shape=(20,), dtype=float32) tf.Tensor(
[ 5.6817527e-03 -4.2833346e-03  5.7172420e-04 -4.1796146e-03
 -1.0826798e-03 -4.0231072e-03 -7.9976744e-04  8.7695509e-05
  1.0268803e-03  2.0830906e-03  4.2944262e-03 -3.8396457e-04
  1.6417374e-03 -8.8754250e-03  4.0789833e-03  4.0702494e-03
 -3.0394266e-03 -4.2119113e-04 -8.8149961e-03  3.5901514e-03], shape=(20,), dtype=float32) tf.Tensor(
[-0.0056991   0.0041341  -0.00073335  0.00424962  0.00107518  0.00405134
  0.00071324 -0.00012209 -0.00114192 -0.00203995 -0.0043975   0.00051214
 -0.00155745  0.00895777 -0.00422057 -0.00422185  0.0030728   0.00039293
  0.00893133 -0.00376079], shape=(20,), dtype=float32) tf.Tensor(
[ 5.5448697e-03 -4.1105910e-03  6.2577770e-04 -4.0810211e-03
 -1.0861913e-03 -3.9437613e-03 -7.3733344e-04  6.2488420e-05
  1.0282379e-03  1.9866326e-03  4.1601914e-03 -4.0359015e-04
  1.4963606e-03 -8.6167408e-03  4.0277042e-03  4.0028631e-03
 -2.9389479e-03 -3.2612853e-04 -8.5952543e-03  3.5699299e-03], shape=(20,), dtype=float32) tf.Tensor(
[-6.2106238e-03  4.5868163e-03 -7.1823195e-04  4.5701163e-03
  1.2180951e-03  4.4528409e-03  8.1505888e-04 -6.7258712e-05
 -1.1769561e-03 -2.2016522e-03 -4.7442610e-03  4.5133100e-04
 -1.7159807e-03  9.7001949e-03 -4.5410185e-03 -4.5488300e-03
  3.3250542e-03  3.4067177e-04  9.7220698e-03 -4.0222998e-03], shape=(20,), dtype=float32) tf.Tensor(
[-0.00538573  0.00391736 -0.00065411  0.00400064  0.00107423  0.00377252
  0.00073944 -0.00010406 -0.00100809 -0.00189517 -0.00413576  0.00044206
 -0.00144718  0.00841195 -0.00393875 -0.00397691  0.0028438   0.00034706
  0.00836769 -0.0034932 ], shape=(20,), dtype=float32) tf.Tensor(
[-6.2609771e-03  4.6230336e-03 -7.3152053e-04  4.6098586e-03
  1.2281720e-03  4.4713351e-03  8.2004041e-04 -7.5963202e-05
 -1.1970262e-03 -2.2208889e-03 -4.7809011e-03  4.6411910e-04
 -1.7345506e-03  9.7731240e-03 -4.5811157e-03 -4.5920694e-03
  3.3443703e-03  3.5498192e-04  9.7927786e-03 -4.0505561e-03], shape=(20,), dtype=float32) tf.Tensor(
[-6.7471424e-03  4.9729235e-03 -8.0624979e-04  4.9766586e-03
  1.3201813e-03  4.8482646e-03  8.8559423e-04 -8.0664373e-05
 -1.2646993e-03 -2.4013664e-03 -5.1712887e-03  4.9548765e-04
 -1.8549752e-03  1.0572436e-02 -4.9512535e-03 -4.9529257e-03
  3.6357504e-03  3.7700334e-04  1.0592197e-02 -4.3657976e-03], shape=(20,), dtype=float32)
variables right before asseritons are [<tf.Variable 'ginn_input_layer/Variable:0' shape=(915,) dtype=float32, numpy=
array([ 7.93516159e-01, -1.15385696e-01,  2.13972121e-01, -3.56907606e-01,
        7.01226830e-01, -4.01539952e-01,  6.82839215e-01,  7.49478638e-01,
       -6.74894750e-01,  4.01318759e-01,  3.12671304e-01, -6.55154586e-01,
        6.73290253e-01, -8.49015772e-01,  6.00232422e-01,  8.14236701e-01,
       -5.97876608e-01, -9.71457064e-01, -1.82775170e-01, -3.08911860e-01,
       -7.84737587e-01,  4.16453540e-01,  9.21217918e-01, -1.32101342e-01,
        7.13061213e-01, -9.88716006e-01,  4.17406201e-01, -7.27670491e-01,
        8.10134292e-01,  5.45784235e-01, -7.41476834e-01,  5.32974005e-01,
        7.80502141e-01,  7.22185254e-01, -7.42396355e-01, -4.60910112e-01,
        5.70265949e-01, -9.58594739e-01, -7.49043941e-01,  6.74115941e-02,
        7.92037725e-01,  3.64689738e-01, -9.34246480e-01,  8.75723720e-01,
       -9.64111924e-01, -1.33518279e-01, -1.00408934e-01,  6.36220574e-01,
        3.92315000e-01, -6.05255246e-01, -2.46369153e-01,  9.13717449e-01,
       -9.23626304e-01, -4.72151935e-02, -5.45031250e-01,  3.17253262e-01,
        8.93762827e-01, -8.07591915e-01, -7.49511421e-01,  4.51531798e-01,
       -1.91506639e-01, -4.52097833e-01,  3.62567037e-01,  1.56589478e-01,
        4.14750785e-01,  1.84347361e-01,  2.15772212e-01, -2.72486985e-01,
       -1.19274467e-01,  7.74010122e-01, -1.52535558e-01, -9.25496697e-01,
        5.89907289e-01,  7.55588591e-01, -7.55951166e-01, -1.51026741e-01,
        5.01725554e-01,  3.40664059e-01, -2.31372818e-01,  1.64513066e-01,
       -7.78356194e-01,  6.33505225e-01,  5.83806515e-01,  5.04386872e-02,
       -5.64036191e-01, -1.95216954e-01,  6.80999339e-01,  9.35262978e-01,
        8.48973274e-01, -3.33662219e-02,  3.12933177e-01, -3.02711755e-01,
        6.05760515e-01, -5.96679091e-01,  2.00930253e-01, -4.32431012e-01,
       -5.00997841e-01,  3.53070229e-01, -1.59008369e-01,  7.02168941e-01,
       -3.91317070e-01, -5.66817880e-01, -2.98116524e-02,  9.57965612e-01,
       -1.61312863e-01,  4.57871675e-01,  2.59735376e-01,  2.89803594e-01,
        4.60936427e-01, -9.42323565e-01,  2.03163862e-01, -5.37147522e-01,
        6.04791045e-01, -7.52415657e-01,  4.67093021e-01,  1.20635360e-01,
       -2.74869084e-01, -2.34385237e-01,  4.97040063e-01, -4.28952664e-01,
       -4.06379253e-01, -8.14750493e-01, -6.84553504e-01,  8.23036551e-01,
        2.75692105e-01,  7.16948152e-01, -5.38795412e-01, -3.02735895e-01,
        4.35444206e-01, -2.90485263e-01, -8.75433207e-01, -8.51987779e-01,
        4.00432408e-01,  9.18666661e-01,  5.31239621e-02, -3.11867654e-01,
        4.95588899e-01,  2.33586743e-01, -6.76062405e-01,  2.28712693e-01,
       -7.14569747e-01,  4.94450808e-01,  5.05838633e-01, -6.48721337e-01,
       -4.92254794e-01, -7.00271010e-01, -5.33242166e-01,  2.71822691e-01,
       -7.64674902e-01,  7.57902205e-01,  9.34066176e-02, -4.48642612e-01,
       -4.15863484e-01,  7.65939593e-01, -8.57596576e-01,  9.29914415e-01,
       -2.87626714e-01, -7.59094715e-01, -8.91866922e-01, -4.70454484e-01,
       -1.49182901e-01, -7.34306455e-01, -2.55929798e-01,  6.75577998e-01,
       -8.89151454e-01,  4.78854328e-01,  5.53010941e-01,  8.15914989e-01,
       -2.92665064e-01, -5.97735405e-01, -7.81376958e-01,  2.62847245e-01,
       -6.15575969e-01, -6.52716637e-01,  3.12758803e-01,  8.99402022e-01,
        6.64472222e-01,  8.34712625e-01,  9.13999498e-01, -5.79623096e-02,
       -9.12859887e-02,  7.16535032e-01,  3.99974048e-01, -4.46744263e-01,
       -6.80343151e-01, -3.70075822e-01, -1.18865266e-01,  8.40933263e-01,
        9.54293370e-01,  3.51915151e-01,  2.70979315e-01,  6.91612244e-01,
        2.57793099e-01,  6.30734712e-02,  8.60368907e-01,  2.40743145e-01,
        7.57942975e-01, -7.93695271e-01,  6.81751430e-01,  2.06740767e-01,
        2.65540302e-01,  6.45625949e-01, -7.60298669e-02, -2.71219850e-01,
       -1.07950859e-01,  4.92492802e-02, -6.99860454e-01, -3.73157382e-01,
        4.33491707e-01, -1.70329630e-01,  7.54544139e-01,  6.19512498e-01,
       -2.13935357e-02,  2.30903625e-01,  6.65790260e-01, -9.21040833e-01,
        3.02875489e-01, -3.32171798e-01,  8.81671786e-01,  8.20830047e-01,
       -1.32236302e-01,  5.09334028e-01, -3.35963786e-01,  2.48345047e-01,
       -1.67326048e-01, -6.42568290e-01, -1.75206587e-01, -6.96798265e-01,
        5.73418140e-01, -3.09532732e-01,  8.05500150e-01, -1.79001167e-01,
       -7.56307662e-01,  9.80614066e-01, -6.45165145e-01, -4.63108197e-02,
       -6.70987666e-01, -5.03446341e-01,  7.88571656e-01,  3.84187192e-01,
        3.76560360e-01,  6.14994347e-01, -7.77750537e-02, -6.62579775e-01,
       -8.12150419e-01,  9.85716045e-01, -4.95815188e-01, -4.99244928e-01,
       -6.19847417e-01, -5.81861794e-01, -7.47032344e-01,  2.23896444e-01,
        8.38207066e-01,  4.68075037e-01,  1.99287161e-01, -4.10758704e-01,
        7.22799003e-01,  4.24241662e-01, -4.03945714e-01,  4.12813395e-01,
        3.76868993e-01, -6.60853028e-01,  7.29760647e-01,  8.26023102e-01,
        9.09006536e-01, -4.90947247e-01,  5.72275758e-01,  2.57622093e-01,
        6.27599418e-01, -2.67918110e-01, -1.93039373e-01, -8.47003818e-01,
        4.95854497e-01, -7.79381156e-01, -7.63011396e-01, -7.87712812e-01,
        7.14009255e-02, -6.13082170e-01,  7.12333322e-01, -6.08763814e-01,
       -5.58755994e-01, -7.65175998e-01, -8.04499149e-01,  2.68162731e-02,
        1.07454203e-01,  8.39715898e-01,  6.84023619e-01, -9.37083781e-01,
        9.29887831e-01,  2.59375433e-03, -9.30839963e-03,  4.68937516e-01,
       -4.10054535e-01,  6.61686480e-01,  4.79839295e-01,  8.31101775e-01,
        4.31255490e-01, -5.00757933e-01, -5.96308112e-02,  3.05695236e-01,
       -9.11173701e-01, -9.03489828e-01,  2.10280251e-02, -8.07422996e-01,
        7.35139668e-01,  1.65286154e-01,  1.93287343e-01,  6.93862200e-01,
        4.08962462e-03,  4.51857924e-01, -3.53804082e-01,  7.92855978e-01,
       -9.33615983e-01,  2.37632439e-01,  7.69394159e-01, -9.28829253e-01,
       -8.47534835e-01,  4.86700863e-01, -2.15562042e-02, -8.64588022e-01,
       -8.52388799e-01,  1.59852386e-01, -5.71861207e-01,  1.89441353e-01,
        9.53917503e-01, -5.69051504e-01, -1.59345970e-01,  4.19490397e-01,
        6.39653325e-01, -1.58325449e-01, -2.66265482e-01,  7.53009558e-01,
        8.02227437e-01, -2.72938788e-01,  9.45739076e-02,  3.55973452e-01,
       -7.62331188e-01, -9.42325965e-02,  3.20461057e-02,  8.44855785e-01,
        9.11553919e-01,  7.69554496e-01, -9.38220322e-01, -7.78641403e-01,
       -9.16897297e-01,  7.75148034e-01,  9.21225488e-01, -3.46087694e-01,
        1.47081584e-01,  8.29313338e-01, -9.84962753e-06,  2.35204995e-02,
        4.72158909e-01, -3.25753726e-02, -3.82105231e-01,  1.19027793e-01,
       -2.60956734e-01, -7.89207339e-01,  8.87176931e-01,  4.44869369e-01,
       -9.05357778e-01, -4.94934410e-01, -4.32844073e-01, -5.37272394e-01,
        6.18817449e-01,  3.71095061e-01,  2.14727730e-01,  5.12148917e-01,
       -5.13655901e-01, -5.16705990e-01, -7.82700896e-01, -1.31557688e-01,
       -5.77414930e-01, -5.20610750e-01,  2.47468978e-01,  3.10324818e-01,
       -9.36218381e-01, -6.27615809e-01, -6.47190809e-01,  6.79873466e-01,
        2.59767830e-01, -1.20428160e-01, -8.03644240e-01,  7.71399438e-01,
       -6.71058297e-01, -2.66981572e-01, -3.46099734e-02, -6.78539276e-01,
        5.33905387e-01, -9.84341979e-01, -1.42501026e-01,  8.22666049e-01,
        5.26133657e-01,  3.57674509e-01, -5.54029524e-01,  4.60919648e-01,
       -6.79934084e-01,  4.92051870e-01, -3.99262518e-01,  9.77681518e-01,
        6.46438658e-01, -3.31808060e-01,  4.31028426e-01,  6.32493258e-01,
        9.61214960e-01,  2.28483185e-01,  3.67525131e-01, -5.31227708e-01,
        5.62833726e-01, -3.42546672e-01,  5.76312482e-01, -3.55331391e-01,
        5.05981982e-01, -3.07111889e-01,  3.10372800e-01, -9.04191494e-01,
        1.19186588e-01, -6.50552690e-01, -8.18181515e-01,  7.86605239e-01,
        6.77419424e-01,  8.27478111e-01, -5.34120612e-02, -3.78601164e-01,
       -4.88200694e-01, -8.81703675e-01, -9.08705175e-01,  8.58496726e-01,
        9.27098572e-01, -9.00791883e-02,  7.87826300e-01,  8.34187493e-02,
        4.58274305e-01,  5.65948486e-01, -7.58592963e-01, -9.98876244e-02,
       -7.03127086e-01, -3.77307624e-01, -2.27397785e-01, -6.17142022e-01,
        9.93997693e-01,  9.21337903e-01, -2.83166647e-01,  6.52795434e-01,
        4.92285863e-02,  2.06521768e-02,  5.43294191e-01, -4.99895886e-02,
        3.39711130e-01, -1.80405691e-01, -3.49055320e-01,  2.39553273e-01,
        6.83580101e-01, -8.92544329e-01, -7.08269119e-01,  6.88179672e-01,
       -5.22891104e-01,  2.77255863e-01, -5.21658957e-02, -5.62973440e-01,
        6.84849143e-01, -7.90948093e-01,  4.78244334e-01, -7.91660964e-01,
       -8.51218700e-01,  7.44482696e-01, -8.49836409e-01, -9.84452903e-01,
       -5.63973546e-01,  8.04286897e-01, -1.29123569e-01,  5.18038094e-01,
        3.14814538e-01,  2.98627734e-01, -7.20108626e-03, -1.50889456e-01,
       -8.12483490e-01, -2.10928842e-01, -2.19156116e-01, -6.79104626e-01,
        5.54768443e-01,  1.79884955e-01,  1.53498739e-01,  4.28206146e-01,
       -4.01899457e-01,  3.26350003e-01, -1.18499734e-01, -2.45337084e-01,
       -6.55303359e-01,  4.81619507e-01,  9.24482524e-01, -7.22549498e-01,
        3.39168787e-01, -2.23176498e-02,  5.78347921e-01,  2.54417229e-02,
       -8.04846644e-01,  3.70094776e-01,  6.64084077e-01, -6.99092388e-01,
        5.63493222e-02,  2.76599854e-01, -5.60552478e-01, -9.31780934e-01,
       -8.61873746e-01, -9.53682661e-01, -4.37726557e-01, -3.82820249e-01,
        4.81511146e-01, -7.67710388e-01,  7.79068112e-01,  4.80325252e-01,
        5.09483993e-01,  8.49776208e-01,  5.66444695e-01,  2.63929844e-01,
        6.31401598e-01,  2.62384444e-01, -5.99204063e-01,  1.75366446e-01,
        5.63576877e-01,  4.21183139e-01,  6.29834056e-01, -1.34502545e-01,
        8.92880619e-01,  8.43460441e-01,  1.78625137e-01, -8.74525383e-02,
        7.15093553e-01, -1.03573829e-01, -1.10415958e-01, -4.49235320e-01,
       -4.04078901e-01,  3.00892383e-01,  3.32444489e-01,  2.21581474e-01,
        3.12259644e-02,  5.80145359e-01, -7.40136027e-01,  4.91394311e-01,
        2.29297861e-01,  8.67696628e-02,  2.49223039e-01, -4.63047117e-01,
       -7.89206386e-01,  9.47640836e-01, -2.92576820e-01, -6.12265766e-01,
       -8.27940047e-01,  4.12244678e-01,  5.19411922e-01, -2.05165580e-01,
       -1.12215534e-01, -5.74243784e-01,  7.11920381e-01,  2.25884140e-01,
       -8.41259599e-01, -6.36766076e-01, -9.23310295e-02, -3.10472786e-01,
       -8.44645739e-01,  9.47636217e-02, -1.75975546e-01,  9.51979637e-01,
       -4.30163920e-01, -8.93091917e-01, -2.35014930e-01, -9.73374724e-01,
        8.40144753e-01,  2.92504638e-01,  1.11310557e-01, -9.18948591e-01,
       -2.50212044e-01,  7.19416499e-01,  7.19179571e-01, -7.71073759e-01,
        9.86190915e-01,  2.12537184e-01, -4.17973325e-02, -8.47433209e-01,
       -1.24700047e-01, -4.76794094e-01, -2.99198478e-01, -5.50191058e-03,
        8.09562266e-01,  7.55835831e-01, -9.40107554e-03, -8.16706836e-01,
       -7.15602338e-01, -4.01378125e-01, -3.03894520e-01,  2.47593746e-02,
       -9.18365240e-01,  1.41392658e-02, -8.46523762e-01, -3.61272320e-02,
        7.67259359e-01, -8.79878581e-01,  3.54704648e-01,  8.09604943e-01,
       -5.91717005e-01, -8.16509843e-01, -2.30212286e-01,  3.53081912e-01,
        4.11080271e-01, -7.05946445e-01,  7.72230774e-02, -6.03936911e-01,
        5.66292167e-01,  5.93103588e-01,  6.96171820e-01, -1.34702995e-01,
       -4.06626582e-01, -9.95727539e-01, -2.43696317e-01,  3.32226485e-01,
        2.70544440e-01,  4.79659081e-01, -2.12323681e-01, -9.67576683e-01,
        6.16217852e-02, -7.24134624e-01,  2.14993194e-01,  9.47745442e-01,
       -3.23512316e-01, -1.19472377e-01, -2.99522728e-01,  3.88082601e-02,
        9.63428542e-02, -6.40858114e-01, -5.96045554e-01, -8.95039737e-02,
        8.76676023e-01, -6.92015231e-01, -7.04747915e-01,  1.25715109e-02,
       -5.30444443e-01,  2.75263935e-01,  3.99501845e-02, -8.08264256e-01,
       -4.07682449e-01,  1.55385628e-01,  2.97612697e-01, -2.45129615e-01,
        7.42328286e-01, -4.77650762e-01,  6.92618787e-01,  6.91681445e-01,
       -1.24203349e-02,  8.96921575e-01,  8.41405571e-01, -7.88622856e-01,
       -6.05597079e-01, -4.20300126e-01, -1.16413616e-01, -3.24636102e-01,
       -1.74107701e-02, -6.34151638e-01,  8.02538276e-01, -3.72257292e-01,
       -4.35904175e-01, -5.87755144e-01, -9.55397427e-01, -2.73766518e-01,
       -2.23747715e-01,  3.79369229e-01,  3.22493166e-01,  9.66449916e-01,
        9.82773066e-01,  6.47633493e-01,  6.53691649e-01,  7.23368451e-02,
       -5.74297547e-01, -4.29185033e-02,  7.30818450e-01,  1.04165263e-01,
        7.71589339e-01, -1.50467068e-01,  7.28371978e-01, -4.62551415e-01,
        9.31184739e-02, -6.38178170e-01, -1.62047014e-01,  7.04480827e-01,
       -9.53352034e-01, -7.35463381e-01, -7.63665020e-01,  9.53523278e-01,
        9.82595742e-01,  1.51749790e-01, -1.96608186e-01, -2.67549634e-01,
       -5.10963857e-01,  8.28903615e-01,  4.42490578e-01, -8.40775788e-01,
       -8.44293296e-01, -8.03983152e-01, -1.33394981e-02,  6.90434575e-01,
       -1.73007786e-01,  7.79271126e-01, -9.18660998e-01,  5.55827133e-02,
       -9.84062180e-02,  8.56834650e-01,  6.29358709e-01,  4.73670065e-02,
       -8.05251837e-01,  7.27132678e-01, -6.92767322e-01,  6.06335402e-01,
        4.98829901e-01,  4.75333244e-01,  5.95172882e-01,  2.81566590e-01,
        8.10948968e-01, -6.86232626e-01,  7.67588913e-01, -9.60782111e-01,
       -2.29166791e-01, -8.14122319e-01, -7.53189862e-01,  8.73016059e-01,
       -8.13780963e-01,  7.01651454e-01, -1.71414346e-01,  2.00813130e-01,
       -2.71361619e-01,  1.42430425e-01,  5.83109399e-03,  9.36371505e-01,
        8.71648312e-01, -7.45763257e-02,  4.63911325e-01,  3.35245579e-01,
       -9.74325240e-01, -9.05486166e-01,  6.23792768e-01,  5.84032357e-01,
        9.99527454e-01,  1.40901580e-01, -9.21331406e-01, -1.96454853e-01,
       -4.67264593e-01,  3.23344111e-01,  6.85002029e-01,  4.41983163e-01,
        8.39803100e-01, -8.38038549e-02, -2.10069403e-01, -8.16436827e-01,
        2.37198070e-01, -2.49484032e-01,  5.52244484e-01, -8.91737580e-01,
       -8.65937352e-01, -7.17030764e-01, -2.90644504e-02, -7.68740177e-01,
       -1.24625161e-01, -9.82306421e-01, -3.96685898e-01,  1.00334741e-01,
        9.72205475e-02, -2.94238508e-01,  4.44057763e-01, -5.01254141e-01,
        6.02735102e-01, -7.86399007e-01, -6.97803855e-01, -5.07117033e-01,
        6.17018938e-01,  2.99585313e-01, -5.99193394e-01, -4.68407601e-01,
        1.18298821e-01,  5.34882426e-01, -8.73750389e-01, -8.68183821e-02,
        5.35658561e-02, -5.87285757e-01, -5.13376474e-01, -2.66507193e-02,
        2.42357299e-01, -9.82281148e-01,  3.24351996e-01,  2.28503332e-01,
       -1.39907598e-01, -4.61589009e-01,  9.96358171e-02, -1.98858172e-01,
        5.28355986e-02,  3.55033964e-01,  4.13769126e-01, -4.49858606e-01,
        4.63773459e-02,  1.05900131e-01,  2.88647473e-01, -6.84072077e-01,
       -8.90591741e-01,  3.05030078e-01, -9.31919873e-01, -8.79714847e-01,
       -3.68878275e-01,  1.86875582e-01,  1.99888758e-02, -8.48924100e-01,
       -5.87907374e-01, -7.87871420e-01,  4.09344316e-01, -2.76625156e-01,
        6.66155219e-01, -3.44162658e-02, -1.14109349e-02,  5.58116317e-01,
       -3.29952925e-01, -4.84653592e-01,  8.84273708e-01, -5.64809561e-01,
        8.21206391e-01,  3.20832044e-01,  1.22462921e-01, -2.10871503e-01,
       -1.89640686e-01,  5.14908016e-01,  6.82054520e-01,  2.00348005e-01,
       -7.34734237e-01,  8.06159317e-01,  3.35550278e-01,  7.54157722e-01,
       -9.21684265e-01, -7.14775980e-01,  9.87451613e-01, -3.86934951e-02,
        9.29499090e-01,  9.67469454e-01,  5.96027911e-01,  3.63380075e-01,
        9.99995708e-01, -9.71786857e-01,  3.45398277e-01, -5.27924061e-01,
        2.21933737e-01,  1.64580271e-01, -2.16902327e-02, -3.15749079e-01,
       -4.89310950e-01,  8.16900134e-02,  2.75878385e-02,  9.97382700e-01,
       -2.51740187e-01, -9.96001959e-01, -8.90745759e-01,  3.59113246e-01,
       -6.13297999e-01, -5.46035051e-01, -6.41274631e-01, -7.90135145e-01,
        3.03470820e-01,  4.26194482e-02,  8.65728974e-01, -9.78875935e-01,
       -7.91749597e-01, -9.52945650e-01, -8.39593887e-01,  2.84619778e-01,
        9.77977633e-01, -8.75860095e-01,  7.82754064e-01, -7.06715405e-01,
       -4.06687081e-01,  5.18890023e-01,  4.67880443e-02,  3.53223354e-01,
       -4.37214226e-02, -2.08883196e-01,  9.29436564e-01, -1.84513062e-01,
       -6.65034115e-01,  8.53922844e-01, -1.78869963e-01,  8.19963038e-01,
        5.84212601e-01, -2.28866175e-01,  2.31312960e-01,  7.84833491e-01,
        1.08052492e-01,  3.98010254e-01, -6.41333520e-01, -6.47005081e-01,
        9.14078891e-01, -5.11660520e-03,  5.84843159e-01,  1.18692480e-01,
       -5.97188771e-01, -7.17348754e-01,  9.39676464e-01, -7.90006697e-01,
        8.36795866e-01,  9.47739959e-01, -5.33035398e-01,  5.60236394e-01,
       -1.69225544e-01,  4.41332728e-01, -7.40779519e-01, -4.17580515e-01,
       -5.95755041e-01,  8.88479710e-01, -9.75655079e-01, -4.71207231e-01,
       -9.72735047e-01,  4.66476142e-01, -5.75098693e-01], dtype=float32)>]
YOU ARE IN ALGO 1
is eager in algo1 True
[0 1]
y_j_T is [0.55674344 0.44325653]
sum_of_var is  tf.Tensor(
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(405,), dtype=float32)
[1 0]
y_j_T is [0.48688495 0.5131151 ]
sum_of_var is  tf.Tensor(
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(405,), dtype=float32)
[1 0]
y_j_T is [0.6102812  0.38971886]
sum_of_var is  tf.Tensor(
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(405,), dtype=float32)
[1 0]
y_j_T is [0.45396575 0.5460342 ]
sum_of_var is  tf.Tensor(
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(405,), dtype=float32)
[1 0]
y_j_T is [0.54179746 0.45820254]
sum_of_var is  tf.Tensor(
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(405,), dtype=float32)
[0 1]
y_j_T is [0.44038284 0.5596171 ]
sum_of_var is  tf.Tensor(
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(405,), dtype=float32)
[1 0]
y_j_T is [0.50789934 0.49210063]
sum_of_var is  tf.Tensor(
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(405,), dtype=float32)
[1 0]
y_j_T is [0.5696154  0.43038452]
sum_of_var is  tf.Tensor(
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(405,), dtype=float32)
[1 0]
y_j_T is [0.5041972 0.4958028]
sum_of_var is  tf.Tensor(
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(405,), dtype=float32)
[0 1]
y_j_T is [0.461884   0.53811604]
sum_of_var is  tf.Tensor(
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(405,), dtype=float32)
YOU ARE IN ALGO 1
is eager in algo1 True
[0 1]
y_j_T is [0.55674344 0.44325653]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
 -0.05588248  0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
 -0.05588248  0.          0.         -0.05588248  0.          0.
  0.          0.          0.          0.          0.          0.
 -0.05588248  0.          0.          0.         -0.05588248  0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.        ], shape=(140,), dtype=float32)
[1 0]
y_j_T is [0.48688495 0.5131151 ]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
 -0.05588248  0.          0.          0.          0.          0.
  0.          0.04442354  0.          0.          0.04442354  0.
  0.04442354  0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
 -0.05588248  0.          0.         -0.05588248  0.          0.
  0.          0.          0.          0.          0.          0.
 -0.05588248  0.          0.          0.         -0.05588248  0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.        ], shape=(140,), dtype=float32)
[1 0]
y_j_T is [0.6102812  0.38971886]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.01287396
  0.          0.          0.          0.01287396  0.          0.
 -0.05588248  0.          0.          0.          0.          0.
  0.          0.04442354  0.          0.          0.04442354  0.
  0.04442354  0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
 -0.05588248  0.          0.         -0.05588248  0.          0.
  0.          0.          0.          0.          0.          0.
 -0.05588248  0.          0.          0.         -0.05588248  0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.01287396
  0.          0.          0.01287396  0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.        ], shape=(140,), dtype=float32)
[1 0]
y_j_T is [0.45396575 0.5460342 ]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.00526277
  0.          0.          0.          0.          0.          0.01287396
  0.          0.          0.00526277  0.01287396  0.          0.
 -0.05588248  0.          0.          0.          0.          0.
  0.          0.04442354  0.00526277  0.          0.04442354  0.
  0.04442354  0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
 -0.05588248  0.          0.         -0.05588248  0.00526277  0.00526277
  0.          0.          0.          0.00526277  0.00526277  0.
 -0.05588248  0.          0.          0.         -0.05588248  0.
  0.00526277  0.00526277  0.00526277  0.00526277  0.          0.00526277
  0.00526277  0.00526277  0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.00526277  0.00526277  0.          0.
  0.          0.          0.          0.          0.          0.00526277
  0.          0.          0.          0.          0.          0.01287396
  0.          0.          0.01287396  0.00526277  0.          0.
  0.          0.          0.          0.          0.00526277  0.
  0.          0.          0.          0.          0.          0.00526277
  0.          0.          0.          0.00526277  0.          0.
  0.          0.        ], shape=(140,), dtype=float32)
[1 0]
y_j_T is [0.54179746 0.45820254]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.00020254  0.00526277
  0.          0.          0.          0.          0.          0.01287396
  0.          0.          0.00526277  0.01287396  0.          0.
 -0.05588248  0.          0.00020254  0.          0.          0.
  0.          0.04442354  0.00526277  0.          0.04442354  0.
  0.04442354  0.          0.          0.          0.          0.
  0.00020254  0.          0.          0.          0.          0.
 -0.05588248  0.          0.         -0.05588248  0.00526277  0.00526277
  0.          0.          0.          0.00526277  0.00526277  0.
 -0.05588248  0.00020254  0.00020254  0.00020254 -0.05588248  0.00020254
  0.00526277  0.00526277  0.00526277  0.00526277  0.          0.00526277
  0.00526277  0.00526277  0.00020254  0.          0.          0.
  0.00020254  0.00020254  0.00020254  0.00020254  0.00020254  0.00020254
  0.          0.          0.          0.          0.00020254  0.
  0.          0.          0.          0.          0.          0.00020254
  0.00020254  0.          0.00526277  0.00526277  0.          0.
  0.          0.          0.          0.          0.          0.00526277
  0.          0.          0.          0.          0.          0.01287396
  0.          0.          0.01287396  0.00526277  0.          0.
  0.00020254  0.00020254  0.00020254  0.00020254  0.00526277  0.00020254
  0.00020254  0.          0.00020254  0.          0.00020254  0.00526277
  0.00020254  0.00020254  0.00020254  0.00526277  0.00020254  0.00020254
  0.00020254  0.00020254], shape=(140,), dtype=float32)
[0 1]
y_j_T is [0.44038284 0.5596171 ]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.00020254  0.00526277
  0.          0.          0.          0.          0.          0.01287396
  0.          0.          0.00526277  0.01287396  0.          0.
 -0.05588248  0.          0.00020254  0.          0.          0.
  0.          0.04442354  0.00526277  0.          0.04442354  0.
  0.04442354  0.          0.          0.          0.          0.
  0.00020254  0.          0.          0.          0.          0.
 -0.05588248  0.          0.         -0.05588248  0.00526277  0.00526277
  0.          0.          0.          0.00526277  0.00526277  0.
 -0.05588248  0.00020254  0.00020254  0.00020254 -0.05588248  0.00020254
  0.00526277  0.00526277  0.00526277  0.00526277  0.          0.00526277
  0.00526277  0.00526277  0.00020254  0.          0.          0.
  0.00020254  0.00020254  0.00020254  0.00020254  0.00020254  0.00020254
  0.          0.          0.          0.          0.00020254  0.
  0.          0.          0.          0.          0.          0.00020254
  0.00020254  0.          0.00526277  0.00526277  0.          0.
  0.          0.          0.          0.          0.          0.00526277
  0.          0.          0.          0.          0.          0.01287396
  0.          0.          0.01287396  0.00526277  0.          0.
  0.00020254  0.00020254  0.00020254  0.00020254  0.00526277  0.00020254
  0.00020254  0.          0.00020254  0.          0.00020254  0.00526277
  0.00020254  0.00020254  0.00020254  0.00526277  0.00020254  0.00020254
  0.00020254  0.00020254], shape=(140,), dtype=float32)
[1 0]
y_j_T is [0.50789934 0.49210063]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.00020254  0.00526277
  0.          0.          0.          0.          0.          0.01287396
  0.          0.          0.00526277  0.01287396  0.          0.
 -0.05588248  0.          0.00020254  0.          0.          0.
  0.          0.04442354  0.00526277  0.          0.04442354  0.
  0.04442354  0.          0.          0.          0.          0.
  0.00020254  0.          0.          0.          0.          0.
 -0.05588248  0.          0.         -0.05588248  0.00526277  0.00526277
  0.          0.          0.          0.00526277  0.00526277  0.
 -0.05588248  0.00020254  0.00020254  0.00020254 -0.05588248  0.00020254
  0.00526277  0.00526277  0.00526277  0.00526277  0.          0.00526277
  0.00526277  0.00526277  0.00020254  0.          0.          0.
  0.00020254  0.00020254  0.00020254  0.00020254  0.00020254  0.00020254
  0.          0.          0.          0.          0.00020254  0.
  0.          0.          0.          0.          0.          0.00020254
  0.00020254  0.          0.00526277  0.00526277  0.          0.
  0.          0.          0.          0.          0.          0.00526277
  0.          0.          0.          0.          0.          0.01287396
  0.          0.          0.01287396  0.00526277  0.          0.
  0.00020254  0.00020254  0.00020254  0.00020254  0.00526277  0.00020254
  0.00020254  0.          0.00020254  0.          0.00020254  0.00526277
  0.00020254  0.00020254  0.00020254  0.00526277  0.00020254  0.00020254
  0.00020254  0.00020254], shape=(140,), dtype=float32)
[1 0]
y_j_T is [0.5696154  0.43038452]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.00020254  0.00526277
  0.          0.          0.          0.          0.          0.01287396
  0.          0.          0.00526277  0.01287396  0.          0.
 -0.05588248  0.          0.00020254  0.          0.          0.
  0.          0.04442354  0.00526277  0.          0.04442354  0.
  0.04442354  0.          0.          0.          0.          0.
  0.00020254  0.          0.          0.          0.          0.
 -0.05588248  0.          0.         -0.05588248  0.00526277  0.00526277
  0.          0.          0.00222511  0.00526277  0.00526277  0.00222511
 -0.05588248  0.00020254  0.00020254  0.00020254 -0.05588248  0.00020254
  0.00526277  0.00526277  0.00526277  0.00526277  0.          0.00526277
  0.00526277  0.00526277  0.00020254  0.          0.00222511  0.
  0.00020254  0.00020254  0.00020254  0.00020254  0.00020254  0.00020254
  0.          0.          0.          0.          0.00020254  0.
  0.          0.          0.          0.          0.00222511  0.00020254
  0.00020254  0.          0.00526277  0.00526277  0.          0.
  0.          0.          0.          0.          0.          0.00526277
  0.          0.          0.          0.          0.          0.01287396
  0.          0.          0.01287396  0.00526277  0.          0.
  0.00020254  0.00020254  0.00020254  0.00020254  0.00526277  0.00020254
  0.00020254  0.          0.00020254  0.          0.00020254  0.00526277
  0.00020254  0.00020254  0.00020254  0.00526277  0.00020254  0.00020254
  0.00020254  0.00020254], shape=(140,), dtype=float32)
[1 0]
y_j_T is [0.5041972 0.4958028]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.00020254  0.00526277
  0.          0.          0.          0.          0.          0.01287396
  0.          0.          0.00526277  0.01287396  0.          0.
 -0.05588248  0.          0.00020254  0.          0.          0.
  0.          0.04442354  0.00526277  0.          0.04442354  0.
  0.04442354  0.          0.          0.          0.          0.
  0.00020254  0.          0.          0.          0.          0.
 -0.05588248  0.          0.         -0.05588248  0.00526277  0.00526277
  0.          0.          0.00222511  0.00526277  0.00526277  0.00222511
 -0.05588248  0.00020254  0.00020254  0.00020254 -0.05588248  0.00020254
  0.00526277  0.00526277  0.00526277  0.00526277  0.          0.00526277
  0.00526277  0.00526277  0.00020254  0.          0.00222511  0.
  0.00020254  0.00020254  0.00020254  0.00020254  0.00020254  0.00020254
  0.          0.          0.          0.          0.00020254  0.03569141
  0.03569141  0.          0.          0.          0.00222511  0.00020254
  0.00020254  0.          0.00526277  0.00526277  0.          0.
  0.          0.          0.          0.          0.          0.00526277
  0.          0.          0.          0.          0.          0.01287396
  0.          0.          0.01287396  0.00526277  0.          0.
  0.00020254  0.00020254  0.00020254  0.00020254  0.00526277  0.00020254
  0.00020254  0.          0.00020254  0.          0.00020254  0.00526277
  0.00020254  0.00020254  0.00020254  0.00526277  0.00020254  0.00020254
  0.00020254  0.00020254], shape=(140,), dtype=float32)
[0 1]
y_j_T is [0.461884   0.53811604]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.00020254  0.00526277
  0.          0.          0.          0.          0.          0.01287396
  0.          0.          0.00526277  0.01287396  0.          0.
 -0.05588248  0.          0.00020254  0.          0.          0.
  0.          0.04442354  0.00526277  0.          0.04442354  0.
  0.04442354  0.          0.          0.          0.          0.
  0.00020254  0.          0.          0.          0.          0.
 -0.05588248  0.          0.         -0.05588248  0.00526277  0.00526277
  0.          0.          0.00222511  0.00526277  0.00526277  0.00222511
 -0.05588248  0.00020254  0.00020254  0.00020254 -0.05588248  0.00020254
  0.00526277  0.00526277  0.00526277  0.00526277  0.          0.00526277
  0.00526277  0.00526277  0.00020254  0.          0.00222511  0.
  0.00020254  0.00020254  0.00020254  0.00020254  0.00020254  0.00020254
  0.          0.          0.          0.          0.00020254  0.03569141
  0.03569141  0.          0.          0.          0.00222511  0.00020254
  0.00020254  0.          0.00526277  0.00526277  0.          0.
  0.          0.          0.          0.          0.          0.00526277
  0.          0.          0.          0.          0.          0.01287396
  0.          0.          0.01287396  0.00526277  0.          0.
  0.00020254  0.00020254  0.00020254  0.00020254  0.00526277  0.00020254
  0.00020254  0.          0.00020254  0.          0.00020254  0.00526277
  0.00020254  0.00020254  0.00020254  0.00526277  0.00020254  0.00020254
  0.00020254  0.00020254], shape=(140,), dtype=float32)
YOU ARE IN ALGO 1
is eager in algo1 True
[0 1]
y_j_T is [0.55674344 0.44325653]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.48688495 0.5131151 ]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.6102812  0.38971886]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.45396575 0.5460342 ]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.54179746 0.45820254]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[0 1]
y_j_T is [0.44038284 0.5596171 ]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.50789934 0.49210063]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.5696154  0.43038452]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.5041972 0.4958028]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[0 1]
y_j_T is [0.461884   0.53811604]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
YOU ARE IN ALGO 1
is eager in algo1 True
[0 1]
y_j_T is [0.55674344 0.44325653]
sum_of_var is  tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)
[1 0]
y_j_T is [0.48688495 0.5131151 ]
sum_of_var is  tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)
[1 0]
y_j_T is [0.6102812  0.38971886]
sum_of_var is  tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)
[1 0]
y_j_T is [0.45396575 0.5460342 ]
sum_of_var is  tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)
[1 0]
y_j_T is [0.54179746 0.45820254]
sum_of_var is  tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)
[0 1]
y_j_T is [0.44038284 0.5596171 ]
sum_of_var is  tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)
[1 0]
y_j_T is [0.50789934 0.49210063]
sum_of_var is  tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)
[1 0]
y_j_T is [0.5696154  0.43038452]
sum_of_var is  tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)
[1 0]
y_j_T is [0.5041972 0.4958028]
sum_of_var is  tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)
[0 1]
y_j_T is [0.461884   0.53811604]
sum_of_var is  tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)
YOU ARE IN ALGO 1
is eager in algo1 True
[0 1]
y_j_T is [0.55674344 0.44325653]
sum_of_var is  tf.Tensor(
[ 0.         -0.04483665  0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.        ], shape=(13,), dtype=float32)
[1 0]
y_j_T is [0.48688495 0.5131151 ]
sum_of_var is  tf.Tensor(
[ 0.         -0.04483665  0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.        ], shape=(13,), dtype=float32)
[1 0]
y_j_T is [0.6102812  0.38971886]
sum_of_var is  tf.Tensor(
[ 0.         -0.04483665  0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.03981858], shape=(13,), dtype=float32)
[1 0]
y_j_T is [0.45396575 0.5460342 ]
sum_of_var is  tf.Tensor(
[ 0.04778735 -0.04483665  0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.03981858], shape=(13,), dtype=float32)
[1 0]
y_j_T is [0.54179746 0.45820254]
sum_of_var is  tf.Tensor(
[ 0.04778735 -0.04483665  0.          0.          0.01006609  0.
  0.          0.01006609  0.          0.          0.          0.
  0.03981858], shape=(13,), dtype=float32)
[0 1]
y_j_T is [0.44038284 0.5596171 ]
sum_of_var is  tf.Tensor(
[ 0.04778735 -0.04483665  0.          0.          0.01006609  0.
  0.          0.01006609  0.          0.          0.          0.
  0.03981858], shape=(13,), dtype=float32)
[1 0]
y_j_T is [0.50789934 0.49210063]
sum_of_var is  tf.Tensor(
[ 0.04778735 -0.04483665  0.          0.          0.01006609  0.
  0.          0.01006609  0.          0.          0.          0.
  0.03981858], shape=(13,), dtype=float32)
[1 0]
y_j_T is [0.5696154  0.43038452]
sum_of_var is  tf.Tensor(
[ 0.04778735 -0.04483665  0.          0.          0.01006609  0.
  0.          0.01006609  0.          0.          0.          0.
  0.03981858], shape=(13,), dtype=float32)
[1 0]
y_j_T is [0.5041972 0.4958028]
sum_of_var is  tf.Tensor(
[ 0.04778735 -0.04483665  0.          0.          0.01006609  0.
  0.          0.01006609  0.          0.          0.          0.
  0.03981858], shape=(13,), dtype=float32)
[0 1]
y_j_T is [0.461884   0.53811604]
sum_of_var is  tf.Tensor(
[ 0.04778735 -0.04483665  0.          0.          0.01006609  0.
  0.          0.01006609  0.          0.          0.          0.
  0.03981858], shape=(13,), dtype=float32)
YOU ARE IN ALGO 1
is eager in algo1 True
[0 1]
y_j_T is [0.55674344 0.44325653]
sum_of_var is  tf.Tensor(
[ 0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
 -0.0491553  0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
 -0.0491553  0.         0.         0.         0.         0.
  0.        -0.0491553  0.         0.         0.         0.       ], shape=(84,), dtype=float32)
[1 0]
y_j_T is [0.48688495 0.5131151 ]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
 -0.0491553   0.          0.          0.04627497  0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.04627497  0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
 -0.0491553   0.          0.          0.          0.          0.
  0.         -0.0491553   0.          0.          0.          0.        ], shape=(84,), dtype=float32)
[1 0]
y_j_T is [0.6102812  0.38971886]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
 -0.0491553   0.          0.          0.04627497  0.          0.
  0.          0.          0.          0.03147621  0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.04627497  0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
 -0.0491553   0.          0.          0.03147621  0.          0.
  0.         -0.0491553   0.          0.          0.          0.        ], shape=(84,), dtype=float32)
[1 0]
y_j_T is [0.45396575 0.5460342 ]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
 -0.0491553   0.          0.          0.04627497  0.          0.
  0.          0.          0.          0.03147621  0.          0.
  0.          0.          0.          0.          0.          0.00592602
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.04627497  0.
  0.          0.          0.          0.00592602  0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.00592602
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
 -0.0491553   0.          0.          0.03147621  0.          0.
  0.00592602 -0.0491553   0.          0.          0.          0.        ], shape=(84,), dtype=float32)
[1 0]
y_j_T is [0.54179746 0.45820254]
sum_of_var is  tf.Tensor(
[ 0.          0.0003806   0.0003806   0.0003806   0.          0.
  0.          0.0003806   0.0003806   0.0003806   0.          0.
 -0.0491553   0.          0.          0.04627497  0.          0.
  0.          0.          0.          0.03147621  0.          0.
  0.          0.          0.          0.          0.0003806   0.00592602
  0.0003806   0.0003806   0.          0.0003806   0.          0.0003806
  0.          0.0003806   0.          0.          0.04627497  0.00076121
  0.00076121  0.0003806   0.          0.00592602  0.          0.
  0.          0.          0.0003806   0.0003806   0.0003806   0.
  0.          0.          0.          0.          0.          0.00592602
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
 -0.0491553   0.          0.          0.03147621  0.          0.
  0.00592602 -0.0491553   0.          0.          0.          0.        ], shape=(84,), dtype=float32)
[0 1]
y_j_T is [0.44038284 0.5596171 ]
sum_of_var is  tf.Tensor(
[ 0.          0.0003806   0.0003806   0.0003806   0.          0.
  0.          0.0003806   0.0003806   0.0003806   0.          0.
 -0.0491553   0.          0.          0.04627497  0.          0.
  0.          0.          0.          0.03147621  0.          0.
  0.          0.          0.          0.          0.0003806   0.00592602
  0.0003806   0.0003806   0.          0.0003806   0.          0.0003806
  0.          0.0003806   0.          0.          0.04627497  0.00076121
  0.00076121  0.0003806   0.          0.00592602  0.          0.
  0.          0.          0.0003806   0.0003806   0.0003806   0.
  0.          0.          0.         -0.01739349  0.          0.00592602
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
 -0.0491553   0.          0.          0.03147621  0.          0.
  0.00592602 -0.0491553   0.          0.          0.          0.        ], shape=(84,), dtype=float32)
[1 0]
y_j_T is [0.50789934 0.49210063]
sum_of_var is  tf.Tensor(
[ 0.          0.0003806   0.0003806   0.0003806   0.          0.
  0.          0.0003806   0.0003806   0.0003806   0.          0.
 -0.0491553   0.          0.          0.04627497  0.          0.
  0.          0.          0.          0.03147621  0.          0.
  0.          0.          0.          0.          0.0003806   0.00592602
  0.0003806   0.0003806   0.          0.0003806   0.          0.0003806
  0.          0.0003806   0.          0.          0.04627497  0.00076121
  0.00076121  0.0003806   0.          0.00592602  0.          0.
  0.          0.          0.0003806   0.0003806   0.0003806   0.
  0.          0.04346015  0.         -0.01739349  0.          0.00592602
  0.          0.          0.04346015  0.          0.          0.
  0.          0.          0.          0.          0.          0.
 -0.0491553   0.          0.          0.03147621  0.          0.
  0.00592602 -0.0491553   0.          0.          0.          0.        ], shape=(84,), dtype=float32)
[1 0]
y_j_T is [0.5696154  0.43038452]
sum_of_var is  tf.Tensor(
[ 0.          0.0003806   0.0003806   0.0003806   0.          0.
  0.          0.0003806   0.0003806   0.0003806   0.          0.
 -0.0491553   0.          0.          0.04627497  0.          0.
  0.          0.          0.          0.03147621  0.          0.
  0.          0.          0.          0.          0.0003806   0.00592602
  0.0003806   0.0003806   0.          0.0003806   0.          0.0003806
  0.03335027  0.0003806   0.          0.          0.04627497  0.00076121
  0.00076121  0.0003806   0.          0.00592602  0.          0.
  0.          0.          0.0003806   0.0003806   0.0003806   0.
  0.          0.04346015  0.         -0.01739349  0.          0.00592602
  0.          0.          0.04346015  0.          0.          0.
  0.          0.          0.          0.          0.          0.
 -0.0491553   0.          0.          0.03147621  0.          0.
  0.00592602 -0.0491553   0.          0.          0.          0.        ], shape=(84,), dtype=float32)
[1 0]
y_j_T is [0.5041972 0.4958028]
sum_of_var is  tf.Tensor(
[ 0.          0.0003806   0.0003806   0.0003806   0.          0.
  0.          0.0003806   0.0003806   0.0003806   0.          0.
 -0.0491553   0.          0.          0.04627497  0.          0.
  0.          0.          0.          0.03147621  0.          0.
  0.          0.          0.          0.          0.0003806   0.00592602
  0.0003806   0.0003806   0.          0.0003806   0.          0.0003806
  0.03335027  0.0003806   0.          0.          0.04627497  0.00076121
  0.00076121  0.0003806   0.          0.00592602  0.03296311  0.
  0.          0.          0.0003806   0.0003806   0.0003806   0.
  0.          0.04346015  0.         -0.01739349  0.          0.00592602
  0.          0.          0.04346015  0.          0.          0.
  0.          0.          0.          0.          0.          0.
 -0.0491553   0.          0.          0.03147621  0.          0.
  0.00592602 -0.0491553   0.          0.          0.          0.        ], shape=(84,), dtype=float32)
[0 1]
y_j_T is [0.461884   0.53811604]
sum_of_var is  tf.Tensor(
[ 0.          0.0003806   0.0003806   0.0003806   0.          0.
  0.          0.0003806   0.0003806   0.0003806   0.          0.
 -0.0491553   0.          0.          0.04627497  0.          0.
  0.          0.          0.          0.03147621  0.          0.
  0.          0.          0.          0.          0.0003806   0.00592602
  0.0003806   0.0003806   0.          0.0003806   0.          0.0003806
  0.03335027  0.0003806   0.          0.          0.04627497  0.00076121
  0.00076121  0.0003806   0.          0.00592602  0.03296311  0.
  0.          0.          0.0003806   0.0003806   0.0003806   0.
  0.          0.04346015  0.         -0.01739349  0.          0.00592602
  0.          0.          0.04346015  0.          0.          0.
  0.          0.          0.          0.          0.          0.
 -0.0491553   0.          0.          0.03147621  0.          0.
  0.00592602 -0.0491553   0.          0.          0.          0.        ], shape=(84,), dtype=float32)
YOU ARE IN ALGO 1
is eager in algo1 True
[0 1]
y_j_T is [0.55674344 0.44325653]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.48688495 0.5131151 ]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.6102812  0.38971886]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.45396575 0.5460342 ]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.54179746 0.45820254]
sum_of_var is  tf.Tensor([0.03268039], shape=(1,), dtype=float32)
[0 1]
y_j_T is [0.44038284 0.5596171 ]
sum_of_var is  tf.Tensor([0.03268039], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.50789934 0.49210063]
sum_of_var is  tf.Tensor([0.03268039], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.5696154  0.43038452]
sum_of_var is  tf.Tensor([0.03268039], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.5041972 0.4958028]
sum_of_var is  tf.Tensor([0.03268039], shape=(1,), dtype=float32)
[0 1]
y_j_T is [0.461884   0.53811604]
sum_of_var is  tf.Tensor([0.03268039], shape=(1,), dtype=float32)
YOU ARE IN ALGO 1
is eager in algo1 True
[0 1]
y_j_T is [0.55674344 0.44325653]
sum_of_var is  tf.Tensor([0. 0.], shape=(2,), dtype=float32)
[1 0]
y_j_T is [0.48688495 0.5131151 ]
sum_of_var is  tf.Tensor([0. 0.], shape=(2,), dtype=float32)
[1 0]
y_j_T is [0.6102812  0.38971886]
sum_of_var is  tf.Tensor([0.00366273 0.        ], shape=(2,), dtype=float32)
[1 0]
y_j_T is [0.45396575 0.5460342 ]
sum_of_var is  tf.Tensor([0.00366273 0.        ], shape=(2,), dtype=float32)
[1 0]
y_j_T is [0.54179746 0.45820254]
sum_of_var is  tf.Tensor([0.00366273 0.        ], shape=(2,), dtype=float32)
[0 1]
y_j_T is [0.44038284 0.5596171 ]
sum_of_var is  tf.Tensor([0.00366273 0.        ], shape=(2,), dtype=float32)
[1 0]
y_j_T is [0.50789934 0.49210063]
sum_of_var is  tf.Tensor([0.00366273 0.        ], shape=(2,), dtype=float32)
[1 0]
y_j_T is [0.5696154  0.43038452]
sum_of_var is  tf.Tensor([0.00366273 0.        ], shape=(2,), dtype=float32)
[1 0]
y_j_T is [0.5041972 0.4958028]
sum_of_var is  tf.Tensor([0.00366273 0.        ], shape=(2,), dtype=float32)
[0 1]
y_j_T is [0.461884   0.53811604]
sum_of_var is  tf.Tensor([0.00366273 0.        ], shape=(2,), dtype=float32)
YOU ARE IN ALGO 1
is eager in algo1 True
[0 1]
y_j_T is [0.55674344 0.44325653]
sum_of_var is  tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(10,), dtype=float32)
[1 0]
y_j_T is [0.48688495 0.5131151 ]
sum_of_var is  tf.Tensor(
[0.         0.         0.         0.01454083 0.         0.
 0.         0.         0.         0.        ], shape=(10,), dtype=float32)
[1 0]
y_j_T is [0.6102812  0.38971886]
sum_of_var is  tf.Tensor(
[0.         0.         0.         0.01454083 0.         0.
 0.         0.         0.         0.        ], shape=(10,), dtype=float32)
[1 0]
y_j_T is [0.45396575 0.5460342 ]
sum_of_var is  tf.Tensor(
[0.         0.         0.         0.01454083 0.         0.
 0.         0.         0.         0.        ], shape=(10,), dtype=float32)
[1 0]
y_j_T is [0.54179746 0.45820254]
sum_of_var is  tf.Tensor(
[0.         0.         0.         0.01454083 0.01487052 0.
 0.         0.         0.         0.        ], shape=(10,), dtype=float32)
[0 1]
y_j_T is [0.44038284 0.5596171 ]
sum_of_var is  tf.Tensor(
[0.         0.         0.         0.01454083 0.01487052 0.
 0.         0.         0.         0.        ], shape=(10,), dtype=float32)
[1 0]
y_j_T is [0.50789934 0.49210063]
sum_of_var is  tf.Tensor(
[0.         0.01039901 0.         0.01454083 0.01487052 0.
 0.         0.         0.         0.        ], shape=(10,), dtype=float32)
[1 0]
y_j_T is [0.5696154  0.43038452]
sum_of_var is  tf.Tensor(
[0.         0.01039901 0.         0.01454083 0.01487052 0.
 0.         0.         0.         0.        ], shape=(10,), dtype=float32)
[1 0]
y_j_T is [0.5041972 0.4958028]
sum_of_var is  tf.Tensor(
[0.         0.01039901 0.         0.01454083 0.01487052 0.
 0.         0.         0.         0.        ], shape=(10,), dtype=float32)
[0 1]
y_j_T is [0.461884   0.53811604]
sum_of_var is  tf.Tensor(
[ 0.          0.01039901  0.          0.01454083  0.01487052 -0.0181232
  0.          0.          0.          0.        ], shape=(10,), dtype=float32)
YOU ARE IN ALGO 1
is eager in algo1 True
[0 1]
y_j_T is [0.55674344 0.44325653]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.48688495 0.5131151 ]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.6102812  0.38971886]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.45396575 0.5460342 ]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.54179746 0.45820254]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[0 1]
y_j_T is [0.44038284 0.5596171 ]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.50789934 0.49210063]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.5696154  0.43038452]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.5041972 0.4958028]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[0 1]
y_j_T is [0.461884   0.53811604]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
YOU ARE IN ALGO 1
is eager in algo1 True
[0 1]
y_j_T is [0.55674344 0.44325653]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.          0.
  0.          0.         -0.00297421  0.          0.          0.
  0.         -0.00297421  0.          0.        ], shape=(16,), dtype=float32)
[1 0]
y_j_T is [0.48688495 0.5131151 ]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.          0.
  0.          0.         -0.00297421  0.          0.          0.
  0.         -0.00297421  0.          0.        ], shape=(16,), dtype=float32)
[1 0]
y_j_T is [0.6102812  0.38971886]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.          0.
  0.          0.         -0.00297421  0.          0.          0.
  0.         -0.00297421  0.          0.        ], shape=(16,), dtype=float32)
[1 0]
y_j_T is [0.45396575 0.5460342 ]
sum_of_var is  tf.Tensor(
[ 0.          0.00672016  0.          0.00672016  0.          0.
  0.          0.         -0.00297421  0.00672016  0.          0.
  0.         -0.00297421  0.          0.        ], shape=(16,), dtype=float32)
[1 0]
y_j_T is [0.54179746 0.45820254]
sum_of_var is  tf.Tensor(
[ 0.          0.00672016  0.          0.00672016  0.          0.00472565
  0.          0.         -0.00297421  0.00672016  0.          0.00472565
  0.         -0.00297421  0.          0.        ], shape=(16,), dtype=float32)
[0 1]
y_j_T is [0.44038284 0.5596171 ]
sum_of_var is  tf.Tensor(
[-0.00467879  0.00672016  0.          0.00672016  0.          0.00472565
  0.          0.         -0.00297421  0.00672016  0.          0.00472565
  0.         -0.00297421  0.          0.        ], shape=(16,), dtype=float32)
[1 0]
y_j_T is [0.50789934 0.49210063]
sum_of_var is  tf.Tensor(
[-0.00467879  0.00672016  0.          0.00672016  0.          0.00472565
  0.          0.         -0.00297421  0.00672016  0.          0.00472565
  0.         -0.00297421  0.          0.        ], shape=(16,), dtype=float32)
[1 0]
y_j_T is [0.5696154  0.43038452]
sum_of_var is  tf.Tensor(
[-0.00467879  0.00672016  0.          0.00672016  0.          0.00472565
  0.          0.         -0.00297421  0.00672016  0.          0.00472565
  0.         -0.00297421  0.          0.        ], shape=(16,), dtype=float32)
[1 0]
y_j_T is [0.5041972 0.4958028]
sum_of_var is  tf.Tensor(
[-0.00467879  0.00672016  0.          0.00672016  0.          0.00472565
  0.          0.         -0.00297421  0.00672016  0.          0.00472565
  0.         -0.00297421  0.          0.        ], shape=(16,), dtype=float32)
[0 1]
y_j_T is [0.461884   0.53811604]
sum_of_var is  tf.Tensor(
[-0.00467879  0.00672016  0.          0.00672016  0.          0.00472565
  0.          0.         -0.00297421  0.00672016  0.          0.00472565
  0.         -0.00297421  0.          0.        ], shape=(16,), dtype=float32)
YOU ARE IN ALGO 1
is eager in algo1 True
[0 1]
y_j_T is [0.55674344 0.44325653]
sum_of_var is  tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)
[1 0]
y_j_T is [0.48688495 0.5131151 ]
sum_of_var is  tf.Tensor([0.00395664 0.00395664 0.        ], shape=(3,), dtype=float32)
[1 0]
y_j_T is [0.6102812  0.38971886]
sum_of_var is  tf.Tensor([0.00395664 0.00395664 0.        ], shape=(3,), dtype=float32)
[1 0]
y_j_T is [0.45396575 0.5460342 ]
sum_of_var is  tf.Tensor([0.00395664 0.00395664 0.        ], shape=(3,), dtype=float32)
[1 0]
y_j_T is [0.54179746 0.45820254]
sum_of_var is  tf.Tensor([0.00395664 0.00395664 0.        ], shape=(3,), dtype=float32)
[0 1]
y_j_T is [0.44038284 0.5596171 ]
sum_of_var is  tf.Tensor([0.00395664 0.00395664 0.        ], shape=(3,), dtype=float32)
[1 0]
y_j_T is [0.50789934 0.49210063]
sum_of_var is  tf.Tensor([0.00395664 0.00395664 0.        ], shape=(3,), dtype=float32)
[1 0]
y_j_T is [0.5696154  0.43038452]
sum_of_var is  tf.Tensor([0.00395664 0.00395664 0.        ], shape=(3,), dtype=float32)
[1 0]
y_j_T is [0.5041972 0.4958028]
sum_of_var is  tf.Tensor([0.00395664 0.00395664 0.        ], shape=(3,), dtype=float32)
[0 1]
y_j_T is [0.461884   0.53811604]
sum_of_var is  tf.Tensor([0.00395664 0.00395664 0.        ], shape=(3,), dtype=float32)
YOU ARE IN ALGO 1
is eager in algo1 True
[0 1]
y_j_T is [0.55674344 0.44325653]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.         -0.02468138
  0.          0.         -0.01234069  0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.        ], shape=(37,), dtype=float32)
[1 0]
y_j_T is [0.48688495 0.5131151 ]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.         -0.02468138
  0.          0.         -0.01234069  0.          0.          0.01336412
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.        ], shape=(37,), dtype=float32)
[1 0]
y_j_T is [0.6102812  0.38971886]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.         -0.02468138
  0.          0.         -0.01234069  0.          0.          0.01336412
  0.          0.          0.          0.01523635  0.          0.
  0.          0.          0.          0.          0.          0.
  0.        ], shape=(37,), dtype=float32)
[1 0]
y_j_T is [0.45396575 0.5460342 ]
sum_of_var is  tf.Tensor(
[ 0.          0.01126476  0.01126476  0.          0.01126476  0.
  0.          0.          0.01126476  0.          0.01126476  0.
  0.          0.          0.          0.          0.         -0.02468138
  0.          0.01126476 -0.01234069  0.          0.          0.01336412
  0.          0.          0.          0.01523635  0.          0.01126476
  0.          0.          0.01126476  0.          0.          0.
  0.        ], shape=(37,), dtype=float32)
[1 0]
y_j_T is [0.54179746 0.45820254]
sum_of_var is  tf.Tensor(
[ 0.          0.01126476  0.01126476  0.02318662  0.01126476  0.
  0.          0.          0.01126476  0.          0.01126476  0.
  0.          0.          0.          0.          0.         -0.02468138
  0.          0.01126476 -0.01234069  0.02318662  0.          0.01336412
  0.          0.          0.          0.01523635  0.          0.01126476
  0.          0.          0.01126476  0.          0.          0.02318662
  0.        ], shape=(37,), dtype=float32)
[0 1]
y_j_T is [0.44038284 0.5596171 ]
sum_of_var is  tf.Tensor(
[ 0.          0.01126476  0.01126476  0.02318662  0.01126476  0.
  0.          0.          0.01126476  0.          0.01126476  0.
  0.          0.          0.          0.          0.         -0.02468138
  0.          0.01126476 -0.01234069  0.02318662  0.          0.01336412
  0.          0.          0.          0.01523635  0.          0.01126476
  0.          0.          0.01126476  0.          0.          0.02318662
  0.        ], shape=(37,), dtype=float32)
[1 0]
y_j_T is [0.50789934 0.49210063]
sum_of_var is  tf.Tensor(
[ 0.          0.01126476  0.01126476  0.02318662  0.01126476  0.
  0.          0.          0.01126476  0.          0.01126476  0.
  0.          0.          0.          0.          0.         -0.02468138
  0.          0.01126476 -0.01234069  0.02318662  0.          0.01336412
  0.          0.          0.          0.01523635  0.          0.01126476
  0.          0.          0.01126476  0.          0.          0.02318662
  0.        ], shape=(37,), dtype=float32)
[1 0]
y_j_T is [0.5696154  0.43038452]
sum_of_var is  tf.Tensor(
[ 0.          0.01126476  0.01126476  0.02318662  0.01126476  0.
  0.          0.          0.01126476  0.          0.01126476  0.
  0.          0.          0.          0.          0.         -0.02468138
  0.          0.01126476 -0.01234069  0.02318662  0.          0.01336412
  0.02376796  0.          0.          0.01523635  0.          0.01126476
  0.          0.          0.01126476  0.          0.          0.02318662
  0.        ], shape=(37,), dtype=float32)
[1 0]
y_j_T is [0.5041972 0.4958028]
sum_of_var is  tf.Tensor(
[ 0.          0.01126476  0.01126476  0.02318662  0.01126476  0.
  0.          0.          0.01126476  0.          0.01126476  0.
  0.          0.          0.          0.          0.         -0.02468138
  0.          0.01126476 -0.01234069  0.02318662  0.          0.01336412
  0.02376796  0.          0.          0.01523635  0.          0.01126476
  0.          0.          0.01126476  0.          0.          0.02318662
  0.        ], shape=(37,), dtype=float32)
[0 1]
y_j_T is [0.461884   0.53811604]
sum_of_var is  tf.Tensor(
[ 0.          0.01126476  0.01126476  0.02318662  0.01126476  0.
  0.          0.          0.01126476  0.          0.01126476  0.
  0.          0.          0.          0.          0.         -0.02468138
 -0.0075356   0.01126476 -0.01234069  0.02318662  0.          0.01336412
  0.02376796  0.          0.          0.01523635  0.          0.01126476
  0.          0.          0.01126476  0.          0.          0.02318662
  0.        ], shape=(37,), dtype=float32)
YOU ARE IN ALGO 1
is eager in algo1 True
[0 1]
y_j_T is [0.55674344 0.44325653]
sum_of_var is  tf.Tensor([0. 0. 0. 0. 0. 0.], shape=(6,), dtype=float32)
[1 0]
y_j_T is [0.48688495 0.5131151 ]
sum_of_var is  tf.Tensor([0. 0. 0. 0. 0. 0.], shape=(6,), dtype=float32)
[1 0]
y_j_T is [0.6102812  0.38971886]
sum_of_var is  tf.Tensor([0. 0. 0. 0. 0. 0.], shape=(6,), dtype=float32)
[1 0]
y_j_T is [0.45396575 0.5460342 ]
sum_of_var is  tf.Tensor([0. 0. 0. 0. 0. 0.], shape=(6,), dtype=float32)
[1 0]
y_j_T is [0.54179746 0.45820254]
sum_of_var is  tf.Tensor([0.         0.05780568 0.         0.         0.         0.        ], shape=(6,), dtype=float32)
[0 1]
y_j_T is [0.44038284 0.5596171 ]
sum_of_var is  tf.Tensor([0.         0.05780568 0.         0.         0.         0.        ], shape=(6,), dtype=float32)
[1 0]
y_j_T is [0.50789934 0.49210063]
sum_of_var is  tf.Tensor([0.         0.05780568 0.         0.         0.         0.        ], shape=(6,), dtype=float32)
[1 0]
y_j_T is [0.5696154  0.43038452]
sum_of_var is  tf.Tensor([0.         0.05780568 0.         0.         0.         0.        ], shape=(6,), dtype=float32)
[1 0]
y_j_T is [0.5041972 0.4958028]
sum_of_var is  tf.Tensor([0.         0.05780568 0.         0.         0.         0.        ], shape=(6,), dtype=float32)
[0 1]
y_j_T is [0.461884   0.53811604]
sum_of_var is  tf.Tensor([0.         0.05780568 0.         0.         0.         0.        ], shape=(6,), dtype=float32)
YOU ARE IN ALGO 1
is eager in algo1 True
[0 1]
y_j_T is [0.55674344 0.44325653]
sum_of_var is  tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0.], shape=(8,), dtype=float32)
[1 0]
y_j_T is [0.48688495 0.5131151 ]
sum_of_var is  tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0.], shape=(8,), dtype=float32)
[1 0]
y_j_T is [0.6102812  0.38971886]
sum_of_var is  tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0.], shape=(8,), dtype=float32)
[1 0]
y_j_T is [0.45396575 0.5460342 ]
sum_of_var is  tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0.], shape=(8,), dtype=float32)
[1 0]
y_j_T is [0.54179746 0.45820254]
sum_of_var is  tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0.], shape=(8,), dtype=float32)
[0 1]
y_j_T is [0.44038284 0.5596171 ]
sum_of_var is  tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0.], shape=(8,), dtype=float32)
[1 0]
y_j_T is [0.50789934 0.49210063]
sum_of_var is  tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0.], shape=(8,), dtype=float32)
[1 0]
y_j_T is [0.5696154  0.43038452]
sum_of_var is  tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0.], shape=(8,), dtype=float32)
[1 0]
y_j_T is [0.5041972 0.4958028]
sum_of_var is  tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0.], shape=(8,), dtype=float32)
[0 1]
y_j_T is [0.461884   0.53811604]
sum_of_var is  tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0.], shape=(8,), dtype=float32)
YOU ARE IN ALGO 1
is eager in algo1 True
[0 1]
y_j_T is [0.55674344 0.44325653]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.48688495 0.5131151 ]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.6102812  0.38971886]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.45396575 0.5460342 ]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.54179746 0.45820254]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[0 1]
y_j_T is [0.44038284 0.5596171 ]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.50789934 0.49210063]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.5696154  0.43038452]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[1 0]
y_j_T is [0.5041972 0.4958028]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
[0 1]
y_j_T is [0.461884   0.53811604]
sum_of_var is  tf.Tensor([0.], shape=(1,), dtype=float32)
YOU ARE IN ALGO 1
is eager in algo1 True
[0 1]
y_j_T is [0.55674344 0.44325653]
sum_of_var is  tf.Tensor([ 0.         -0.01906022  0.        ], shape=(3,), dtype=float32)
[1 0]
y_j_T is [0.48688495 0.5131151 ]
sum_of_var is  tf.Tensor([ 0.         -0.01906022  0.        ], shape=(3,), dtype=float32)
[1 0]
y_j_T is [0.6102812  0.38971886]
sum_of_var is  tf.Tensor([ 0.         -0.01906022  0.        ], shape=(3,), dtype=float32)
[1 0]
y_j_T is [0.45396575 0.5460342 ]
sum_of_var is  tf.Tensor([ 0.         -0.01906022  0.        ], shape=(3,), dtype=float32)
[1 0]
y_j_T is [0.54179746 0.45820254]
sum_of_var is  tf.Tensor([ 0.         -0.01906022  0.        ], shape=(3,), dtype=float32)
[0 1]
y_j_T is [0.44038284 0.5596171 ]
sum_of_var is  tf.Tensor([ 0.         -0.01906022  0.        ], shape=(3,), dtype=float32)
[1 0]
y_j_T is [0.50789934 0.49210063]
sum_of_var is  tf.Tensor([ 0.         -0.01906022  0.        ], shape=(3,), dtype=float32)
[1 0]
y_j_T is [0.5696154  0.43038452]
sum_of_var is  tf.Tensor([ 0.         -0.01906022  0.        ], shape=(3,), dtype=float32)
[1 0]
y_j_T is [0.5041972 0.4958028]
sum_of_var is  tf.Tensor([ 0.         -0.01906022  0.        ], shape=(3,), dtype=float32)
[0 1]
y_j_T is [0.461884   0.53811604]
sum_of_var is  tf.Tensor([ 0.         -0.01906022  0.        ], shape=(3,), dtype=float32)
YOU ARE IN ALGO 1
is eager in algo1 True
[0 1]
y_j_T is [0.55674344 0.44325653]
sum_of_var is  tf.Tensor([0. 0.], shape=(2,), dtype=float32)
[1 0]
y_j_T is [0.48688495 0.5131151 ]
sum_of_var is  tf.Tensor([0. 0.], shape=(2,), dtype=float32)
[1 0]
y_j_T is [0.6102812  0.38971886]
sum_of_var is  tf.Tensor([0. 0.], shape=(2,), dtype=float32)
[1 0]
y_j_T is [0.45396575 0.5460342 ]
sum_of_var is  tf.Tensor([0.         0.01982943], shape=(2,), dtype=float32)
[1 0]
y_j_T is [0.54179746 0.45820254]
sum_of_var is  tf.Tensor([0.         0.01982943], shape=(2,), dtype=float32)
[0 1]
y_j_T is [0.44038284 0.5596171 ]
sum_of_var is  tf.Tensor([0.         0.01982943], shape=(2,), dtype=float32)
[1 0]
y_j_T is [0.50789934 0.49210063]
sum_of_var is  tf.Tensor([0.         0.01982943], shape=(2,), dtype=float32)
[1 0]
y_j_T is [0.5696154  0.43038452]
sum_of_var is  tf.Tensor([0.         0.01982943], shape=(2,), dtype=float32)
[1 0]
y_j_T is [0.5041972 0.4958028]
sum_of_var is  tf.Tensor([0.         0.01982943], shape=(2,), dtype=float32)
[0 1]
y_j_T is [0.461884   0.53811604]
sum_of_var is  tf.Tensor([0.         0.01982943], shape=(2,), dtype=float32)
YOU ARE IN ALGO 1
is eager in algo1 True
[0 1]
y_j_T is [0.55674344 0.44325653]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.         -0.04176581
  0.          0.          0.          0.          0.          0.
  0.          0.         -0.04176581  0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.         -0.04176581  0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.         -0.04176581  0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.        ], shape=(177,), dtype=float32)
[1 0]
y_j_T is [0.48688495 0.5131151 ]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.         -0.04176581
  0.          0.          0.          0.          0.          0.
  0.          0.         -0.04176581  0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.         -0.04176581  0.          0.          0.          0.0844858
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.0844858   0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.         -0.04176581  0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.        ], shape=(177,), dtype=float32)
[1 0]
y_j_T is [0.6102812  0.38971886]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.         -0.04176581
  0.          0.00395386  0.          0.          0.          0.
  0.          0.         -0.04176581  0.          0.          0.
  0.          0.          0.          0.00395386  0.          0.00395386
  0.          0.00395386  0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.         -0.04176581  0.          0.          0.          0.0844858
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.0844858   0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.         -0.04176581  0.          0.          0.
  0.          0.          0.          0.          0.          0.00395386
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.        ], shape=(177,), dtype=float32)
[1 0]
y_j_T is [0.45396575 0.5460342 ]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.         -0.04176581
  0.          0.00395386  0.          0.          0.          0.
  0.          0.         -0.04176581  0.          0.          0.
  0.          0.          0.          0.00395386  0.          0.00395386
  0.          0.00395386  0.          0.          0.          0.
  0.          0.          0.          0.          0.00324282  0.
  0.          0.          0.          0.          0.          0.00324282
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.         -0.04176581  0.          0.          0.          0.0844858
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.0844858   0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.00324282  0.00324282  0.          0.          0.
  0.          0.          0.          0.00324282  0.00324282  0.00324282
  0.          0.          0.00324282  0.          0.          0.
  0.          0.          0.          0.          0.          0.00324282
  0.          0.          0.00324282  0.          0.          0.
  0.          0.         -0.04176581  0.          0.          0.
  0.          0.          0.          0.          0.          0.00395386
  0.00324282  0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.00324282  0.00324282  0.00324282  0.00324282  0.00324282  0.
  0.          0.00324282  0.00324282  0.00324282  0.          0.00324282
  0.00324282  0.00324282  0.          0.          0.          0.00324282
  0.          0.          0.        ], shape=(177,), dtype=float32)
[1 0]
y_j_T is [0.54179746 0.45820254]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.         -0.04176581
  0.          0.00395386  0.          0.          0.          0.
  0.          0.         -0.04176581  0.          0.          0.
  0.          0.          0.          0.00395386  0.          0.00395386
  0.          0.00395386  0.          0.          0.          0.
  0.          0.          0.          0.          0.00324282  0.
  0.          0.08681605  0.          0.          0.          0.00324282
  0.          0.          0.08681605  0.          0.          0.
  0.          0.08681605  0.08681605  0.08681605  0.          0.08681605
  0.         -0.04176581  0.08681605  0.08681605  0.          0.0844858
  0.          0.          0.08681605  0.08681605  0.08681605  0.
  0.08681605  0.          0.08681605  0.08681605  0.0844858   0.
  0.          0.          0.          0.08681605  0.          0.
  0.          0.          0.08681605  0.08681605  0.08681605  0.
  0.          0.08681605  0.          0.          0.          0.
  0.08681605  0.          0.          0.08681605  0.08681605  0.08681605
  0.          0.00324282  0.00324282  0.          0.08681605  0.08681605
  0.          0.          0.          0.00324282  0.00324282  0.00324282
  0.          0.08681605  0.00324282  0.08681605  0.08681605  0.08681605
  0.08681605  0.08681605  0.08681605  0.08681605  0.08681605  0.00324282
  0.          0.          0.00324282  0.08681605  0.08681605  0.
  0.08681605  0.         -0.04176581  0.08681605  0.          0.
  0.          0.          0.          0.          0.          0.00395386
  0.00324282  0.          0.          0.08681605  0.08681605  0.08681605
  0.08681605  0.          0.08681605  0.          0.          0.08681605
  0.          0.08681605  0.08681605  0.          0.          0.
  0.00324282  0.00324282  0.00324282  0.00324282  0.00324282  0.
  0.          0.00324282  0.00324282  0.00324282  0.          0.00324282
  0.00324282  0.00324282  0.          0.          0.          0.00324282
  0.          0.          0.        ], shape=(177,), dtype=float32)
[0 1]
y_j_T is [0.44038284 0.5596171 ]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.         -0.04176581
  0.          0.00395386  0.          0.          0.          0.
  0.          0.         -0.04176581  0.          0.          0.
  0.          0.          0.          0.00395386  0.          0.00395386
  0.          0.00395386  0.          0.          0.          0.
 -0.03915343  0.         -0.03915343  0.          0.00324282 -0.03915343
 -0.03915343  0.08681605  0.          0.          0.          0.00324282
  0.          0.          0.08681605  0.          0.          0.
  0.          0.08681605  0.08681605  0.08681605  0.          0.08681605
  0.         -0.04176581  0.08681605  0.08681605  0.          0.0844858
  0.          0.          0.08681605  0.08681605  0.08681605  0.
  0.08681605  0.          0.08681605  0.08681605  0.0844858   0.
  0.          0.          0.          0.08681605  0.          0.
  0.          0.          0.08681605  0.08681605  0.08681605  0.
  0.          0.08681605  0.         -0.03915343  0.          0.
  0.08681605  0.          0.          0.08681605  0.08681605  0.08681605
  0.          0.00324282  0.00324282  0.          0.08681605  0.08681605
  0.          0.          0.          0.00324282  0.00324282  0.00324282
  0.          0.08681605  0.00324282  0.08681605  0.08681605  0.08681605
  0.08681605  0.08681605  0.08681605  0.08681605  0.08681605  0.00324282
  0.          0.          0.00324282  0.08681605  0.08681605  0.
  0.08681605  0.         -0.04176581  0.08681605  0.          0.
  0.          0.          0.          0.          0.          0.00395386
  0.00324282  0.          0.          0.08681605  0.08681605  0.08681605
  0.08681605  0.          0.08681605  0.          0.          0.08681605
  0.          0.08681605  0.08681605  0.          0.          0.
  0.00324282  0.00324282  0.00324282  0.00324282  0.00324282  0.
  0.          0.00324282  0.00324282  0.00324282  0.          0.00324282
  0.00324282  0.00324282  0.          0.          0.          0.00324282
  0.          0.          0.        ], shape=(177,), dtype=float32)
[1 0]
y_j_T is [0.50789934 0.49210063]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.         -0.04176581
  0.          0.00395386  0.          0.          0.          0.
  0.          0.         -0.04176581  0.          0.          0.
  0.          0.          0.          0.00395386  0.          0.00395386
  0.          0.00395386  0.          0.          0.          0.
 -0.03915343  0.         -0.03915343  0.          0.00324282 -0.03915343
 -0.03915343  0.08681605  0.          0.          0.          0.00324282
  0.          0.          0.08681605  0.          0.          0.
  0.          0.08681605  0.08681605  0.08681605  0.          0.08681605
  0.         -0.04176581  0.08681605  0.08681605  0.          0.0844858
  0.          0.          0.08681605  0.08681605  0.08681605  0.
  0.08681605  0.          0.08681605  0.08681605  0.0844858   0.
  0.          0.          0.          0.08681605  0.          0.
  0.          0.          0.08681605  0.08681605  0.08681605  0.
  0.          0.08681605  0.         -0.03915343  0.          0.
  0.08681605  0.          0.          0.08681605  0.08681605  0.08681605
  0.          0.00324282  0.00324282  0.          0.08681605  0.08681605
  0.          0.          0.          0.00324282  0.00324282  0.00324282
  0.          0.08681605  0.00324282  0.08681605  0.08681605  0.08681605
  0.08681605  0.08681605  0.08681605  0.08681605  0.08681605  0.00324282
  0.          0.          0.00324282  0.08681605  0.08681605  0.
  0.08681605  0.         -0.04176581  0.08681605  0.          0.
  0.          0.          0.          0.          0.          0.00395386
  0.00324282  0.          0.          0.08681605  0.08681605  0.08681605
  0.08681605  0.          0.08681605  0.          0.          0.08681605
  0.          0.08681605  0.08681605  0.          0.          0.
  0.00324282  0.00324282  0.00324282  0.00324282  0.00324282  0.
  0.          0.00324282  0.00324282  0.00324282  0.          0.00324282
  0.00324282  0.00324282  0.          0.          0.          0.00324282
  0.          0.          0.        ], shape=(177,), dtype=float32)
[1 0]
y_j_T is [0.5696154  0.43038452]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.         -0.04176581
  0.          0.00395386  0.          0.          0.          0.
  0.          0.         -0.04176581  0.          0.06498707  0.
  0.06498707  0.          0.06498707  0.00395386  0.          0.00395386
  0.          0.00395386  0.          0.          0.          0.
 -0.03915343  0.         -0.03915343  0.          0.00324282 -0.03915343
 -0.03915343  0.08681605  0.          0.06498707  0.06498707  0.00324282
  0.          0.06498707  0.08681605  0.          0.          0.
  0.          0.08681605  0.08681605  0.08681605  0.          0.08681605
  0.         -0.04176581  0.08681605  0.08681605  0.          0.0844858
  0.          0.          0.08681605  0.08681605  0.08681605  0.
  0.08681605  0.          0.08681605  0.08681605  0.0844858   0.
  0.          0.          0.          0.08681605  0.          0.
  0.          0.          0.08681605  0.08681605  0.08681605  0.
  0.          0.08681605  0.         -0.03915343  0.          0.
  0.08681605  0.          0.          0.08681605  0.08681605  0.08681605
  0.          0.00324282  0.00324282  0.          0.08681605  0.08681605
  0.          0.          0.          0.00324282  0.00324282  0.00324282
  0.          0.08681605  0.00324282  0.08681605  0.08681605  0.08681605
  0.08681605  0.08681605  0.08681605  0.08681605  0.08681605  0.00324282
  0.          0.          0.00324282  0.08681605  0.08681605  0.
  0.08681605  0.         -0.04176581  0.08681605  0.          0.
  0.          0.          0.          0.          0.          0.00395386
  0.00324282  0.          0.          0.08681605  0.08681605  0.08681605
  0.08681605  0.          0.08681605  0.          0.          0.08681605
  0.          0.08681605  0.08681605  0.          0.          0.
  0.00324282  0.00324282  0.00324282  0.00324282  0.00324282  0.
  0.          0.00324282  0.00324282  0.00324282  0.          0.00324282
  0.00324282  0.00324282  0.          0.          0.          0.00324282
  0.          0.          0.        ], shape=(177,), dtype=float32)
[1 0]
y_j_T is [0.5041972 0.4958028]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.         -0.04176581
  0.          0.00395386  0.          0.          0.          0.
  0.          0.         -0.04176581  0.          0.06498707  0.
  0.06498707  0.          0.06498707  0.00395386  0.          0.00395386
  0.          0.00395386  0.          0.          0.          0.
 -0.03915343  0.         -0.03915343  0.          0.00324282 -0.03915343
 -0.03915343  0.08681605  0.          0.06498707  0.06498707  0.00324282
  0.          0.06498707  0.08681605  0.          0.          0.
  0.          0.08681605  0.08681605  0.08681605  0.          0.08681605
  0.         -0.04176581  0.08681605  0.08681605  0.          0.0844858
  0.          0.          0.08681605  0.08681605  0.08681605  0.
  0.08681605  0.          0.08681605  0.08681605  0.0844858   0.
  0.          0.          0.          0.08681605  0.          0.
  0.          0.          0.08681605  0.08681605  0.08681605  0.
  0.          0.08681605  0.         -0.03915343  0.          0.
  0.08681605  0.          0.          0.08681605  0.08681605  0.08681605
  0.          0.00324282  0.00324282  0.          0.08681605  0.08681605
  0.          0.          0.          0.00324282  0.00324282  0.00324282
  0.          0.08681605  0.00324282  0.08681605  0.08681605  0.08681605
  0.08681605  0.08681605  0.08681605  0.08681605  0.08681605  0.00324282
  0.          0.          0.00324282  0.08681605  0.08681605  0.
  0.08681605  0.         -0.04176581  0.08681605  0.          0.
  0.          0.          0.          0.          0.          0.00395386
  0.00324282  0.          0.          0.08681605  0.08681605  0.08681605
  0.08681605  0.          0.08681605  0.          0.          0.08681605
  0.          0.08681605  0.08681605  0.          0.          0.
  0.00324282  0.00324282  0.00324282  0.00324282  0.00324282  0.
  0.          0.00324282  0.00324282  0.00324282  0.          0.00324282
  0.00324282  0.00324282  0.          0.          0.          0.00324282
  0.          0.          0.        ], shape=(177,), dtype=float32)
[0 1]
y_j_T is [0.461884   0.53811604]
sum_of_var is  tf.Tensor(
[ 0.          0.          0.          0.          0.         -0.04176581
  0.          0.00395386  0.          0.          0.          0.
  0.          0.         -0.04176581  0.          0.06498707  0.
  0.06498707  0.          0.06498707  0.00395386  0.          0.00395386
  0.          0.00395386  0.          0.          0.          0.
 -0.03915343  0.         -0.03915343  0.          0.00324282 -0.03915343
 -0.03915343  0.08681605  0.          0.06498707  0.06498707  0.00324282
  0.          0.06498707  0.08681605  0.          0.          0.
  0.          0.08681605  0.08681605  0.08681605  0.          0.08681605
  0.         -0.04176581  0.08681605  0.08681605  0.          0.0844858
  0.          0.          0.08681605  0.08681605  0.08681605  0.
  0.08681605  0.          0.08681605  0.08681605  0.0844858   0.
  0.          0.          0.          0.08681605  0.          0.
  0.          0.          0.08681605  0.08681605  0.08681605  0.
  0.          0.08681605  0.         -0.03915343  0.          0.
  0.08681605  0.          0.          0.08681605  0.08681605  0.08681605
  0.          0.00324282  0.00324282  0.          0.08681605  0.08681605
  0.          0.          0.          0.00324282  0.00324282  0.00324282
  0.          0.08681605  0.00324282  0.08681605  0.08681605  0.08681605
  0.08681605  0.08681605  0.08681605  0.08681605  0.08681605  0.00324282
  0.          0.          0.00324282  0.08681605  0.08681605  0.
  0.08681605  0.         -0.04176581  0.08681605  0.          0.
  0.          0.          0.          0.          0.          0.00395386
  0.00324282  0.          0.          0.08681605  0.08681605  0.08681605
  0.08681605  0.          0.08681605  0.          0.          0.08681605
  0.          0.08681605  0.08681605  0.          0.          0.
  0.00324282  0.00324282  0.00324282  0.00324282  0.00324282  0.
  0.          0.00324282  0.00324282  0.00324282  0.          0.00324282
  0.00324282  0.00324282  0.          0.          0.          0.00324282
  0.          0.          0.        ], shape=(177,), dtype=float32)
YOU ARE IN ALGO 1
is eager in algo1 True
[0 1]
y_j_T is [0.55674344 0.44325653]
sum_of_var is  tf.Tensor([0. 0.], shape=(2,), dtype=float32)
[1 0]
y_j_T is [0.48688495 0.5131151 ]
sum_of_var is  tf.Tensor([0. 0.], shape=(2,), dtype=float32)
[1 0]
y_j_T is [0.6102812  0.38971886]
sum_of_var is  tf.Tensor([0. 0.], shape=(2,), dtype=float32)
[1 0]
y_j_T is [0.45396575 0.5460342 ]
sum_of_var is  tf.Tensor([0.         0.00777712], shape=(2,), dtype=float32)
[1 0]
y_j_T is [0.54179746 0.45820254]
sum_of_var is  tf.Tensor([0.         0.00777712], shape=(2,), dtype=float32)
[0 1]
y_j_T is [0.44038284 0.5596171 ]
sum_of_var is  tf.Tensor([0.         0.00777712], shape=(2,), dtype=float32)
[1 0]
y_j_T is [0.50789934 0.49210063]
sum_of_var is  tf.Tensor([0.         0.00777712], shape=(2,), dtype=float32)
[1 0]
y_j_T is [0.5696154  0.43038452]
sum_of_var is  tf.Tensor([0.         0.00777712], shape=(2,), dtype=float32)
[1 0]
y_j_T is [0.5041972 0.4958028]
sum_of_var is  tf.Tensor([0.         0.00777712], shape=(2,), dtype=float32)
[0 1]
y_j_T is [0.461884   0.53811604]
sum_of_var is  tf.Tensor([0.         0.00777712], shape=(2,), dtype=float32)
Traceback (most recent call last):
  File "/Users/arimuu/OneDrive/TMU/Thä¿®å£«è«–æ–‡/ScraperProgram/GINN/ginn_model.py", line 356, in <module>
    main()
  File "/Users/arimuu/OneDrive/TMU/Thä¿®å£«è«–æ–‡/ScraperProgram/GINN/ginn_model.py", line 346, in main
    g_model.fit(data.inputs, epochs=3 )
  File "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py", line 1216, in fit
    tmp_logs = self.train_function(iterator)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py", line 879, in train_function
    return step_function(self, iterator)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py", line 869, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))
  File "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py", line 1285, in run
    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py", line 2833, in call_for_each_replica
    return self._call_for_each_replica(fn, args, kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py", line 3608, in _call_for_each_replica
    return fn(*args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py", line 597, in wrapper
    return func(*args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py", line 861, in run_step
    outputs = model.train_step(data)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py", line 818, in train_step
    self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py", line 533, in minimize
    grads_and_vars = self._compute_gradients(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py", line 587, in _compute_gradients
    grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py", line 473, in _get_gradients
    grads = tape.gradient(loss, var_list, grad_loss)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py", line 1075, in gradient
    flat_grad = imperative_grad.imperative_grad(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py", line 71, in imperative_grad
    return pywrap_tfe.TFE_Py_TapeGradient(
tensorflow.python.framework.errors_impl.InternalError: Recorded operation 'GINN_op' returned too few gradients. Expected 2 but received 1
[1m[7m%[27m[1m[0m                                                 [0m[27m[24m[Jarimuu@ARs-MacBook-Pro GINN % [K[?2004heexit[?2004l

Script done on Sun Feb 20 16:49:18 2022
